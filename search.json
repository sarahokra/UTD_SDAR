[
  {
    "objectID": "TWBC.html",
    "href": "TWBC.html",
    "title": "Female Breast Cancer in Taiwan 2010-2021",
    "section": "",
    "text": "# Load required packages\npacman::p_load(ggplot2, plotly, scales)\n\n# Ë®≠ÂÆöÂ∑•‰ΩúÁõÆÈåÑ\n#setwd(\"C:/Users/dells/Desktop/quarto/quarto_website/UTD_SDAR\")\n\n# ÂåØÂÖ• CSV Êñá‰ª∂\nTWBC &lt;- read.csv(\"TWBC_R.csv\")\n\n# Ê™¢Ë¶ñË≥áÊñôË°®Ê†º\nView(TWBC)\n\n# Create the base plot\nbc &lt;- ggplot(TWBC, aes(N_Incidence, N_Deaths, color = Region)) +\n  geom_point(aes(size = N_Incidence, frame = Year, ids = City))\n\nWarning in geom_point(aes(size = N_Incidence, frame = Year, ids = City)):\nIgnoring unknown aesthetics: frame and ids\n\n# Display the static plot\nbc\n\n\n\n\n\n\n\n# Enhance the plot\nbc &lt;- ggplot(TWBC, aes(N_Incidence, N_Deaths, color = Region)) +\n  geom_point(aes(size = N_Incidence, frame = Year, ids = City, alpha = 0.3)) +\n  scale_x_log10(labels = scales::comma_format()) +\n  labs(title = \"Female Breast Cancer in Taiwan: Nunber of Incidence vs Deaths\",\n       x = \"Nunber of Incidence\", \n       y = \"Nunber of Deaths\",\n       color = \"Region\",\n       size = \"Number of Incidence\") +\n  theme_minimal()\n\nWarning in geom_point(aes(size = N_Incidence, frame = Year, ids = City, :\nIgnoring unknown aesthetics: frame and ids\n\n# Display the enhanced static plot\nbc  \n\n\n\n\n\n\n\n# Create the interactive plot\ninteractive_plot &lt;- ggplotly(bc)\n\n# Display the interactive plot\ninteractive_plot",
    "crumbs": [
      "Home",
      "My Implementation",
      "Female Breast Cancer in Taiwan"
    ]
  },
  {
    "objectID": "TWBC.html#dashboard",
    "href": "TWBC.html#dashboard",
    "title": "Female Breast Cancer in Taiwan 2010-2021",
    "section": "Dashboard",
    "text": "Dashboard\n\n\nFull Screen",
    "crumbs": [
      "Home",
      "My Implementation",
      "Female Breast Cancer in Taiwan"
    ]
  },
  {
    "objectID": "TWBC.html#website-application",
    "href": "TWBC.html#website-application",
    "title": "Female Breast Cancer in Taiwan 2010-2021",
    "section": "Website Application",
    "text": "Website Application\n\n\nFull Screen",
    "crumbs": [
      "Home",
      "My Implementation",
      "Female Breast Cancer in Taiwan"
    ]
  },
  {
    "objectID": "IM_Progress02.html",
    "href": "IM_Progress02.html",
    "title": "Progress Presentation",
    "section": "",
    "text": "Traditional Chinese Medicine for Everyday Wellness",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Progress Presentation"
    ]
  },
  {
    "objectID": "IM_Progress02.html#data-collection-progress-30",
    "href": "IM_Progress02.html#data-collection-progress-30",
    "title": "Progress Presentation",
    "section": "2.1 Data Collection (Progress: 30%)",
    "text": "2.1 Data Collection (Progress: 30%)\n\n2.1.1 Web Scraping TCM Textbooks and Websites Using Python\nExample script:\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\nheaders = {\"User-Agent\": \"Mozilla/5.0\"}\nbase_url = \"https://www.zenheart.com.tw/\"\nmeridian_urls = [f\"{base_url}Meridian{str(i).zfill(2)}.php\" for i in range(1, 15)]\n\nall_acupoints = []\n\ndef extract_zhuzhi(html):\n    soup = BeautifulSoup(html, 'html.parser')\n    for p in soup.find_all('p'):\n        if p.get_text(strip=True) == \"‰∏ªÊ≤ª\":\n            container_div = p.find_parent('div')\n            if container_div:\n                h3 = container_div.find('h3')\n                if h3:\n                    return h3.get_text(strip=True)\n    return None\n\ndef get_acupoint_name(soup):\n    h2 = soup.find(\"h2\", class_=\"h1 u-heading-v7__title\")\n    return h2.get_text(strip=True) if h2 else \"\"\n\nfor meridian_url in meridian_urls:\n    try:\n        res = requests.get(meridian_url, headers=headers)\n        res.encoding = 'utf-8'\n        soup = BeautifulSoup(res.text, 'html.parser')\n\n        links = soup.find_all(\"a\", href=True)\n        acupoint_links = [\n            base_url + link[\"href\"]\n            for link in links if \"Meridian\" in link[\"href\"] and \"_\" in link[\"href\"]\n        ]\n        acupoint_links = list(set(acupoint_links))\n\n        for acupoint_url in acupoint_links:\n            try:\n                r = requests.get(acupoint_url, headers=headers)\n                r.encoding = 'utf-8'\n                acupoint_html = r.text\n                s = BeautifulSoup(acupoint_html, 'html.parser')\n\n                zhuzhi = extract_zhuzhi(acupoint_html)\n                name = get_acupoint_name(s)\n\n                all_acupoints.append({\n                    \"Acupoint Name\": name,\n                    \"URL\": acupoint_url,\n                    \"Indications\": zhuzhi\n                })\n                time.sleep(0.3)\n\n            except Exception as e:\n                continue\n\n    except Exception as e:\n        continue\n\ndf = pd.DataFrame(all_acupoints)\ndf.to_csv(\"zenheart_acupoints.csv\", index=False, encoding='utf-8-sig')\n\n\n\n\n2.1.2 Manual Data Correction\n\nIncludes issues like acupoint codes or characters inconsistent\n\nCurrently performing manual matching and correction\n\nüì• View Dataset",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Progress Presentation"
    ]
  },
  {
    "objectID": "IM_Progress02.html#importing-excel-file-into-postgresql-trial",
    "href": "IM_Progress02.html#importing-excel-file-into-postgresql-trial",
    "title": "Progress Presentation",
    "section": "2.2 Importing Excel File into PostgreSQL (Trial)",
    "text": "2.2 Importing Excel File into PostgreSQL (Trial)\n\nSet up connections: to connect to local SQL database\n\nImported meridian and acupoint data into PostgreSQL\n\nExample script:\n\n\nCode\nlibrary(readxl)\nlibrary(DBI)\nlibrary(RPostgres)\n\nacupoint &lt;- read_excel(\"TCMtest01.xlsx\", sheet = \"Acupoint\")\nmeridian &lt;- read_excel(\"TCMtest01.xlsx\", sheet = \"Meridian\")\n\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname = \"TCMtest01\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"postgres\",\n  password = \"Ting87724$\"\n)\n\ndbExecute(con, \"DROP TABLE IF EXISTS Acupoint;\")\ndbExecute(con, \"DROP TABLE IF EXISTS Meridian;\")\n\ndbExecute(con, \"\n  CREATE TABLE meridian (\n    meridian_code VARCHAR PRIMARY KEY,\n    meridian_eng VARCHAR,\n    meridian_chi VARCHAR\n  );\n\")\n\ndbExecute(con, \"\n  CREATE TABLE acupoint (\n    acupoint_code VARCHAR PRIMARY KEY,\n    acupoint_eng VARCHAR,\n    acupoint_chi VARCHAR,\n    body_acu VARCHAR,\n    indication VARCHAR,\n    function VARCHAR,\n    meridian_code VARCHAR REFERENCES Meridian(meridian_code),\n    location_chi VARCHAR,\n    location_ima VARCHAR\n  );\n\")\n\nmeridian &lt;- meridian[, !names(meridian) %in% \"m_ID\"]\nacupoint &lt;- acupoint[, !names(acupoint) %in% \"a_ID\"]\n\ndbWriteTable(con, \"meridian\", meridian, append = TRUE, row.names = FALSE)\ndbWriteTable(con, \"acupoint\", acupoint, append = TRUE, row.names = FALSE)\ndbDisconnect(con)\n\n\n\n2.2.1 TCMtest01 Database:",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Progress Presentation"
    ]
  },
  {
    "objectID": "IM_Progress02.html#building-an-r-shiny-app-trial",
    "href": "IM_Progress02.html#building-an-r-shiny-app-trial",
    "title": "Progress Presentation",
    "section": "2.3 Building an R Shiny App (Trial)",
    "text": "2.3 Building an R Shiny App (Trial)\nPreliminary interface design:",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Progress Presentation"
    ]
  },
  {
    "objectID": "IM_assignment07.html",
    "href": "IM_assignment07.html",
    "title": "Assignment 7",
    "section": "",
    "text": "I used my password to access my database on my local PostgreSQL server.\n\n\n\nSee bellowed Shinyapp.\n\n\n\nSee bellowed Shinyapp.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 7"
    ]
  },
  {
    "objectID": "IM_assignment07.html#a.-be-sure-you-enter-your-password-to-access-your-database-on-your-local-postgresql-server",
    "href": "IM_assignment07.html#a.-be-sure-you-enter-your-password-to-access-your-database-on-your-local-postgresql-server",
    "title": "Assignment 7",
    "section": "",
    "text": "I used my password to access my database on my local PostgreSQL server.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 7"
    ]
  },
  {
    "objectID": "IM_assignment07.html#b.-change-the-sorting-of-salary-to-from-high-to-low",
    "href": "IM_assignment07.html#b.-change-the-sorting-of-salary-to-from-high-to-low",
    "title": "Assignment 7",
    "section": "",
    "text": "See bellowed Shinyapp.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 7"
    ]
  },
  {
    "objectID": "IM_assignment07.html#c.-try-another-variable-in-another-table",
    "href": "IM_assignment07.html#c.-try-another-variable-in-another-table",
    "title": "Assignment 7",
    "section": "",
    "text": "See bellowed Shinyapp.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 7"
    ]
  },
  {
    "objectID": "IM_assignment05.html",
    "href": "IM_assignment05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Choose 1 or 2 to answer and answer the rest. Prepare your answers in presentation format and be ready to present in class",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 5"
    ]
  },
  {
    "objectID": "IM_assignment05.html#a-the-graph-is-disconnected.",
    "href": "IM_assignment05.html#a-the-graph-is-disconnected.",
    "title": "Assignment 5",
    "section": "(a) The graph is disconnected.",
    "text": "(a) The graph is disconnected.\nA disconnected graph in an ERD means that some entities (rectangles) are not connected to the rest of the entities. This implies that entities in the database have no relationships with some of the other entities. For example, in an enterprise, some departments work together in a branch but not with the departments in other branches.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 5"
    ]
  },
  {
    "objectID": "IM_assignment05.html#b-the-graph-has-a-cycle.",
    "href": "IM_assignment05.html#b-the-graph-has-a-cycle.",
    "title": "Assignment 5",
    "section": "(b) The graph has a cycle.",
    "text": "(b) The graph has a cycle.\nA graph containing a cycle means that in an ERD, starting from a certain entity, following a series of relationships will eventually lead back to the same entity. This implies that no entity is entirely isolated in the database‚Äîeach entity is related to at least two other entities, and eventually forming a cycle. For example, an Employee belongs to a Department, which may be linked to certain Projects, and these projects may involve the same or other employees, thus forming a cycle.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 5"
    ]
  },
  {
    "objectID": "IM_assignment05.html#a-consider-the-employee-database",
    "href": "IM_assignment05.html#a-consider-the-employee-database",
    "title": "Assignment 5",
    "section": "(a) Consider the employee database",
    "text": "(a) Consider the employee database\n\nwhere the primary keys are underlined. Give an expression in SQL for each of the following queries. (Hint: use from employee as e, works as w, company as c, manages as m)\n\ni Find ID and name of each employee who lives in the same city as the location of the company for which the employee works.\nSELECT e.ID, e.person_name\nFROM employee AS e\nJOIN works AS w ON e.ID = w.ID\nJOIN company AS c ON w.company_name = c.company_name\nWHERE e.city = c.city;\n\n\nii Find ID and name of each employee who lives in the same city and on the same street as does her or his manager.\nSELECT e.ID, e.person_name\nFROM employee AS e\nJOIN manages AS m ON e.ID = m.ID\nJOIN employee AS mgr ON m.manager_id = mgr.ID\nWHERE e.city = mgr.city AND e.street = mgr.street;\n\n\niii Find ID and name of each employee who earns more than the average salary of all employees of her or his company.\nSELECT e.ID, e.person_name\nFROM employee AS e\nJOIN works AS w ON e.ID = w.ID\nWHERE w.salary &gt; (\n    SELECT AVG(w2.salary)\n    FROM works AS w2\n    WHERE w2.company_name = w.company_name\n);",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 5"
    ]
  },
  {
    "objectID": "IM_assignment05.html#b-consider-the-following-sql-query-that-seeks-to-find-a-list-of-titles-of-all-courses-taught-in-spring-2017-along-with-the-name-of-the-instructor.",
    "href": "IM_assignment05.html#b-consider-the-following-sql-query-that-seeks-to-find-a-list-of-titles-of-all-courses-taught-in-spring-2017-along-with-the-name-of-the-instructor.",
    "title": "Assignment 5",
    "section": "(b) Consider the following SQL query that seeks to find a list of titles of all courses taught in Spring 2017 along with the name of the instructor.",
    "text": "(b) Consider the following SQL query that seeks to find a list of titles of all courses taught in Spring 2017 along with the name of the instructor.\n\nWhat is wrong with this query? (Hint: check book website)\n\nfile.exists(\"sql.db\")\n\n[1] TRUE\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Establishing a SQLite Connection\nconn &lt;- dbConnect(SQLite(), \"sql.db\")\n\n# Set Quarto to use this SQL connection\nknitr::opts_chunk$set(connection = conn)\n\n\nselect name, title\nfrom instructor natural join teaches natural join section natural join course\nwhere semester = 'Spring' and year = 2017;\n\n\n3 records\n\n\nname\ntitle\n\n\n\n\nBrandt\nGame Design\n\n\nBrandt\nGame Design\n\n\nKim\nIntro. to Digital Systems\n\n\n\n\n\nNatural join is an operation performed on two relations, producing a new relation as a result. Unlike the Cartesian product, which combines every tuple from the first relation with every tuple from the second relation without any condition, natural join selects only those tuples where the values match on the common attributes present in both relations.\nTherefore, in this case, natural join may incorrectly match dept_name in instructor and course table:\nBoth instructor and course tables contain the dept_name attribute. However, dept_name in the instructor table represents the department to which the instructor belongs, whereas in the course table, it represents the department offering the course. Since natural join enforces equality on all common attributes, this leads to the following issue: If the instructor‚Äôs department (instructor.dept_name) does not match the department offering the course (ourse.dept_name), the corresponding record will not appear in the result. This incorrectly excludes courses that are cross-listed across departments, leading to incomplete query results.\nTo avoid natural join from treating all matching attributes as join conditions, SQL provides a form of inner join that allows us to specify explicit join conditions, as shown below:\n\nselect name, title\nfrom (instructor join teaches using (ID)) \njoin section using (course_id, sec_id)\njoin course using (course_id)\nwhere section.semester = 'Spring' and section.year = 2017;\n\n\n3 records\n\n\nname\ntitle\n\n\n\n\nBrandt\nGame Design\n\n\nBrandt\nGame Design\n\n\nKim\nIntro. to Digital Systems\n\n\n\n\n\n(The above content refers to P.126-P.131 of the textbook, SKS.)",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 5"
    ]
  },
  {
    "objectID": "IM_assignment03.html",
    "href": "IM_assignment03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "file.exists(\"sql.db\")\n\n[1] TRUE\n\n\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Establishing a SQLite Connection\nconn &lt;- dbConnect(SQLite(), \"sql.db\")\n\n# Set Quarto to use this SQL connection\nknitr::opts_chunk$set(connection = conn)",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#i.-students-ids-hint-from-the-takes-relation",
    "href": "IM_assignment03.html#i.-students-ids-hint-from-the-takes-relation",
    "title": "Assignment 3",
    "section": "i. Students IDs (hint: from the takes relation)",
    "text": "i. Students IDs (hint: from the takes relation)\n\nSELECT ID FROM takes;\n\n\nDisplaying records 1 - 10\n\n\nID\n\n\n\n\n00128\n\n\n00128\n\n\n12345\n\n\n12345\n\n\n12345\n\n\n12345\n\n\n19991\n\n\n23121\n\n\n44553\n\n\n45678",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#ii.-instructors",
    "href": "IM_assignment03.html#ii.-instructors",
    "title": "Assignment 3",
    "section": "ii. Instructors",
    "text": "ii. Instructors\n\nselect name from instructor\n\n\nDisplaying records 1 - 10\n\n\nname\n\n\n\n\nSrinivasan\n\n\nWu\n\n\nMozart\n\n\nEinstein\n\n\nEl Said\n\n\nGold\n\n\nKatz\n\n\nCalifieri\n\n\nSingh\n\n\nCrick",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#iii.-departments",
    "href": "IM_assignment03.html#iii.-departments",
    "title": "Assignment 3",
    "section": "iii. Departments",
    "text": "iii. Departments\n\nselect dept_name from department\n\n\n7 records\n\n\ndept_name\n\n\n\n\nBiology\n\n\nComp. Sci.\n\n\nElec. Eng.\n\n\nFinance\n\n\nHistory\n\n\nMusic\n\n\nPhysics",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#i.-find-the-id-and-name-of-each-student-who-has-taken-at-least-one-comp.-sci.-course-make-sure-there-are-no-duplicate-names-in-the-result.",
    "href": "IM_assignment03.html#i.-find-the-id-and-name-of-each-student-who-has-taken-at-least-one-comp.-sci.-course-make-sure-there-are-no-duplicate-names-in-the-result.",
    "title": "Assignment 3",
    "section": "i. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.",
    "text": "i. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\nSELECT DISTINCT S.ID, S.name\nFROM student S\nJOIN takes T ON S.ID = T.ID\nJOIN course C ON T.course_id = C.course_id\nWHERE C.dept_name = 'Comp. Sci.';\n\n\n6 records\n\n\nID\nname\n\n\n\n\n00128\nZhang\n\n\n12345\nShankar\n\n\n45678\nLevy\n\n\n54321\nWilliams\n\n\n76543\nBrown\n\n\n98765\nBourikas",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#ii.-add-grades-to-the-list.",
    "href": "IM_assignment03.html#ii.-add-grades-to-the-list.",
    "title": "Assignment 3",
    "section": "ii. Add grades to the list.",
    "text": "ii. Add grades to the list.\n\nSELECT DISTINCT S.ID, S.name, T.grade\nFROM student S\nJOIN takes T ON S.ID = T.ID\nJOIN course C ON T.course_id = C.course_id\nWHERE C.dept_name = 'Comp. Sci.';\n\n\nDisplaying records 1 - 10\n\n\nID\nname\ngrade\n\n\n\n\n00128\nZhang\nA\n\n\n00128\nZhang\nA-\n\n\n12345\nShankar\nC\n\n\n12345\nShankar\nA\n\n\n45678\nLevy\nF\n\n\n45678\nLevy\nB+\n\n\n45678\nLevy\nB\n\n\n54321\nWilliams\nA-\n\n\n54321\nWilliams\nB+\n\n\n76543\nBrown\nA",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#iii.-find-the-id-and-name-of-each-student-who-has-not-taken-any-course-offered-before-2017.",
    "href": "IM_assignment03.html#iii.-find-the-id-and-name-of-each-student-who-has-not-taken-any-course-offered-before-2017.",
    "title": "Assignment 3",
    "section": "iii. Find the ID and name of each student who has not taken any course offered before 2017.",
    "text": "iii. Find the ID and name of each student who has not taken any course offered before 2017.\n\nSELECT S.ID, S.name\nFROM student S\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM takes T\n    WHERE T.ID = S.ID\n    AND T.year &lt; 2017\n);\n\n\nDisplaying records 1 - 10\n\n\nID\nname\n\n\n\n\n00128\nZhang\n\n\n12345\nShankar\n\n\n19991\nBrandt\n\n\n23121\nChavez\n\n\n44553\nPeltier\n\n\n45678\nLevy\n\n\n54321\nWilliams\n\n\n55739\nSanchez\n\n\n70557\nSnow\n\n\n76543\nBrown",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#iv.-for-each-department-find-the-maximum-salary-of-instructors-in-that-department.-you-may-assume-that-every-department-has-at-least-one-instructor.",
    "href": "IM_assignment03.html#iv.-for-each-department-find-the-maximum-salary-of-instructors-in-that-department.-you-may-assume-that-every-department-has-at-least-one-instructor.",
    "title": "Assignment 3",
    "section": "iv. For each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.",
    "text": "iv. For each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\n\nSELECT dept_name, MAX(salary) AS max_salary\nFROM instructor\nGROUP BY dept_name;\n\n\n7 records\n\n\ndept_name\nmax_salary\n\n\n\n\nBiology\n72000\n\n\nComp. Sci.\n92000\n\n\nElec. Eng.\n80000\n\n\nFinance\n90000\n\n\nHistory\n62000\n\n\nMusic\n40000\n\n\nPhysics\n95000",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#v.-find-the-lowest-across-all-departments-of-the-per-department-maximum-salary-computed-by-the-preceding-query.",
    "href": "IM_assignment03.html#v.-find-the-lowest-across-all-departments-of-the-per-department-maximum-salary-computed-by-the-preceding-query.",
    "title": "Assignment 3",
    "section": "v. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.",
    "text": "v. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\nSELECT MIN(max_salary) AS lowest_max_salary\nFROM (\n    SELECT dept_name, MAX(salary) AS max_salary\n    FROM instructor\n    GROUP BY dept_name\n) AS department_max_salaries;\n\n\n1 records\n\n\nlowest_max_salary\n\n\n\n\n40000",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment03.html#vi.-add-names-to-the-list",
    "href": "IM_assignment03.html#vi.-add-names-to-the-list",
    "title": "Assignment 3",
    "section": "vi. Add names to the list",
    "text": "vi. Add names to the list\n\nSELECT I.name, D.lowest_max_salary\nFROM instructor I\nJOIN (\n    SELECT dept_name, MIN(max_salary) AS lowest_max_salary\n    FROM (\n        SELECT dept_name, MAX(salary) AS max_salary\n        FROM instructor\n        GROUP BY dept_name\n    ) AS department_max_salaries\n) AS D ON I.dept_name = D.dept_name AND I.salary = D.lowest_max_salary;\n\n\n1 records\n\n\nname\nlowest_max_salary\n\n\n\n\nMozart\n40000",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 3"
    ]
  },
  {
    "objectID": "IM_assignment01.html",
    "href": "IM_assignment01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Answer 1, 2, 6 and one of the remaining questions. Prepare your answers in presentation format (e.g.¬†post on website). Members will be selected to present answers in class. Be ready for your presentation.\n\n1. Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g.¬†airlines, online trade, banking, university system)\n\n\n\n\n\n\n\n\nApplications\nPurpose\nFeatures\n\n\n\n\nOnline Shopping Platform (e.g., Amazon)\nStores user account information, product details, order history, logistics data, and user behavior.\nSupports concurrent access for multiple users and maintains query efficiency.\n\n\nBanking System (e.g., Bank of America Mobile App)\nManages account balances, transaction histories, loans, and other financial product information.\nEnsures data consistency and security while preventing data loss and errors.\n\n\nUniversity Course Management System (e.g., UTD Course Registration System)\nStores course information (e.g., course ID, credit hours, schedule, instructor, etc.), tracks registered students, calculates total earned credits, and determines tuition fees.\nSupports concurrent access for multiple users and enables real-time information updates.\n\n\n\n\n\n2. Propose three applications in domain projects (e.g.¬†criminology, economics, brain science, etc.) Be sure you include:\n\nPurpose\nFunctions\nSimple interface design\n\n\n\n\n\n\n\n\n\n\nDomain\nPurpose\nFunctions\nInterface Design\n\n\n\n\nHealthcare\nControl of acute infectious diseases ‚Äì Supply users with immediate prevention information during pandemics.\n\nDisplay current policies, infection, and death counts.\nBrowse locations visited by infected individuals.\nProvide testing reservations.\n\n\nEssential info display area for policies and counts.\nSearch bar and infection map.\nTesting facility search field.\n\n\n\nEducation\nLearning portfolio management ‚Äì Enable students to store learning data, assess their chances of admission, and support them in the university application process reviews.\n\nDisplay and edit student information.\nRecord and query course history.\nAnalyze student performance and estimate admission chances.\n(Students) Upload documents (e.g., transcripts, awards). / (Teachers) Verify the authenticity of student submissions.\nPackage and automatically send data to schools.\n\n\nStudent info display area.\nCourse record display area and search field.\nPerformance radar chart and school. recommendations list.\nUpload fields for transcripts, awards, and reflections / Verification fields for student documents.\nData packaging and send button.\n\n\n\nTourism\nTravel planning ‚Äì Store information about tourist attractions and suggest destinations, itineraries, or activities based on visitors‚Äô interests and needs.\n\nCollect tourist preferences (e.g., nature, culture).\nGenerate recommendations based on weather, traffic, and feedback.\nProvide maps and route planning.\nCollect tourist feedback to improve services.\n\n\nPreference selection interface.\nRecommended itinerary and attractions list.\nRoute planning map.\nFeedback input field.\n\n\n\n\n\n\n3. If data can be retrieved efficiently and effectively, why data mining is needed?\nData retrieval focuses on efficiently locating data already stored in a database. In contrast, data mining applies statistical methods and artificial intelligence to extract valuable insights from existing data. This process assists businesses and researchers with decision-making, forecasting future trends, and even uncovering new knowledge. For example, in healthcare, medical data mining helps identify disease risk factors, categorize patient groups, and determine effective treatment strategies, ultimately enhancing the quality and efficiency of healthcare services. In social media, analyzing user behavior and sentiment on platforms aids businesses in understanding market trends, product reputation, and customer satisfaction, facilitating the development of effective marketing strategies.\n\n\n4. Why NoSQL systems emerged in the 2000s? Briefly contrast their features with traditional database systems.\nAfter the 2000s, the demand for emerging applications like social media and big data analytics shifted data storage requirements. Data types expanded beyond traditional structured formats to include semi-structured and unstructured formats like JSON and XML. These changes necessitated large-scale storage and rapid scalability. To accommodate high-concurrency read and write operations, NoSQL systems became suitable alternatives to traditional relational database management systems (RDBMS), which rely on row-oriented storage and strict consistency. NoSQL databases offer better flexibility for diverse data types, making them useful for large-scale social media data and real-time queries.\nKey Characteristics of NoSQL Systems:\n1. Flexible Data Model: NoSQL databases lack a fixed schema, storing semi-structured or unstructured data (e.g., JSON, XML).\n2. High Scalability: Their distributed architecture supports cost-effective horizontal scaling compared to traditional vertical scaling.\n3. Eventual Consistency: NoSQL allows temporary data inconsistencies, enhancing availability through eventual consistency.\n4. Flexible Query Mechanisms: Many NoSQL databases offer custom APIs or alternative query languages instead of SQL.\n5. Optimized for Big Data: NoSQL systems excel in handling large data volumes, such as big data storage and log analysis.\nNoSQL Systems vs.¬†Traditional RDBMS:\n\n\n\n\n\n\n\n\nFeature\nNoSQL Systems\nRelational Databases (RDBMS)\n\n\n\n\nData Model\nSemi-structured or unstructured data: document-based, key-value, and graph-based models.\nStructured data stored in tables (rows and columns) with a strict schema.\n\n\nQuery Language\nCustom APIs or NoSQL query languages.\nSQL (Structured Query Language).\n\n\nScalability\nHorizontal scaling (Scale-out) fits distributed architectures.\nVertical scaling (Scale-up) depends on enhancing single-node performance.\n\n\nConsistency\nEventual consistency favors availability and scalability.\nStrong consistency (ACID) guarantees strict data integrity.\n\n\nUse Cases\nSocial media, big data applications, distributed systems.\nFinancial transactions, enterprise applications, and traditional business systems.\n\n\nPerformance Advantages\nOptimized for high-volume read and write operations.\nOptimized for structured queries and transactional processing.\n\n\n\n\n\n5. What are the things current database system cannot do?\n1. Automated Semantic Understanding: Databases cannot grasp the semantics of data and can only operate according to established rules.\n2. Automated Decision-Making Suggestions: Database systems do not possess advanced analytical abilities and depend on external applications for data analysis.\n\n\n6. Describe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\n\n\n\n\n\n\n\n\nTable\nFields\nPurpose\n\n\n\n\nUsers\nUserID, Username, Account, Citizenship, UserLevel, ProfilePicture, Membership, AverageLogintime.\nStores basic user information and authentication data.\n\n\nPosts\nPostID, UserID, Content, ContentEmotion, Tag, Language, Location, Timestamp, LikesCount, CommentsCount, AverageViewingTime.\nStores the content of each post along with its interaction data.\n\n\nInteractions\nInteractionID, UserID, PostID, InteractionType (e.g., Like, Comment, Share), InteractionContent, Timestamp.\nStores the interactions link with each post.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 1"
    ]
  },
  {
    "objectID": "DV_assignment07.html",
    "href": "DV_assignment07.html",
    "title": "Assignment 7",
    "section": "",
    "text": "knitr::opts_chunk$set(warning = FALSE)",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 7"
    ]
  },
  {
    "objectID": "DV_assignment07.html#scatterplot-in-shiny",
    "href": "DV_assignment07.html#scatterplot-in-shiny",
    "title": "Assignment 7",
    "section": "Scatterplot in Shiny",
    "text": "Scatterplot in Shiny\n\n\nScatterplot Visualization: The scatterplot plots data points for each Texas county, where: The x-axis typically represents the voter registration numbers. The y-axis represents voter turnout, allowing users to observe trends and outliers in turnout relative to registration across counties.\nInteractive Election Type Filters: The app includes interactive controls (such as buttons or checkboxes) for filtering the displayed data by election type. This feature enables users to toggle between election types (e.g., presidential, midterm, primary) and focus only on federal elections conducted in Texas from 2016 onward. By selecting or deselecting election types, users can observe differences in turnout and registration for each election context.\nDynamic Plot Updates: When the user adjusts the filters, the scatterplot updates dynamically, enabling a comparison of turnout versus registration across different election types and timeframes. This feature provides a deeper look into how different federal elections might impact voter behavior at the county level.\nThe interactive nature of the scatterplot, combined with election-type filters, allows for a user-friendly exploration of turnout and registration patterns over time across Texas counties.",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 7"
    ]
  },
  {
    "objectID": "DV_assignment07.html#bubble-chart-in-shiny",
    "href": "DV_assignment07.html#bubble-chart-in-shiny",
    "title": "Assignment 7",
    "section": "Bubble Chart in Shiny",
    "text": "Bubble Chart in Shiny\n\n\nThe Shiny app features an interactive bubble chart that compares voter registration against voter turnout across Texas counties. In this visualization:\n\nX-Axis: Represents the voter registration rate for each county.\nY-Axis: Represents the voter turnout rate, allowing for a quick comparison of how many registered voters actually turned out in each county.\nBubble Size: Reflects the percentage of the Black population in each county. Larger bubbles indicate a higher proportion of Black residents within that county.\nUser Controls: Users can select specific Texas counties and adjust parameters to display rates of registered voters, turnout, and Black population percentage.\n\nThis design allows users to investigate trends by demographic and region, revealing patterns in voter registration and turnout in relation to the Black population distribution across Texas counties. The adjustable inputs provide a tailored look at each county‚Äôs data, enabling insights into which counties may benefit from targeted voter engagement strategies.",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 7"
    ]
  },
  {
    "objectID": "DV_assignment05.html",
    "href": "DV_assignment05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "1. Using sample datasets or own data, create the following charts using only R graphics\nfunctions (i.e.¬†without using any other packages). Be sure you customize the chart with your own style/theme (e.g.¬†font, color, pch, etc.)\n\n# Ë®≠ÂÆöÂ∑•‰ΩúÁõÆÈåÑ\n#setwd(\"C:/Users/dells/Desktop/quarto/quarto_website/UTD_SDAR\")\n\n# ÂåØÂÖ• CSV Êñá‰ª∂\nTWBC2021 &lt;- read.csv(\"TWBC2021.csv\")\n\n# Ê™¢Ë¶ñË≥áÊñôË°®Ê†º\nView(TWBC2021)\n\na. Histogram\n\n# Histogram\npar(col=\"gray50\", fg=\"gray30\", col.axis=\"gray50\")\nhist(TWBC2021$A_Age, breaks = 10, \n     col = \"gray80\", freq = FALSE, \n     main = \"Histogram of Averge Incidence Age of\\n Female Breast Cancer with Normal Distribution\", \n     xlab = \"Averge Incidence Age\", \n     cex.lab = 1.2) \nmean_hpi &lt;- mean(TWBC2021$A_Age, na.rm = TRUE)\nsd_hpi &lt;- sd(TWBC2021$A_Age, na.rm = TRUE)\nx &lt;- seq(min(TWBC2021$A_Age, na.rm = TRUE), max(TWBC2021$A_Age, na.rm = TRUE), length = 100)\ndn &lt;- dnorm(x, mean = mean_hpi, sd = sd_hpi)\nlines(x, dn, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nb. Barchart\ni. Vertical\n\n# Barchart(Vertical)\nsorted_data &lt;- TWBC2021[order(TWBC2021$N_Incidence), ]\n\npar(mar=c(6, 5.5, 3, 2))\nbarplot(\n  sorted_data$N_Incidence, \n  names.arg = sorted_data$City, \n  las = 2,              # XËª∏Ê®ôÁ±§ÊóãËΩâ\n  col = \"lightblue\",     # Ê¢ùÂΩ¢È°èËâ≤\n  main = \"Number of Incidence by City/County\",\n  ylab = \"Number of Incidence\",\n  ylim = c(0, 3000),\n  cex.names = 0.6,\n  cex.lab = 0.8,         # Ë™øÊï¥YËª∏Ê®ôÁ±§ÔºàylabÔºâÁöÑÂ≠óÈ´îÂ§ßÂ∞è\n  cex.axis = 0.7\n)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nii. Horizonal\n\n# Barchart(Horizonal)\npar(mar=c(4.5, 6, 3, 2.1)) \nbarplot(\n  sorted_data$N_Incidence, \n  names.arg = sorted_data$City, \n  las = 1,              # XËª∏Ê®ôÁ±§ÊóãËΩâ\n  col = \"lightblue\",     # Ê¢ùÂΩ¢È°èËâ≤\n  main = \"Number of Incidence by City/County\",\n  xlab = \"Number of Incidence\",  # XËª∏Ê®ôÁ±§ÊîπÁÇ∫Ë°®Á§∫ N_Incidence\n  horiz = TRUE,          # Ë®≠ÁΩÆÊ¢ùÂΩ¢ÂúñÁÇ∫Ê©´Âêë\n  cex.names = 0.55,\n  cex.lab = 0.8,         # Ë™øÊï¥XËª∏Ê®ôÁ±§ÔºàxlabÔºâÁöÑÂ≠óÈ´îÂ§ßÂ∞è\n  cex.axis = 0.7,\n  xlim = c(0, 3000)      # Ë®≠ÁΩÆXËª∏ÁØÑÂúç\n)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nc. Piechart\n\n# Piechart\npar(mar=c(1, 1.5, 3, 1.5), lwd=1)\nregion_counts &lt;- table(TWBC2021$Region)\ncolors &lt;- c(\"darkorchid2\", \"limegreen\", \"firebrick2\", \"darkorange2\", \"royalblue\")\npie(region_counts, \n    labels = paste(names(region_counts)), \n    main = \"Distribution of Regions in the Dataset\", \n    col = colors)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\nd. Boxplot\n\n# Boxplot\npar(mar=c(4.5, 6, 3, 2.1),las = 1) \nboxplot(\n  A_Age ~ Region,         # YËª∏ÊòØN_IncidenceÔºåXËª∏ÊòØDivisions\n  data = TWBC2021,                     # Êï∏Êìö‰æÜÊ∫ê\n  main = \"Boxplot of N_Incidence by Divisions\",  # ÂúñË°®Ê®ôÈ°å\n  xlab = \"Region\",              # XËª∏Ê®ôÁ±§\n  ylab = \"Averge Incidence Age\" ,            # YËª∏Ê®ôÁ±§\n  cex.axis = 0.6\n)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\ne. Scatterplot\n\n# Scatterplot\npar(las=1, mar=c(6, 6, 4, 4), cex= .7) \nplot.new()\nplot.window(range(TWBC2021$N_Incidence, na.rm = TRUE), range(TWBC2021$N_Deaths, na.rm = TRUE))\nTWBC2021 &lt;- TWBC2021[order(TWBC2021$N_Incidence), ]\nlines(TWBC2021$N_Incidence, TWBC2021$N_Deaths, col=\"gray50\")\npoints(TWBC2021$N_Incidence, TWBC2021$N_Deaths, pch=21, bg=\"white\", cex=1) \npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\nbox(bty=\"o\")\naxis(1, at = NULL, labels = TRUE) \naxis(2, at = seq(0, 500, 100))\naxis(4, at = seq(0, 500, 100))\nmtext(\"Number of Incidence\", side=1, line=3, outer=FALSE, col=\"black\", cex=0.8)\nmtext(\"Number of Deaths\", side=2, line=3, las=0, outer=FALSE, col=\"black\", cex=0.8)\ntext(750, 400, \"Number of Incidence\\n vs Deaths\")\ntitle(main = \"Number of Incidence versus Number of Deaths\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n\n2. Repeat 1 using ggplot2, with your own style.\n\n# Ë®≠ÁΩÆ CRAN Èè°ÂÉè\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\n# ÂÆâË£ù ggplot2ÔºàËã•Â∞öÊú™ÂÆâË£ùÔºâ\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\n# ËºâÂÖ• ggplot2 Â•ó‰ª∂\nlibrary(ggplot2)\n\n# ÂÆâË£ù dplyrÔºàËã•Â∞öÊú™ÂÆâË£ùÔºâ\nif (!requireNamespace(\"dplyr\", quietly = TRUE)) {\n  install.packages(\"dplyr\")\n}\n\n\n# ËºâÂÖ• dplyr Â•ó‰ª∂\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.5.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# 1. Histogram\nggplot(TWBC2021, aes(x = A_Age)) +\n  geom_histogram(aes(y = ..density..), bins = 10, fill = \"gray80\", color = \"gray50\") +\n  stat_function(fun = dnorm, args = list(mean = mean(TWBC2021$A_Age, na.rm = TRUE), \n                                         sd = sd(TWBC2021$A_Age, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram of Average Incidence Age of Female Breast Cancer\\nwith Normal Distribution\",\n       x = \"Average Incidence Age\", y = \"Density\") +\n  theme_minimal() + \n  theme(plot.title = element_text(hjust = 0.5))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n# 2. Barchart (Vertical)\nsorted_data &lt;- TWBC2021 %&gt;% arrange(N_Incidence)\nsorted_data &lt;- TWBC2021 %&gt;% arrange(N_Incidence)\nggplot(sorted_data, aes(x = reorder(City, N_Incidence), y = N_Incidence)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Number of Incidence by City/County\", x = \"City\", y = \"Number of Incidence\") +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, size = 8),\n    panel.background = element_rect(fill = \"transparent\", color = NA),  # ËÆìÁπ™ÂúñÂçÄËÉåÊôØÈÄèÊòé\n    plot.background = element_rect(fill = \"transparent\", color = NA),   # ËÆìÊï¥È´îËÉåÊôØÈÄèÊòé\n    panel.grid.major = element_line(color = \"gray90\"),   # ‰øùÁïô‰∏ªË¶ÅÊ†ºÁ∑ö\n    panel.grid.minor = element_line(color = \"gray90\"),   # ‰øùÁïôÊ¨°Ë¶ÅÊ†ºÁ∑ö\n    plot.title = element_text(hjust = 0.5)  # Â∞áÊ®ôÈ°åÁΩÆ‰∏≠\n  )\n\n\n\n\n\n\n\n# 3. Barchart (Horizontal)\nggplot(sorted_data, aes(x = reorder(City, N_Incidence), y = N_Incidence)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Number of Incidence by City/County\", y = \"City\", x = \"Number of Incidence\") +\n  coord_flip() +\n  theme_minimal() + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# 4. Piechart\nregion_counts &lt;- TWBC2021 %&gt;%\n  count(Region)\nggplot(region_counts, aes(x = \"\", y = n, fill = Region)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Distribution of Regions in the Dataset\", x = NULL, y = NULL) +\n  theme_void() +\n  scale_fill_manual(values = c(\"darkorchid2\", \"limegreen\", \"firebrick2\", \"darkorange2\", \"royalblue\")) + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# 5. Boxplot\nggplot(TWBC2021, aes(x = Region, y = A_Age)) +\n  geom_boxplot(fill = \"gray\") +  # Â∞áÁõíÂ≠êÁöÑÈ°èËâ≤Ë®≠ÁÇ∫ÁÅ∞Ëâ≤\n  labs(title = \"Boxplot of Average Incidence Age by Region\", x = \"Region\", y = \"Average Incidence Age\") +\n  theme_minimal() + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# 6. Scatterplot\nggplot(TWBC2021, aes(x = N_Incidence, y = N_Deaths)) +\n  geom_point(shape = 21, fill = \"white\", size = 3) +\n  geom_line(color = \"gray50\") +\n  labs(title = \"Number of Incidence vs Number of Deaths\", x = \"Number of Incidence\", y = \"Number of Deaths\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +  # Â∞áÊ®ôÈ°åÁΩÆ‰∏≠\n  annotate(\"text\", x = 750, y = 400, label = \"Number of Incidence\\n vs Deaths\")\n\n\n\n\n\n\n\n\n\n\n3. Export the charts using different formats such as:\n\n.pdf\n\n\n\n\n\n\n.jpg\n\n.svg\n\n.tiff\n\n.bmp\n\n\nNote the differences in these file format:\nOn the Quarto website, PDF and SVG files have higher image quality, while JPEG and BMP files have lower image quality, and TIFF files cannot be displayed.",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 5"
    ]
  },
  {
    "objectID": "DV_assignment03.html",
    "href": "DV_assignment03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "1. Rerun murrell01.R\n\nChoose one of the six charts and explain how it is configured by adding documentation to the codes.\n\n\n# Scatterplot\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16) # Define the x values\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8) # Define the y1 values\ny2 &lt;- c(4, .8, .5, .45, .4, .3) # Define the y2 values\n\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) # las=1: Make axis labels always horizontal\n# mar=c(4, 4, 2, 4): Set margins (bottom, left, top, right)\n# cex=0.7: Set the text size to 70% of the default size\n\nplot.new() # Create a new blank plot\nplot.window(range(x), c(0, 6)) # Set up the plotting window with defined x and y ranges\n# range(x): Set the range of x-axis based on x values\n# c(0, 6): Set the y-axis range from 0 to 6\n\nlines(x, y1) # Draw a line connecting the points defined by x and y1 values\nlines(x, y2) # Draw a line connecting the points defined by x and y2 values\n\npoints(x, y1, pch=16, cex=2) # Plot points for the first set of data (x, y1) using solid circles (pch=16)\n# cex=2: Set the point size to twice the default size\npoints(x, y2, pch=21, bg=\"white\", cex=2) # Plot points for the second set of data (x, y2) using open circles with a white fill (pch=21, bg=\"white\")\n# cex=2: Set the point size to twice the default size\n\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\") # Set the color for lines, text, and axes to gray50\n\naxis(1, at=seq(0, 16, 4)) # Draw the x-axis with ticks at positions 0, 4, 8, 12, and 16\n# The first number in the sequence (0) stands for the starting point of the axis ticks\naxis(2, at=seq(0, 6, 2)) # Draw the left y-axis with ticks at positions 0, 2, 4, and 6\naxis(4, at=seq(0, 6, 2)) # Draw the right y-axis with ticks at positions 0, 2, 4, and 6\n\nbox(bty=\"u\") # Draw a box around the plot with only the top and bottom borders\n\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8) # Add x-axis label: \"Travel Time (s)\" at the bottom (side=1)\n# line=2: Position the label two lines away from the axis\n# cex=0.8: Set the text size to 80% of the default\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8) # Add left y-axis label: \"Responses per Travel\" (side=2)\n# line=2: Position the label two lines away from the axis\n# las=0: Make the text parallel to the axis\n# cex=0.8: Set the text size to 80% of the default\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8) # Add right y-axis label: \"Responses per Second\" (side=4)\n# line=2: Position the label two lines away from the axis\n# las=0: Make the text parallel to the axis\n# cex=0.8: Set the text size to 80% of the default\n\ntext(4, 5, \"Bird 131\") # Add text label \"Bird 131\" at coordinates (x=4, y=5)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\") # Restore the default graphical parameters\n# mar=c(5.1, 4.1, 4.1, 2.1): Set margins back to default values\n# col=\"black\", fg=\"black\", col.axis=\"black\": Reset colors to black\n\n\n\n2. Rerun anscombe01.R (in Teams folder)\n\nCompare the regression models\nCompare different ways to create the plots (e.g.¬†changing colors, line types, plot characters)\n\n\ndata(anscombe)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\nThe image depicts Anscombe‚Äôs quartet, a set of four different datasets that are constructed to have nearly identical simple descriptive statistics (e.g., mean, variance, correlation, and linear regression line) but very different distributions and patterns.\nComparison of the Regression Models:\n\n\n\n\n\n\n\n\n\n\n\nTop Left (Dataset 1)\nTop Right (Dataset 2)\nBottom Left (Dataset 3)\nBottom Right (Dataset 4)\n\n\n\n\nFeatures of data\nThe data points form a roughly linear relationship.\nThe data points form a nonlinear pattern, resembling a curve.\nThere is a strong linear relationship, but one point is an outlier.\nAll the points, except one, are nearly identical, with one extreme outlier on the x-axis.\n\n\nThe relationship between data and regression line\nThe data points fit well with the regression line.\nThough computed to fit linearly, the regression line does not capture this nonlinear relationship well.\nThis single outlier‚Äôs presence significantly affects the regression line, shifting it away from the expected fit.\nThe regression line suggests a relationship, but the x-values‚Äô lack of variation means the model is not reliable.\n\n\nThe degree of agreement between the data and the regression line\nThis dataset behaves as expected, showing a good linear fit with minimal deviation.\nThis illustrates that linear regression may not always be the best fit for curved data.\nThis demonstrates the sensitivity of linear regression to outliers.\nThis illustrates how a single influential point can distort a regression analysis.\n\n\n\nEven though all four datasets have similar linear regression results (same slope, intercept, and R-squared values), their visual patterns are significantly different. This highlights the importance of visualizing data before relying on statistical models, as summary statistics alone can be misleading.\n\n\n3. Can you finetune the charts without using other packages (consult RGraphics by Murrell)?\n\nUse a serif font\nTry non-default colors\nUse own plotting character\n\n\n# Scatterplot with refined elements\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Set up graphical parameters\npar(family=\"serif\", las=1, mar=c(4, 4, 2, 4), cex=0.7) \n\n# Start a new plot\nplot.new()\nplot.window(range(x), c(0, 6))\n\n# Draw lines for y1 and y2 with custom colors\nlines(x, y1, col=\"blue\", lwd=2)\nlines(x, y2, col=\"orangered3\", lwd=2)\n\n# Use custom plotting characters and colors for points\npoints(x, y1, pch=17, cex=2, col=\"darkblue\")   # pch=17: filled triangle\npoints(x, y2, pch=15, cex=2, col=\"orange\")     # pch=15: filled square\n\n# Customize axis colors and text\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4), col.axis=\"forestgreen\", lwd=2)  # Custom x-axis color\naxis(2, at=seq(0, 6, 2), col.axis=\"steelblue\", lwd=2)   # Custom left y-axis color\naxis(4, at=seq(0, 6, 2), col.axis=\"darkorange\", lwd=2)  # Custom right y-axis color\n\n# Add a customized box with specific line width\nbox(bty=\"u\", lwd=2)\n\n# Add axis labels with custom color and size\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8, col=\"darkgreen\")\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8, col=\"navy\")\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8, col=\"orangered2\")\n\n# Add a text annotation\ntext(4, 5, \"Bird 131\", col=\"deeppink2\", font=3, cex=2)  # font=3: Italic text\n\n\n\n\n\n\n\n# Reset to default settings\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n\n4. How about with ggplot2? (use tidyverse package)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Prepare data frame\ndata &lt;- data.frame(\n  x = c(0.5, 2, 4, 8, 12, 16),\n  y1 = c(1, 1.3, 1.9, 3.4, 3.9, 4.8),\n  y2 = c(4, 0.8, 0.5, 0.45, 0.4, 0.3)\n)\n\n# Create charts using ggplot\nggplot(data, aes(x = x)) +\n  \n  # Draw y1 polyline and points\n  geom_line(aes(y = y1), color = \"blue\", linewidth = 1) +  # ‰ΩøÁî® linewidth ÊõøÊèõ size\n  geom_point(aes(y = y1), shape = 17, size = 3, color = \"darkblue\") +\n  \n  # Draw y2 polyline and points\n  geom_line(aes(y = y2), color = \"orangered3\", linewidth = 1) +  # ‰ΩøÁî® linewidth ÊõøÊèõ size\n  geom_point(aes(y = y2), shape = 15, size = 3, color = \"orange\") +\n  \n  # Customize x- and y-axis labels and colors\n  scale_x_continuous(name = \"Travel Time (s)\", breaks = seq(0, 16, 4), limits = c(0, 16), \n                     labels = seq(0, 16, 4), expand = c(0, 0)) +\n  scale_y_continuous(\n    name = \"Responses per Travel\",\n    sec.axis = sec_axis(~ ., name = \"Responses per Second\", breaks = seq(0, 6, 2))\n  ) +\n  \n  # Annotate text\n  annotate(\"text\", x = 4, y = 5, label = \"Bird 131\", color = \"deeppink2\", family = \"serif\", fontface = \"italic\", size = 6) +\n  \n  # Theme customization, adjust axis and text color\n  theme_minimal(base_family = \"serif\") +\n  theme(\n    axis.title.x = element_text(size = 12, color = \"darkgreen\"),\n    axis.title.y = element_text(size = 12, color = \"navy\"),\n    axis.text.x = element_text(color = \"forestgreen\"),\n    axis.text.y = element_text(color = \"steelblue\"),\n    axis.text.y.right = element_text(color = \"orangered2\"),\n    axis.title.y.right = element_text(size = 12, color = \"orangered2\"),\n    panel.border = element_blank(),\n    axis.line = element_line(color = \"gray50\", linewidth = 1)  # ‰ΩøÁî® linewidth ÊõøÊèõ size\n  )\n\n\n\n\n\n\n\n\n\n\n5. Pre-hackathon by team:\n\nTeam work: Replicate the Scatterplot matrix below (hint: Acquire data using the following codes)\nSend the codes to the TA. The first team delivering the code and chart will win a prize (by time stamp and product)\n\nDownload COVID data from OWID GitHub owidall = read.csv(‚Äúhttps://github.com/owid/covid-19- data/blob/master/public/data/owid-covid-data.csv?raw=true‚Äù) # Deselect cases/rows with OWID owidall = owidall[!grepl(‚Äú^OWID‚Äù, owidall$iso_code), ] # Subset by continent: Europe owideu = subset(owidall, continent==‚ÄúEurope‚Äù)\n\n#ÂåØÂÖ•/ËÆÄÂèñowideu.xlsx\nlibrary(readxl)\nowideu_data &lt;- read_excel(\"owideu.xlsx\")\n\n#Ê™¢Êü•owideu_dataÊ™îÊ°à\nhead(owideu_data)\n\n# A tibble: 6 √ó 67\n  iso_code continent location date      total_cases new_cases new_cases_smoothed\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1 ALB      Europe    Albania  2020-01-‚Ä¶           0         0                 NA\n2 ALB      Europe    Albania  2020-01-‚Ä¶           0         0                 NA\n3 ALB      Europe    Albania  2020-01-‚Ä¶           0         0                 NA\n4 ALB      Europe    Albania  2020-01-‚Ä¶           0         0                 NA\n5 ALB      Europe    Albania  2020-01-‚Ä¶           0         0                 NA\n6 ALB      Europe    Albania  2020-01-‚Ä¶           0         0                  0\n# ‚Ñπ 60 more variables: total_deaths &lt;dbl&gt;, new_deaths &lt;dbl&gt;,\n#   new_deaths_smoothed &lt;dbl&gt;, total_cases_per_million &lt;dbl&gt;,\n#   new_cases_per_million &lt;dbl&gt;, new_cases_smoothed_per_million &lt;dbl&gt;,\n#   total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;,\n#   new_deaths_smoothed_per_million &lt;dbl&gt;, reproduction_rate &lt;dbl&gt;,\n#   icu_patients &lt;lgl&gt;, icu_patients_per_million &lt;lgl&gt;, hosp_patients &lt;lgl&gt;,\n#   hosp_patients_per_million &lt;lgl&gt;, weekly_icu_admissions &lt;lgl&gt;, ‚Ä¶\n\nView(owideu_data)\n\n#ÊèêÂèñowideu_data‰∏≠ÁöÑdateÊ¨Ñ‰Ωç‰∏¶ËΩâÊèõÊàêÊó•ÊúüÊ†ºÂºè\ndate_vector &lt;- as.Date(owideu_data$date) \n\n\nlength(date_vector)\n\n[1] 84123\n\nnrow(owideu_data)     # Ê™¢Êü• owideu_data Ë≥áÊñôÊ°ÜÁöÑË°åÊï∏\n\n[1] 84123\n\n#Èï∑Â∫¶ÂíåË°åÊï∏Ë¶Å‰∏ÄÊ®£ÊâçËÉΩÂêà‰Ωµ\n\n#‰ΩøÁî® mutate() ‰æÜÊñ∞Â¢ûËÆäÊï∏\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.5.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nowideu_data &lt;- owideu_data %&gt;%\n  mutate(date_vector = date_vector)\n\n\n#ÂåØÂá∫owideu_datatÊàêexeclÊ™î\n# install.packages(\"writexl\")\n# library(writexl)\n# write_xlsx(owideu_data, \"owideu_data.xlsx\")\n\n#x= owideu_data$date_vector\n#y= owideu_data$new_deaths\n\n# Ë®≠ÂÆöÊ®ôÁ±§ÊñπÂêë„ÄÅÈÇäË∑ùËàáÊñáÂ≠óÂ§ßÂ∞è\npar(family=\"serif\", las=1, mar=c(5, 5, 2, 2), cex=.8) \nplot.new()\n\n# ‰ΩøÁî®Êó•ÊúüÊ†ºÂºèÁπ™ÂúñÁØÑÂúç\nplot.window(range(owideu_data$date_vector), range(owideu_data$new_deaths, na.rm=TRUE))\n\n# Áπ™Ë£ΩË≥áÊñôÈªû\npoints(owideu_data$date_vector, owideu_data$new_deaths, pch=16, col=\"deeppink2\", cex=0.6)\n\n# Áπ™Ë£Ω x Ëª∏Âíå y Ëª∏\naxis(1, at=seq(min(owideu_data$date_vector), max(owideu_data$date_vector), by=\"2 months\"), las=2, cex.axis=0.8, labels=format(seq(min(owideu_data$date_vector), max(owideu_data$date_vector), by=\"2 months\"), \"%Y-%m\"))\naxis(2, at=seq(0, 10000, 2000), las=2, cex.axis=0.8)\n\n# Áπ™Ë£ΩÂúñÊ°Ü\nbox(bty=\"o\")\n\n# Ê∑ªÂä†Ê®ôÁ±§\nmtext(\"Date\", side=1, line=3.5, cex=1)  # x Ëª∏Ê®ôÁ±§\nmtext(\"COVID Deaths in Europe (Daily)\", side=2, line=3, las=0, cex=1)  # y Ëª∏Ê®ôÁ±§\n\n# Ê∑ªÂä†ÂúãÂÆ∂Ê®ôË®ª (Ê†πÊìöËßÄÂØüÈªûÁöÑ‰ΩçÁΩÆÊâãÂãïË®≠ÂÆö)\ntext(as.Date(\"2020-04-01\"), 5800, \"Spain\", cex=0.8)\ntext(as.Date(\"2020-04-15\"), 4800, \"Spain\", cex=0.8)\ntext(as.Date(\"2020-12-01\"), 6600, \"Germany\", cex=0.8)\ntext(as.Date(\"2021-10-15\"), 4500, \"Ukraine\", cex=0.8)\ntext(as.Date(\"2022-12-15\"), 1300, \"Germany\", cex=0.8)\ntext(as.Date(\"2023-09-01\"), 300, \"Italy\", cex=0.8)\n\n\n\n\n\n\n\n# ÊÅ¢Âæ©È†êË®≠Áπ™ÂúñÂèÉÊï∏\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DV_assignment01.html",
    "href": "DV_assignment01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1. Try Anscombe‚Äôs examples\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\n\n\n2. Google ‚Äúgenerative art‚Äù. Cite some examples.\n\nAuthor: Hieroglyphica (aka Jason Sholl)Source: At the Forefront of Generative Art with Hieroglyphica\n Source: Generative Artists Club ‚Äì GenArtClub\n Author: James Pricer, DNA Data Portrait of undefined a CoderSource: New Austin Gallery Will Focus on Generative Art\n\n\n\n\nAuthor: Hao Hua\nSource: Generative Art\n\n\n\n\n3. Run Fall.R\n\nGive your own colors (e.g.¬†Winter).\nExport the file and post on your GitHub website.\n\nset color=‚Äú#56B4E9‚Äù\n\n# Ë®≠ÁΩÆ CRAN Èè°ÂÉè\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# ÂÆâË£ùÊâÄÈúÄÁöÑÂ•ó‰ª∂\nif (!require(\"gsubfn\")) install.packages(\"gsubfn\")\n\nLoading required package: gsubfn\n\n\nLoading required package: proto\n\nif (!require(\"proto\")) install.packages(\"proto\")\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\nWarning: package 'tidyverse' was built under R version 4.5.2\n\n\nWarning: package 'dplyr' was built under R version 4.5.2\n\n\nWarning: package 'lubridate' was built under R version 4.5.2\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Data Visualization\n## Objective: Create graphics with R\n## Title: Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\ninstall.packages(c(\"gsubfn\", \"proto\", \"tidyverse\"))\n\nWarning: packages 'gsubfn', 'proto', 'tidyverse' are in use and will not be\ninstalled\n\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n  \n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"#56B4E9\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\n\n\n\n\n\n4. Write a critique on a chart in published work (book/article/news website)s\n\n\n\n\n\n\nSource:¬†ËÇ∫ÁôåÊàêÊñ∞ÂúãÁóÖ ÂêÑÊúüÊï∏Ê≤ªÁôÇÊàêÊïàÂ§ß‰∏çÂêåÔºÅÂ∞àÂÆ∂ÔºöÊó©ÊúüÁôºÁèæË°ìÂæåÂ≠òÊ¥ªÁéáÈÄæ 9 Êàê\n\nReview:\nThis bar chart shows Taiwan‚Äôs five-year survival rate (blue) and prevalence rate (orange) of lung cancer in 2014. For public health practitioners, this bar chart highlights the sharp decline in five-year survival rates as the lung cancer stage increases, while conversely, the number of patients increases significantly. It reveals the importance of cancer screening (early detection, early treatment) and the unfortunate fact that most people wait until the last stage to be diagnosed. This chart can be used for health education. On the other hand, although the configuration of the bar chart is readable, it is difficult for laypeople without a medical background to understand the relationship between the change in the five-year survival rate (blue) and the change in the proportion of patients (orange).\n\n\n \nSource:¬†Âè∞ÁÅ£‰π≥ÁôåÁôºÁîüÁéáÁõ£Ê∏¨\n\nReview:\nThis chart illustrates the correlation between breast cancer incidence and age by birth generation in Taiwan and the United States. Different colors denote different birth generations; additionally, on the original webpage, left-clicking each age group will prompt an explanation of the corresponding data analysis results (see the second image for details). These two features assist readers in comprehending the chart‚Äôs significance. Charts for Taiwan and the United States are also presented simultaneously, enabling readers to compare the disparities between the two countries effortlessly. This chart demonstrates that in Taiwan, later generations have a higher rate of breast cancer at the same age, while this phenomenon is not observed in the United States.",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 1"
    ]
  },
  {
    "objectID": "DC_assignment05.html",
    "href": "DC_assignment05.html",
    "title": "Assignment 5 - Government data API",
    "section": "",
    "text": "# Load library\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.5.2\n\n\nWarning: package 'dplyr' was built under R version 4.5.2\n\n\nWarning: package 'lubridate' was built under R version 4.5.2\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import CSV file\ngovfiles &lt;- read.csv(\"govinfo-search-results-2025-12-04T00_41_54.csv\")\n\nWarning in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,\n: embedded nul(s) found in input\n\n# Show first 10 rows with selected columns\ngovfiles %&gt;%\n  select(index, title, collection, publishdate) %&gt;%\n  head(10)\n\n   index                                           title           collection\n1      1          Daily Digest/Senate Committee Meetings Congressional Record\n2      2  Daily Digest/COMMITTEE MEETINGS FOR 2025-12-03 Congressional Record\n3      3                         ARMS SALES NOTIFICATION Congressional Record\n4      4                         ARMS SALES NOTIFICATION Congressional Record\n5      5                         ARMS SALES NOTIFICATION Congressional Record\n6      6                         ARMS SALES NOTIFICATION Congressional Record\n7      7                         ARMS SALES NOTIFICATION Congressional Record\n8      8                         ARMS SALES NOTIFICATION Congressional Record\n9      9                               MEASURES REFERRED Congressional Record\n10    10 SUBMISSION OF CONCURRENT AND SENATE RESOLUTIONS Congressional Record\n   publishdate\n1   2025-12-02\n2   2025-12-02\n3   2025-12-02\n4   2025-12-02\n5   2025-12-02\n6   2025-12-02\n7   2025-12-02\n8   2025-12-02\n9   2025-12-02\n10  2025-12-02\n\n\n\n\n\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Modified to download 10 most recent documents from Foreign Relations Committee\n\n# Start with a clean plate and lean loading to save memory\ngc(reset=T)\n\n# install.packages(c(\"purrr\", \"magrittr\", \"readr\"))\nlibrary(purrr)\nlibrary(magrittr)\nlibrary(readr)\n\n## ============ RELATIVE PATHS (for RStudio Project) ============\n\n# Path to your downloaded CSV file (put it in UTD_SDAR folder)\ncsv_file_path &lt;- \"govinfo-search-results-2025-12-04T00_41_54.csv\"\n\n# Directory to save the downloaded PDFs\n# This will create a \"govfiles\" folder inside UTD_SDAR\nsave_dir &lt;- \"govfiles/\"\n\n# Create the folder if it doesn't exist\nif (!dir.exists(save_dir)) {\n  dir.create(save_dir)\n  cat(\"Created folder:\", save_dir, \"\\n\")\n}\n\n# ============================================================\n\n## Read CSV file (skip first 2 rows as header info)\ngovfiles &lt;- read.csv(file = csv_file_path, skip = 0)\n\n# Check the data\nhead(govfiles)\ncolnames(govfiles)\n\n# Get PDF links and IDs\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$index\n\n# Check how many files are available\ncat(\"Total files available:\", length(pdf_govfiles_url), \"\\n\")\n\n# Show first 10 PDF links\ncat(\"\\nFirst 10 PDF links to download:\\n\")\nhead(pdf_govfiles_url, 10)\n\n# Function to download PDFs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\")\n    Sys.sleep(runif(1, 1, 3))  # Random sleep 1-3 seconds to avoid being blocked\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# ============ DOWNLOAD 10 MOST RECENT DOCUMENTS ============\n\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads - 10 most recent documents from Foreign Relations Committee\")\n\n# Download only the first 10 documents\nresults &lt;- 1:10 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n\nmessage(\"Finished downloads\")\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\n\n# Print results\ncat(\"\\n========== DOWNLOAD RESULTS ==========\\n\")\nprint(results)\ncat(\"\\nTime taken:\", time.taken, \"\\n\")\ncat(\"Files saved to:\", save_dir, \"\\n\")",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 5"
    ]
  },
  {
    "objectID": "DC_assignment05.html#a.-download-ten-most-recent-documents-from-foreign-relations-committee-using-govtdata01.r",
    "href": "DC_assignment05.html#a.-download-ten-most-recent-documents-from-foreign-relations-committee-using-govtdata01.r",
    "title": "Assignment 5 - Government data API",
    "section": "",
    "text": "## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Modified to download 10 most recent documents from Foreign Relations Committee\n\n# Start with a clean plate and lean loading to save memory\ngc(reset=T)\n\n# install.packages(c(\"purrr\", \"magrittr\", \"readr\"))\nlibrary(purrr)\nlibrary(magrittr)\nlibrary(readr)\n\n## ============ RELATIVE PATHS (for RStudio Project) ============\n\n# Path to your downloaded CSV file (put it in UTD_SDAR folder)\ncsv_file_path &lt;- \"govinfo-search-results-2025-12-04T00_41_54.csv\"\n\n# Directory to save the downloaded PDFs\n# This will create a \"govfiles\" folder inside UTD_SDAR\nsave_dir &lt;- \"govfiles/\"\n\n# Create the folder if it doesn't exist\nif (!dir.exists(save_dir)) {\n  dir.create(save_dir)\n  cat(\"Created folder:\", save_dir, \"\\n\")\n}\n\n# ============================================================\n\n## Read CSV file (skip first 2 rows as header info)\ngovfiles &lt;- read.csv(file = csv_file_path, skip = 0)\n\n# Check the data\nhead(govfiles)\ncolnames(govfiles)\n\n# Get PDF links and IDs\npdf_govfiles_url &lt;- govfiles$pdfLink\npdf_govfiles_id &lt;- govfiles$index\n\n# Check how many files are available\ncat(\"Total files available:\", length(pdf_govfiles_url), \"\\n\")\n\n# Show first 10 PDF links\ncat(\"\\nFirst 10 PDF links to download:\\n\")\nhead(pdf_govfiles_url, 10)\n\n# Function to download PDFs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\")\n    Sys.sleep(runif(1, 1, 3))  # Random sleep 1-3 seconds to avoid being blocked\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# ============ DOWNLOAD 10 MOST RECENT DOCUMENTS ============\n\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads - 10 most recent documents from Foreign Relations Committee\")\n\n# Download only the first 10 documents\nresults &lt;- 1:10 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n\nmessage(\"Finished downloads\")\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\n\n# Print results\ncat(\"\\n========== DOWNLOAD RESULTS ==========\\n\")\nprint(results)\ncat(\"\\nTime taken:\", time.taken, \"\\n\")\ncat(\"Files saved to:\", save_dir, \"\\n\")",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 5"
    ]
  },
  {
    "objectID": "DC_assignment05.html#a.-how-useable-the-scraped-data",
    "href": "DC_assignment05.html#a.-how-useable-the-scraped-data",
    "title": "Assignment 5 - Government data API",
    "section": "a. How useable the scraped data?",
    "text": "a. How useable the scraped data?\nThe scraped data from GovInfo is relatively useable. The CSV file contains structured metadata including document titles, publication dates, PDF links, and collection types. However, there are some limitations:\n\nFile naming: The downloaded PDFs are named generically (govfiles_1.pdf, govfiles_2.pdf), which makes it difficult to identify the content without opening each file.\nMixed content types: The search results include various document types (Congressional Records, Bills, Reports), which may require additional filtering for specific research purposes.\nLarge download size: The default export contains 1000 records, but we only needed 10 documents. There is no option to customize the number of records exported.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 5"
    ]
  },
  {
    "objectID": "DC_assignment05.html#b.-how-to-improve",
    "href": "DC_assignment05.html#b.-how-to-improve",
    "title": "Assignment 5 - Government data API",
    "section": "b. How to improve?",
    "text": "b. How to improve?\n\nBetter file naming: Modify the R script to use meaningful filenames based on document titles or package IDs instead of generic numbers.\nAdd filtering: Include code to filter documents by collection type, date range, or keywords before downloading.\nError handling: Add more robust error handling to track which downloads succeeded or failed.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 5"
    ]
  },
  {
    "objectID": "DC_assignment03.html",
    "href": "DC_assignment03.html",
    "title": "Assignment 3: Mapping Census data",
    "section": "",
    "text": "Use tidycensus to retrieve ACS 2023 5-year estimates, create a tidy dataset, and produce one map and one table with short interpretation.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DC_assignment03.html#setup",
    "href": "DC_assignment03.html#setup",
    "title": "Assignment 3: Mapping Census data",
    "section": "1. Setup",
    "text": "1. Setup\n\nObtain a Census API key (https://api.census.gov/data/key_signup.html).\nIn R: census_api_key(‚ÄúYOUR_KEY‚Äù, install = FALSE).\nInstall and load R packges: tidycensus, tigris, sf, dplyr, ggplot2, readr.\n\n\n# Packages\nlibrary(tidycensus)\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.5.2\n\n\nLinking to GEOS 3.13.1, GDAL 3.11.4, PROJ 9.7.0; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.5.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# 1) API key (uncomment and paste your key)\ncensus_api_key(\"5f1dc6a21e748ab03bc00d3a101977c8c84d3e1c\", install = FALSE)\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DC_assignment03.html#choose-a-geography-and-variables",
    "href": "DC_assignment03.html#choose-a-geography-and-variables",
    "title": "Assignment 3: Mapping Census data",
    "section": "2. Choose a geography and variable(s)",
    "text": "2. Choose a geography and variable(s)\n\nGeography: one of state, county, tract, or block group (within a state).\nVariables: pick at least two ACS variables (e.g., B19013_001 median HH income; B17001_002people below poverty).\nUse load_variables(2023, ‚Äúacs5‚Äù, cache = TRUE) to search codes.\n\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n# View a few example codes\nvars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 √ó 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income ‚Ä¶ tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income ‚Ä¶ tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income ‚Ä¶ tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income ‚Ä¶ tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income ‚Ä¶ tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income ‚Ä¶ tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income ‚Ä¶ tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income ‚Ä¶ tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income ‚Ä¶ tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income ‚Ä¶ tract    \n\n# 3) Parameters (EDIT ME)\nstate_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DC_assignment03.html#download-data-with-geometry",
    "href": "DC_assignment03.html#download-data-with-geometry",
    "title": "Assignment 3: Mapping Census data",
    "section": "3. Download data (with geometry)",
    "text": "3. Download data (with geometry)\n\nUse get_acs() with your geography and variables.\nKeep estimate and moe (margins of error).\n\n\n# 4) Download\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2019-2023 5-year ACS\n\n# 5) Wide format for convenience\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DC_assignment03.html#deliverables",
    "href": "DC_assignment03.html#deliverables",
    "title": "Assignment 3: Mapping Census data",
    "section": "4. Deliverables",
    "text": "4. Deliverables\n\nMap: choropleth of one variable (with a clear legend and title).\nTable: top/bottom 10 areas by the other variable (include MOE).\nInterpret map and table data\n\n\n# 6) Map (edit titles/theme)\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = NA) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Income ‚Äî \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 7) Table (top/bottom by poverty count)\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_poverty)) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_poverty) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83738 xmax: -94.9085 ymax: 33.43045\nGeodetic CRS:  NAD83\n# A tibble: 10 √ó 4\n   NAME                  estimate_poverty moe_poverty                   geometry\n   &lt;chr&gt;                            &lt;dbl&gt;       &lt;dbl&gt;         &lt;MULTIPOLYGON [¬∞]&gt;\n 1 Harris County, Texas            749481       15891 (((-94.97839 29.68365, -9‚Ä¶\n 2 Dallas County, Texas            359950       10475 (((-97.03852 32.56, -97.0‚Ä¶\n 3 Bexar County, Texas             294002        8323 (((-98.80655 29.69071, -9‚Ä¶\n 4 Hidalgo County, Texas           237121        8739 (((-98.58634 26.25824, -9‚Ä¶\n 5 Tarrant County, Texas           229884        7849 (((-97.55053 32.56258, -9‚Ä¶\n 6 El Paso County, Texas           160998        7148 (((-106.6455 31.89867, -1‚Ä¶\n 7 Travis County, Texas            140926        6636 (((-98.15927 30.37665, -9‚Ä¶\n 8 Cameron County, Texas           102583        4345 (((-97.24047 26.41119, -9‚Ä¶\n 9 Collin County, Texas             69846        4614 (((-96.8441 32.98891, -96‚Ä¶\n10 Denton County, Texas             65649        4233 (((-97.39826 32.99996, -9‚Ä¶\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -103.9839 ymin: 26.598 xmax: -97.29015 ymax: 36.05771\nGeodetic CRS:  NAD83\n# A tibble: 10 √ó 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [¬∞]&gt;\n 1 Kenedy County, Texas                  3           5 (((-97.39839 26.86789, -‚Ä¶\n 2 Loving County, Texas                  5           7 (((-103.9839 31.99411, -‚Ä¶\n 3 King County, Texas                   29          26 (((-100.5187 33.83565, -‚Ä¶\n 4 Borden County, Texas                 35          28 (((-101.6913 32.96184, -‚Ä¶\n 5 Sterling County, Texas               37          25 (((-101.267 31.64419, -1‚Ä¶\n 6 Roberts County, Texas                50          35 (((-101.086 35.82337, -1‚Ä¶\n 7 Kent County, Texas                   56          32 (((-101.0389 33.30571, -‚Ä¶\n 8 McMullen County, Texas               57          47 (((-98.80251 28.10305, -‚Ä¶\n 9 Terrell County, Texas                70          46 (((-102.5669 30.28327, -‚Ä¶\n10 Glasscock County, Tex‚Ä¶               91          67 (((-101.7761 32.08693, -‚Ä¶\n\n# 8) Save outputs (optional)\n# readr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")\n\n\nInterpretation\nThe choropleth map of Texas counties illustrates the distribution of median household income (ACS 2023 5-year estimates). The results show a clear geographic divide: counties around major metropolitan areas such as Austin, Dallas, and Houston generally report higher incomes, while counties along the southern border and rural West Texas exhibit much lower household incomes. This highlights substantial urban‚Äìrural and regional disparities in economic well-being across the state.\nThe tables rank counties by the estimated number of people living below the poverty line. The top 10 counties (e.g., Harris, Dallas, Bexar) contain the largest absolute numbers of individuals in poverty. These are the most populous counties, which means that even with relatively high median incomes, the sheer size of the population results in large numbers of residents living in poverty. Conversely, the bottom 10 counties (e.g., Kenedy, Loving, King) report only a handful of people in poverty. However, these counties are extremely small in population, and the margins of error (MOE) are often larger than or close to the estimates themselves, making these figures less reliable.\nIn summary, the map highlights regional disparities in income levels, while the tables reveal that poverty in absolute numbers is concentrated in Texas‚Äôs largest urban counties, and very small counties show unstable estimates due to limited sample sizes.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DC_assignment03.html#post-output-on-website-titled-assignment-3-mapping-census-data",
    "href": "DC_assignment03.html#post-output-on-website-titled-assignment-3-mapping-census-data",
    "title": "Assignment 3: Mapping Census data",
    "section": "5. Post output on website titled Assignment 3: Mapping Census data",
    "text": "5. Post output on website titled Assignment 3: Mapping Census data\nSee this page.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 3"
    ]
  },
  {
    "objectID": "DC_assignment01.html",
    "href": "DC_assignment01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Shiu-Ting‚Äôs website was built using RStudio Quarto. The theme is customized from the default one provided by Quarto. The navigation bar includes links to the website‚Äôs Homepage (Home), the author‚Äôs profile page with CV (about), and the author‚Äôs GitHub (GitHub icon). The sidebar displays assignments from three data-related classes ‚Äì EPPS6356, EPPS6354, & EPPS6302 ‚Äì taught by Dr.¬†Ho at the University of Texas at Dallas.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 1"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Shiu-Ting Ling",
    "section": "",
    "text": "A graduate student enrolled in the third semester of the SDAR program.\n\nCV"
  },
  {
    "objectID": "DataFramed320.html",
    "href": "DataFramed320.html",
    "title": "The Next Industrial Revolution is Industrial AI - Review",
    "section": "",
    "text": "Review\nThe podcast ‚ÄúThe Next Industrial Revolution is Industrial AI‚Äù features a discussion between Barbara Humpton, Siemens USA CEO, and Olympia Brikis, Director of Industrial AI, on AI‚Äôs impact on manufacturing, innovation, productivity, workforce, and the industry‚Äôs future.\nThe talk starts by positioning industrial AI as succeeding earlier revolutions like electrification and automation. Unlike consumer AI, which focuses on convenience, industrial AI handles proprietary data, complex systems, and high-stakes decisions. Humpton and Brikis cite examples of AI transforming operations: in automotive, visual inspection ensures quality; Siemens‚Äô factories use predictive maintenance and automation to free workers for cognitive tasks; Jet Zero‚Äôs aircraft employs Siemens‚Äô generative AI and digital twin tech for efficiency and sustainability.\nBeyond technical advances, speakers highlight the human side of industrial AI. Automation boosts output without job losses, empowering workers with AI copilots for higher-level tasks. They discuss adoption challenges like access to capital and training for small and medium enterprises. Success depends on technology and workers‚Äô openness, curiosity, and engagement with new tools.\nWhat stands out in this discussion is the balance between innovation and inclusivity. Industrial AI is not just shown as a technical upgrade but as a collaborative transformation between people and machines. The examples of night-shift workers relying on AI copilots, or training programs made for new employees, show how AI can reduce stress, improve job satisfaction, and expand access to industrial opportunities.\nIn conclusion, this talk envisions industrial AI as a major driver of progress, highlighting its role in fostering creativity, resilience, and growth alongside efficiency. Success in adopting AI globally depends on balancing innovation with workforce development, ensuring technology empowers rather than excludes. The dialogue between Humpton and Brikis shows that the ‚Äúnext industrial revolution‚Äù will be shaped by human adaptability as much as technological breakthroughs."
  },
  {
    "objectID": "DC_assignment02.html",
    "href": "DC_assignment02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Search ‚ÄúTrump‚Äù, ‚ÄúHarris‚Äù, and ‚Äúelection‚Äù on Google Trend website in (See bellowed plot).\n\n\n\n\n\nDownload the Google Trend data in csv file.\n\n\n\n\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(zoo)        # as.yearmon() converts \"YYYY-MM\" to month-year date format\n\n# 1) Read file: skip the first two header/description rows\nraw &lt;- read_excel(\"TrumpHarrisElection_download.xlsx\",\n                  sheet = \"TrumpHarrisElection\", skip = 2, col_names = FALSE)\nnames(raw) &lt;- c(\"date\",\"Trump\",\"Harris\",\"election\")\n\n# 2) Clean data: keep only rows matching YYYY-MM format, handle \"&lt;1\" values\ndf &lt;- raw %&gt;%\n  mutate(date = str_trim(as.character(date))) %&gt;%     # trim whitespace, ensure string type\n  filter(str_detect(date, \"^\\\\d{4}-\\\\d{2}$\")) %&gt;%     # keep only YYYY-MM format\n  mutate(\n    # Convert YYYY-MM to the first day of each month (Date type)\n    date = as.Date(as.yearmon(date)),\n    # Replace \"&lt;1\" with 0, then convert to numeric\n    across(c(Trump, Harris, election), ~ as.numeric(str_replace(.x, \"&lt;1\", \"0\")))\n  ) %&gt;%\n  arrange(date)\n\n# 3) Get start and end dates\nstart_date &lt;- min(df$date)\nend_date   &lt;- max(df$date)\nstart_date; end_date\n\n[1] \"2004-01-01\"\n\n\n[1] \"2025-09-01\"\n\n\nThe time range of the dataset is from 2004-01-01 to 2025-09-01.\n\n\n\n\n# 4) Time interval (in days), verify if data is monthly\ngap_days &lt;- diff(df$date)\nsummary(gap_days)\n\nTime differences in days\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   30.00   31.00   30.44   31.00   31.00 \n\n# Alternatively, check intervals in months\ngap_months &lt;- diff(as.yearmon(df$date)) * 12\nsummary(gap_months)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1 \n\n\n\nThe data are collected on a monthly basis, with most intervals being 30 or 31 days.\nThe summary shows that the minimum interval is 28 days and the maximum is 31 days, which corresponds to the varying number of days in each month.\n\n\n\n\n\n\n\nSearching ‚ÄúTrump‚Äù,‚ÄúHarris‚Äù, & ‚Äúelection‚Äù on Google Trends\n\n#install.packages(\"gtrendsR\")\n\nlibrary(gtrendsR)\n\n# ---- Query \"Trump\", \"Harris\", \"election\" (all time, all locations) -----\nTrumpHarrisElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\") # Query \"Trump\", \"Harris\", and \"election\" on Google Trends (all time, all locations), store data in TrumpHarrisElection\nplot(TrumpHarrisElection)  # Plot the data stored in TrumpHarrisElection\n\n\n\n\n\n\n\n\nSearching ‚Äútariff‚Äù on Google Trends\n\n# ---- Query \"tariff\" (all time, all locations) -----\nplot(gtrends(c(\"tariff\"), time = \"all\"))  # Plot Google Trends data for \"tariff\" (all time, all locations)\n\n\n\n\n\n\n\n# ---- Query \"tariff\" (all time, specific locations) -----\ndata(\"countries\") # Load Google Trends country codes\nplot(gtrends(c(\"tariff\"), geo = \"GB\", time = \"all\")) # Plot Google Trends data for \"tariff\" in UK (all time)\n\n\n\n\n\n\n\nplot(gtrends(c(\"tariff\"), geo = c(\"US\",\"GB\",\"TW\"), time = \"all\")) # Plot Google Trends data for \"tariff\" in US, UK, Taiwan (all time)\n\n\n\n\n\n\n\n# ---- Query \"tariff\", \"China military\", \"Taiwan\" (all time, all locations) -----\nplot(gtrends(c(\"tariff\",\"China military\", \"Taiwan\"), time = \"all\")) # Plot Google Trends data for \"tariff\", \"China military\", and \"Taiwan\" (all time, all locations)\n\n\n\n\n\n\n\n# ---- Query \"tariff\", \"China military\", \"Taiwan\" - additional code -----\ntct = gtrends(c(\"tariff\",\"China military\", \"Taiwan\"), time = \"all\")  # Query Google Trends for \"tariff\", \"China military\", and \"Taiwan\" (all time, all locations), store data in tct\ntct = data.frame(tct$interest_over_time) # Extract the interest_over_time table from tct list; convert to data.frame format, overwriting the original tct\n\n\n# ---- Supplementary code -----\npar(family=\"Georgia\") # Set chart font to Georgia\ntg = gtrends(\"tariff\", time = \"all\") # Query Google Trends for \"tariff\" (all time), store data in tg\ntg_iot = tg$interest_over_time # Extract the \"interest over time\" time series data and store in tg_iot\ndata(\"categories\") # Load Google Trends category names (e.g., Health)\n\n\n\n\nSave the data into csv\n\n# This code is displayed but not executed\n\n# Extract interest_over_time (search interest time series data)\nTrumpHarrisElection_iot &lt;- TrumpHarrisElection$interest_over_time\n\n# Save as CSV file\nwrite.csv(TrumpHarrisElection_iot, \"TrumpHarrisElection.csv\", row.names = FALSE)\n\nSave the data into R formats\n\n# This code is displayed but not executed\n\n# Save in RData format\nsave(TrumpHarrisElection_iot, file = \"TrumpHarrisElection.RData\")\n\n# Or use RDS (more commonly used)\nsaveRDS(TrumpHarrisElection_iot, \"TrumpHarrisElection.rds\")\n\nRead CSV file\n\n# Read CSV\nTrumpHarrisElection_csv &lt;- read.csv(\"TrumpHarrisElection.csv\")\n\nCheck time range (Dates)\n\nrange(TrumpHarrisElection_csv$date)   # Start and end dates\n\n[1] \"2004-01-01\" \"2025-09-01\"\n\n\nThe time range of the dataset is from 2004-01-01 to 2025-09-01.\nCheck intervals\n\n# Ensure date is in datetime format\nTrumpHarrisElection_csv$date &lt;- as.POSIXct(TrumpHarrisElection_csv$date)\n\n# Extract unique dates only, sorted in ascending order\nuniq_dt &lt;- sort(unique(TrumpHarrisElection_csv$date))\n\n# Calculate intervals between consecutive dates (in days)\ngap_days &lt;- as.numeric(diff(uniq_dt), units = \"days\")\nsummary(gap_days)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   30.00   31.00   30.44   31.00   31.04 \n\n\nThis indicates that the dataset is collected on a monthly basis, where the number of days per interval depends on the calendar month (28, 30, or 31 days).\nView each keyword trend\n\ntable(TrumpHarrisElection_csv$keyword)   # Count of records for each keyword\n\n\nelection   Harris    Trump \n     261      261      261 \n\n\nEach keyword (Trump, Harris, and election) has 261 observations in the dataset.\nIllustrate the comparison of three keywords\n\nlibrary(ggplot2)\n\nggplot(TrumpHarrisElection_csv, aes(x = date, y = as.numeric(hits), color = keyword)) +\n  geom_line() +\n  labs(title = \"Google Trends: Trump vs Harris vs Election\",\n       x = \"Date\", y = \"Search interest (0-100)\") +\n  theme_minimal()\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload data manually\nUse ‚ÄògtrendsR‚Äô script\n\n\n\n\nTime interval\nSelect ‚ÄúPast Day‚Äù; data may be hourly or every 8 minutes. Select ‚ÄúAll Time‚Äù, data is usually monthly (YYYY-MM).\nFreely specify time = ‚Äútoday 1-m‚Äù, ‚Äútoday 12-m‚Äù, ‚Äúall‚Äù, or other date ranges.\n\n\nGeography\nManually switch to a single region (e.g., Global, US).\nSet geo directly in the code as ‚ÄúUS‚Äù, ‚ÄúTW‚Äù, ‚ÄúGB‚Äù. Compare multiple countries at once.\n\n\nData structure\nThe downloaded CSV file only has a few columns, such as ‚ÄúTime‚Äù and ‚ÄúSearch Popularity for Each Keyword‚Äù.\nReturn a list containing multiple data frames:\n‚Ä¢ $interest_over_time (time series)\n‚Ä¢ $interest_by_region (search volume by region)\n‚Ä¢ $related_queries (related search queries)\n‚Ä¢ $related_topics (Related Topics)\n\n\nAdvantages\nEasy to use, ideal for quickly checking trends or making basic comparisons.\nIntegrate processes (automated analysis, archiving, plotting) in R for easy reproducibility and sharing.\n\n\nLimitations\nRequires manual operation and cannot be automatically updated or processed in batches.\nMay encounter Google Trends API rate limits (429 error). If called too frequently, may be temporarily blocked.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DC_assignment02.html#a.-use-google-trends-website-httpstrends.google.comhome-to",
    "href": "DC_assignment02.html#a.-use-google-trends-website-httpstrends.google.comhome-to",
    "title": "Assignment 2",
    "section": "",
    "text": "Search ‚ÄúTrump‚Äù, ‚ÄúHarris‚Äù, and ‚Äúelection‚Äù on Google Trend website in (See bellowed plot).\n\n\n\n\n\nDownload the Google Trend data in csv file.\n\n\n\n\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(zoo)        # as.yearmon() converts \"YYYY-MM\" to month-year date format\n\n# 1) Read file: skip the first two header/description rows\nraw &lt;- read_excel(\"TrumpHarrisElection_download.xlsx\",\n                  sheet = \"TrumpHarrisElection\", skip = 2, col_names = FALSE)\nnames(raw) &lt;- c(\"date\",\"Trump\",\"Harris\",\"election\")\n\n# 2) Clean data: keep only rows matching YYYY-MM format, handle \"&lt;1\" values\ndf &lt;- raw %&gt;%\n  mutate(date = str_trim(as.character(date))) %&gt;%     # trim whitespace, ensure string type\n  filter(str_detect(date, \"^\\\\d{4}-\\\\d{2}$\")) %&gt;%     # keep only YYYY-MM format\n  mutate(\n    # Convert YYYY-MM to the first day of each month (Date type)\n    date = as.Date(as.yearmon(date)),\n    # Replace \"&lt;1\" with 0, then convert to numeric\n    across(c(Trump, Harris, election), ~ as.numeric(str_replace(.x, \"&lt;1\", \"0\")))\n  ) %&gt;%\n  arrange(date)\n\n# 3) Get start and end dates\nstart_date &lt;- min(df$date)\nend_date   &lt;- max(df$date)\nstart_date; end_date\n\n[1] \"2004-01-01\"\n\n\n[1] \"2025-09-01\"\n\n\nThe time range of the dataset is from 2004-01-01 to 2025-09-01.\n\n\n\n\n# 4) Time interval (in days), verify if data is monthly\ngap_days &lt;- diff(df$date)\nsummary(gap_days)\n\nTime differences in days\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   30.00   31.00   30.44   31.00   31.00 \n\n# Alternatively, check intervals in months\ngap_months &lt;- diff(as.yearmon(df$date)) * 12\nsummary(gap_months)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1 \n\n\n\nThe data are collected on a monthly basis, with most intervals being 30 or 31 days.\nThe summary shows that the minimum interval is 28 days and the maximum is 31 days, which corresponds to the varying number of days in each month.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DC_assignment02.html#b.-use-gtrendsr-package-to-collect-data-use-gtrendsr01.r-program",
    "href": "DC_assignment02.html#b.-use-gtrendsr-package-to-collect-data-use-gtrendsr01.r-program",
    "title": "Assignment 2",
    "section": "",
    "text": "Searching ‚ÄúTrump‚Äù,‚ÄúHarris‚Äù, & ‚Äúelection‚Äù on Google Trends\n\n#install.packages(\"gtrendsR\")\n\nlibrary(gtrendsR)\n\n# ---- Query \"Trump\", \"Harris\", \"election\" (all time, all locations) -----\nTrumpHarrisElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\") # Query \"Trump\", \"Harris\", and \"election\" on Google Trends (all time, all locations), store data in TrumpHarrisElection\nplot(TrumpHarrisElection)  # Plot the data stored in TrumpHarrisElection\n\n\n\n\n\n\n\n\nSearching ‚Äútariff‚Äù on Google Trends\n\n# ---- Query \"tariff\" (all time, all locations) -----\nplot(gtrends(c(\"tariff\"), time = \"all\"))  # Plot Google Trends data for \"tariff\" (all time, all locations)\n\n\n\n\n\n\n\n# ---- Query \"tariff\" (all time, specific locations) -----\ndata(\"countries\") # Load Google Trends country codes\nplot(gtrends(c(\"tariff\"), geo = \"GB\", time = \"all\")) # Plot Google Trends data for \"tariff\" in UK (all time)\n\n\n\n\n\n\n\nplot(gtrends(c(\"tariff\"), geo = c(\"US\",\"GB\",\"TW\"), time = \"all\")) # Plot Google Trends data for \"tariff\" in US, UK, Taiwan (all time)\n\n\n\n\n\n\n\n# ---- Query \"tariff\", \"China military\", \"Taiwan\" (all time, all locations) -----\nplot(gtrends(c(\"tariff\",\"China military\", \"Taiwan\"), time = \"all\")) # Plot Google Trends data for \"tariff\", \"China military\", and \"Taiwan\" (all time, all locations)\n\n\n\n\n\n\n\n# ---- Query \"tariff\", \"China military\", \"Taiwan\" - additional code -----\ntct = gtrends(c(\"tariff\",\"China military\", \"Taiwan\"), time = \"all\")  # Query Google Trends for \"tariff\", \"China military\", and \"Taiwan\" (all time, all locations), store data in tct\ntct = data.frame(tct$interest_over_time) # Extract the interest_over_time table from tct list; convert to data.frame format, overwriting the original tct\n\n\n# ---- Supplementary code -----\npar(family=\"Georgia\") # Set chart font to Georgia\ntg = gtrends(\"tariff\", time = \"all\") # Query Google Trends for \"tariff\" (all time), store data in tg\ntg_iot = tg$interest_over_time # Extract the \"interest over time\" time series data and store in tg_iot\ndata(\"categories\") # Load Google Trends category names (e.g., Health)",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DC_assignment02.html#c.-save-the-data-into-csv-and-r-formats.",
    "href": "DC_assignment02.html#c.-save-the-data-into-csv-and-r-formats.",
    "title": "Assignment 2",
    "section": "",
    "text": "Save the data into csv\n\n# This code is displayed but not executed\n\n# Extract interest_over_time (search interest time series data)\nTrumpHarrisElection_iot &lt;- TrumpHarrisElection$interest_over_time\n\n# Save as CSV file\nwrite.csv(TrumpHarrisElection_iot, \"TrumpHarrisElection.csv\", row.names = FALSE)\n\nSave the data into R formats\n\n# This code is displayed but not executed\n\n# Save in RData format\nsave(TrumpHarrisElection_iot, file = \"TrumpHarrisElection.RData\")\n\n# Or use RDS (more commonly used)\nsaveRDS(TrumpHarrisElection_iot, \"TrumpHarrisElection.rds\")\n\nRead CSV file\n\n# Read CSV\nTrumpHarrisElection_csv &lt;- read.csv(\"TrumpHarrisElection.csv\")\n\nCheck time range (Dates)\n\nrange(TrumpHarrisElection_csv$date)   # Start and end dates\n\n[1] \"2004-01-01\" \"2025-09-01\"\n\n\nThe time range of the dataset is from 2004-01-01 to 2025-09-01.\nCheck intervals\n\n# Ensure date is in datetime format\nTrumpHarrisElection_csv$date &lt;- as.POSIXct(TrumpHarrisElection_csv$date)\n\n# Extract unique dates only, sorted in ascending order\nuniq_dt &lt;- sort(unique(TrumpHarrisElection_csv$date))\n\n# Calculate intervals between consecutive dates (in days)\ngap_days &lt;- as.numeric(diff(uniq_dt), units = \"days\")\nsummary(gap_days)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   30.00   31.00   30.44   31.00   31.04 \n\n\nThis indicates that the dataset is collected on a monthly basis, where the number of days per interval depends on the calendar month (28, 30, or 31 days).\nView each keyword trend\n\ntable(TrumpHarrisElection_csv$keyword)   # Count of records for each keyword\n\n\nelection   Harris    Trump \n     261      261      261 \n\n\nEach keyword (Trump, Harris, and election) has 261 observations in the dataset.\nIllustrate the comparison of three keywords\n\nlibrary(ggplot2)\n\nggplot(TrumpHarrisElection_csv, aes(x = date, y = as.numeric(hits), color = keyword)) +\n  geom_line() +\n  labs(title = \"Google Trends: Trump vs Harris vs Election\",\n       x = \"Date\", y = \"Search interest (0-100)\") +\n  theme_minimal()\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DC_assignment02.html#d.-what-are-the-differences-between-the-two-methods",
    "href": "DC_assignment02.html#d.-what-are-the-differences-between-the-two-methods",
    "title": "Assignment 2",
    "section": "",
    "text": "Download data manually\nUse ‚ÄògtrendsR‚Äô script\n\n\n\n\nTime interval\nSelect ‚ÄúPast Day‚Äù; data may be hourly or every 8 minutes. Select ‚ÄúAll Time‚Äù, data is usually monthly (YYYY-MM).\nFreely specify time = ‚Äútoday 1-m‚Äù, ‚Äútoday 12-m‚Äù, ‚Äúall‚Äù, or other date ranges.\n\n\nGeography\nManually switch to a single region (e.g., Global, US).\nSet geo directly in the code as ‚ÄúUS‚Äù, ‚ÄúTW‚Äù, ‚ÄúGB‚Äù. Compare multiple countries at once.\n\n\nData structure\nThe downloaded CSV file only has a few columns, such as ‚ÄúTime‚Äù and ‚ÄúSearch Popularity for Each Keyword‚Äù.\nReturn a list containing multiple data frames:\n‚Ä¢ $interest_over_time (time series)\n‚Ä¢ $interest_by_region (search volume by region)\n‚Ä¢ $related_queries (related search queries)\n‚Ä¢ $related_topics (Related Topics)\n\n\nAdvantages\nEasy to use, ideal for quickly checking trends or making basic comparisons.\nIntegrate processes (automated analysis, archiving, plotting) in R for easy reproducibility and sharing.\n\n\nLimitations\nRequires manual operation and cannot be automatically updated or processed in batches.\nMay encounter Google Trends API rate limits (429 error). If called too frequently, may be temporarily blocked.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DC_assignment04.html",
    "href": "DC_assignment04.html",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "## Workshop: Scraping webpages with R rvest package\n# Prerequisites: Chrome browser, Selector Gadget (https://selectorgadget.com/)\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.5.2\n\n\nWarning: package 'dplyr' was built under R version 4.5.2\n\n\nWarning: package 'lubridate' was built under R version 4.5.2\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n#Reading the HTML code from the Wiki website\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\"    \n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for &lt;table class=....&gt; tag. Leave the table close\n## Right click the table and Copy XPath, paste at html_nodes(xpath =)\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\nclass(foreignreserve)\n\n[1] \"list\"\n\nfores = foreignreserve[[1]]\n\n# Check the structure of the fores data frame\n# View(fores)\nhead(fores)\n\n# A tibble: 6 √ó 8\n  Country(as recognize‚Ä¶¬π Continent Foreign exchange res‚Ä¶¬≤ Foreign exchange res‚Ä¶¬≥\n  &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;                  &lt;chr&gt;                 \n1 Country(as recognized‚Ä¶ Continent Including gold         Including gold        \n2 Country(as recognized‚Ä¶ Continent millions U.S.$         Change                \n3 China                  Asia      3,643,149              41,079                \n4 Japan                  Asia      1,324,210              19,774                \n5 Switzerland            Europe    1,007,710              13,935                \n6 Russia                 Europe/A‚Ä¶ 734,100                14,300                \n# ‚Ñπ abbreviated names: ¬π‚Äã`Country(as recognized by the U.N.)`,\n#   ¬≤‚Äã`Foreign exchange reserves`, ¬≥‚Äã`Foreign exchange reserves`\n# ‚Ñπ 4 more variables: `Foreign exchange reserves` &lt;chr&gt;,\n#   `Foreign exchange reserves` &lt;chr&gt;, `Last reporteddate` &lt;chr&gt;, Ref. &lt;chr&gt;\n\n# Remove rows 2 and 3 from the table\nfores = fores[-c(1, 2), ]\n\n# Assign new column names\nnames(fores) &lt;- c(\n  \"Country\",                    # Country name\n  \"Continent\",                  # Continent\n  \"Forex_IncGold_Millions\",    # Foreign exchange reserves (including gold) in millions USD\n  \"Change_IncGold\",            # Change (including gold)\n  \"Forex_ExcGold_Millions\",    # Foreign exchange reserves (excluding gold) in millions USD\n  \"Change_ExcGold\",            # Change (excluding gold)\n  \"Last_Reported_Date\",        # Last reported date\n  \"Reference\"                   # Reference\n)\n\ncolnames(fores)\n\n[1] \"Country\"                \"Continent\"              \"Forex_IncGold_Millions\"\n[4] \"Change_IncGold\"         \"Forex_ExcGold_Millions\" \"Change_ExcGold\"        \n[7] \"Last_Reported_Date\"     \"Reference\"             \n\n# Add a RANK column at the beginning\nfores &lt;- fores %&gt;%\n  mutate(Rank = row_number(), .before = 1)\n\n# View(fores)\ncolnames(fores)\n\n[1] \"Rank\"                   \"Country\"                \"Continent\"             \n[4] \"Forex_IncGold_Millions\" \"Change_IncGold\"         \"Forex_ExcGold_Millions\"\n[7] \"Change_ExcGold\"         \"Last_Reported_Date\"     \"Reference\"             \n\n# Display the first 10 rows of the Country variable in the data frame\nhead(fores$Country, n=10)\n\n [1] \"China\"        \"Japan\"        \"Switzerland\"  \"Russia\"       \"India\"       \n [6] \"Taiwan\"       \"Saudi Arabia\" \"Hong Kong\"    \"South Korea\"  \"Brazil\"      \n\n## Clean up variables\n## What type is Rank? \n## How about Date? How to fix it?\n# Remove trailing notes in Date variable\n# library(stringr)\n# fores$newdate = str_split_fixed(fores$Last_Reported_Date, \"\\\\[\", n = 2)[, 1]\n# fores$newdate = trimws(fores$newdate)\n# fores[1:2, ]\n# Hint:\n# newfores = fores[-c(1, 2), ]\n## write.csv(fores, \"newfores.csv\", row.names = FALSE)\n\n\n\nIn this assignment, I scraped the Pretty Cure (ÂÖâ‰πãÁæéÂ∞ëÂ•≥) information from Wikipedia. I extracted the series list tables from both the Traditional Chinese and English Wikipedia pages, then integrated the information from both sources into a single table/dataframe.\n\nChinese Wikipedia: https://zh.wikipedia.org/wiki/ÂÖâ‰πãÁæéÂ∞ëÂ•≥Á≥ªÂàó\nEnglish Wikipedia: https://en.wikipedia.org/wiki/Pretty_Cure\n\nThe Chinese page contains detailed broadcast period information in the format ‚ÄúYYYYÂπ¥MMÊúàDDÊó•‚Äù, while the English page provides series titles, generation, episodes, and director information. By merging both sources, I created a comprehensive dataset that includes English titles with properly formatted start and end dates.\n\n\n\ni. Date variable\nii. Removing unneeded rows and columns.\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n# -------------Import \"Pretty Cure series\" Wikipedia (Chinese)-------------\n\nurl_chi &lt;- 'https://zh.wikipedia.org/wiki/%E5%85%89%E4%B9%8B%E7%BE%8E%E5%B0%91%E5%A5%B3%E7%B3%BB%E5%88%97'\nwiki_pc_chi &lt;- read_html(url_chi)\nclass(wiki_pc_chi)\n\n[1] \"xml_document\" \"xml_node\"    \n\npretty_cure_chi &lt;- wiki_pc_chi %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[2]') %&gt;%\n  html_table()\nclass(pretty_cure_chi)\n\n[1] \"list\"\n\npc_chi = pretty_cure_chi[[1]]\n\n# Remove the last row of the data frame\npc_chi &lt;- pc_chi[-nrow(pc_chi), ]\n\nlibrary(stringr)\npc_chi &lt;- pc_chi %&gt;%\n  mutate(\n    # Extract start date (first date)\n    `Êí≠Âá∫ÈñãÂßã` = str_extract(`Êí≠Âá∫ÊúüÈñì`, \"\\\\d{4}Âπ¥\\\\d{1,2}Êúà\\\\d{1,2}Êó•\"),\n    # Extract end date (second date)\n    `Êí≠Âá∫ÁµêÊùü` = str_extract(`Êí≠Âá∫ÊúüÈñì`, \"\\\\d{4}Âπ¥\\\\d{1,2}Êúà\\\\d{1,2}Êó•(?!.*Âπ¥)\")\n    )\n\n# View results\nhead(pc_chi %&gt;% select(Á≥ªÂàó, `Êí≠Âá∫ÊúüÈñì`, `Êí≠Âá∫ÈñãÂßã`, `Êí≠Âá∫ÁµêÊùü`))\n\n# A tibble: 6 √ó 4\n  Á≥ªÂàó  Êí≠Âá∫ÊúüÈñì                  Êí≠Âá∫ÈñãÂßã     Êí≠Âá∫ÁµêÊùü     \n  &lt;chr&gt; &lt;chr&gt;                     &lt;chr&gt;        &lt;chr&gt;        \n1 1     2004Âπ¥2Êúà1Êó•2005Âπ¥1Êúà30Êó• 2004Âπ¥2Êúà1Êó• 2005Âπ¥1Êúà30Êó•\n2 2     2005Âπ¥2Êúà6Êó•2006Âπ¥1Êúà29Êó• 2005Âπ¥2Êúà6Êó• 2006Âπ¥1Êúà29Êó•\n3 3     2006Âπ¥2Êúà5Êó•2007Âπ¥1Êúà28Êó• 2006Âπ¥2Êúà5Êó• 2007Âπ¥1Êúà28Êó•\n4 4     2007Âπ¥2Êúà4Êó•2008Âπ¥1Êúà27Êó• 2007Âπ¥2Êúà4Êó• 2008Âπ¥1Êúà27Êó•\n5 5     2008Âπ¥2Êúà3Êó•2009Âπ¥1Êúà25Êó• 2008Âπ¥2Êúà3Êó• 2009Âπ¥1Êúà25Êó•\n6 6     2009Âπ¥2Êúà1Êó•2010Âπ¥1Êúà31Êó• 2009Âπ¥2Êúà1Êó• 2010Âπ¥1Êúà31Êó•\n\nlibrary(lubridate)\n# Set English locale\nSys.setlocale(\"LC_TIME\", \"English\")\n\n[1] \"English_United States.1252\"\n\n# Split and convert from Broadcast period\npc_chi &lt;- pc_chi %&gt;%\n  # Step 1: Split dates\n  mutate(\n    temp = str_replace(`Êí≠Âá∫ÊúüÈñì`, \"(Êó•)(\\\\d{4}Âπ¥)\", \"\\\\1|\\\\2\")\n) %&gt;%\n  separate(temp, into = c(\"Êí≠Âá∫ÈñãÂßã\", \"Êí≠Âá∫ÁµêÊùü\"), sep = \"\\\\|\") %&gt;%\n  \n  # Step 2: Convert to English date format\n  mutate(\n    Start_date = `Êí≠Âá∫ÈñãÂßã` %&gt;%\n      str_replace_all(\"Âπ¥|Êúà\", \"-\") %&gt;%\n      str_replace(\"Êó•\", \"\") %&gt;%\n      ymd() %&gt;%\n      format(\"%e %b %Y\") %&gt;%\n      str_trim(),\n    \n    End_date = `Êí≠Âá∫ÁµêÊùü` %&gt;%\n      str_replace_all(\"Âπ¥|Êúà\", \"-\") %&gt;%\n      str_replace(\"Êó•\", \"\") %&gt;%\n      ymd() %&gt;%\n      format(\"%e %b %Y\") %&gt;%\n      str_trim()\n  )\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [22].\n\n# 3. View results\npc_chi %&gt;%\n  select(Á≥ªÂàó, ‰ΩúÂìÅÂêçÁ®±, `Êí≠Âá∫ÈñãÂßã`, Start_date, \n         `Êí≠Âá∫ÁµêÊùü`, End_date) %&gt;%\n  head(10)\n\n# A tibble: 10 √ó 6\n   Á≥ªÂàó  ‰ΩúÂìÅÂêçÁ®±              Êí≠Âá∫ÈñãÂßã     Start_date Êí≠Âá∫ÁµêÊùü      End_date   \n   &lt;chr&gt; &lt;chr&gt;                 &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;      \n 1 1     ÂÖâ‰πãÁæéÂ∞ëÂ•≥            2004Âπ¥2Êúà1Êó• 1 Feb 2004 2005Âπ¥1Êúà30Êó• 30 Jan 2005\n 2 2     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ Max Heart  2005Âπ¥2Êúà6Êó• 6 Feb 2005 2006Âπ¥1Êúà29Êó• 29 Jan 2006\n 3 3     ÂÖâ‰πãÁæéÂ∞ëÂ•≥Splash Star 2006Âπ¥2Êúà5Êó• 5 Feb 2006 2007Âπ¥1Êúà28Êó• 28 Jan 2007\n 4 4     Yes! ÂÖâ‰πãÁæéÂ∞ëÂ•≥5      2007Âπ¥2Êúà4Êó• 4 Feb 2007 2008Âπ¥1Êúà27Êó• 27 Jan 2008\n 5 5     Yes! ÂÖâ‰πãÁæéÂ∞ëÂ•≥5GoGo! 2008Âπ¥2Êúà3Êó• 3 Feb 2008 2009Âπ¥1Êúà25Êó• 25 Jan 2009\n 6 6     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºöÂπ∏Á¶èÁ≤æÈùà  2009Âπ¥2Êúà1Êó• 1 Feb 2009 2010Âπ¥1Êúà31Êó• 31 Jan 2010\n 7 7     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºöÁîúËúúÂ§©‰Ωø  2010Âπ¥2Êúà7Êó• 7 Feb 2010 2011Âπ¥1Êúà30Êó• 30 Jan 2011\n 8 8     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºöÁæéÊ®ÇÂ§©‰Ωø  2011Âπ¥2Êúà6Êó• 6 Feb 2011 2012Âπ¥1Êúà29Êó• 29 Jan 2012\n 9 9     Smile ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºÅ    2012Âπ¥2Êúà5Êó• 5 Feb 2012 2013Âπ¥1Êúà27Êó• 27 Jan 2013\n10 10    ÂøÉÂãïÔºÅÂÖâ‰πãÁæéÂ∞ëÂ•≥      2013Âπ¥2Êúà3Êó• 3 Feb 2013 2014Âπ¥1Êúà26Êó• 26 Jan 2014\n\npc_chi &lt;- pc_chi %&gt;%\n  rename(\n    Series = `Á≥ªÂàó`\n  )\n\ncolnames(pc_chi)\n\n [1] \"Series\"     \"‰ª£\"         \"‰ΩúÂìÅÂêçÁ®±\"   \"Êí≠Âá∫ÊúüÈñì\"   \"Á≥ªÂàóÂ∞éÊºî\"  \n [6] \"ÈõÜÊï∏\"       \"ÂÇôË®ª\"       \"Êí≠Âá∫ÈñãÂßã\"   \"Êí≠Âá∫ÁµêÊùü\"   \"Start_date\"\n[11] \"End_date\"  \n\n# -------------Import \"Pretty Cure\" Wikipedia (English)-------------\nurl_eng &lt;- 'https://en.wikipedia.org/wiki/Pretty_Cure'\nwiki_pc_eng &lt;- read_html(url_eng)\nclass(wiki_pc_eng)\n\n[1] \"xml_document\" \"xml_node\"    \n\npretty_cure_eng &lt;- wiki_pc_eng %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[3]') %&gt;%\n  html_table()\nclass(pretty_cure_eng)\n\n[1] \"list\"\n\npc_eng = pretty_cure_eng[[1]]\n\n# Remove the last row of the data frame\npc_eng &lt;- pc_eng[-nrow(pc_eng), ]\n\nnames(pc_eng) &lt;- c(\n  \"Blank\",\n  \"Series\",\n  \"Title\",\n  \"Generation\", \n  \"Run\",\n  \"Episodes\",\n  \"Series director\",\n  \"Ref(s)\"\n)\n\npc_eng &lt;- pc_eng %&gt;% select(-Blank)\ncolnames(pc_eng)\n\n[1] \"Series\"          \"Title\"           \"Generation\"      \"Run\"            \n[5] \"Episodes\"        \"Series director\" \"Ref(s)\"         \n\nhead(pc_eng)\n\n# A tibble: 6 √ó 7\n  Series Title              Generation Run   Episodes `Series director` `Ref(s)`\n  &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;   \n1 1      Pretty Cure        1st        2004‚Ä¶ 49       Daisuke Nishio    \"\"      \n2 2      Pretty Cure Max H‚Ä¶ 1st        2005‚Ä¶ 47       Daisuke Nishio    \"\"      \n3 3      PreCure Splash St‚Ä¶ 2nd        2006‚Ä¶ 49       Toshiaki Komura   \"\"      \n4 4      Yes! PreCure 5     3rd        2007‚Ä¶ 49       Toshiaki Komura   \"\"      \n5 5      Yes! PreCure 5 Go‚Ä¶ 3rd        2008‚Ä¶ 48       Toshiaki Komura   \"\"      \n6 6      Fresh PreCure!     4th        2009‚Ä¶ 50       Junji Shimizu, A‚Ä¶ \"\"      \n\n# Merge Broadcast period from pc_chi to pc_eng\npc_df &lt;- pc_eng %&gt;%\n  left_join(pc_chi %&gt;% select(Series, Start_date, End_date), \n            by = \"Series\")\n\npc_df &lt;- pc_df %&gt;%\n  rename(\n    `Start date` = Start_date,\n    `End date` = End_date\n  )\n\npc_df &lt;- pc_df %&gt;%\n  select(Series, Title, Generation, `Start date`, `End date`, \n         Episodes, `Series director`, `Ref(s)`)\n\n# View results\ncolnames(pc_df)\n\n[1] \"Series\"          \"Title\"           \"Generation\"      \"Start date\"     \n[5] \"End date\"        \"Episodes\"        \"Series director\" \"Ref(s)\"         \n\n# Display the first 10 rows of the Title variable in the data frame\nhead(pc_df$Title, n=10)\n\n [1] \"Pretty Cure\"           \"Pretty Cure Max Heart\" \"PreCure Splash Star\"  \n [4] \"Yes! PreCure 5\"        \"Yes! PreCure 5 GoGo!\"  \"Fresh PreCure!\"       \n [7] \"HeartCatch PreCure!\"   \"Suite PreCure\"         \"Smile PreCure!\"       \n[10] \"DokiDoki! PreCure\"    \n\n# Print the results\nprint(pc_df, n = 22)\n\n# A tibble: 22 √ó 8\n   Series Title    Generation `Start date` `End date` Episodes `Series director`\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;            \n 1 1      Pretty ‚Ä¶ 1st        1 Feb 2004   30 Jan 20‚Ä¶ 49       Daisuke Nishio   \n 2 2      Pretty ‚Ä¶ 1st        6 Feb 2005   29 Jan 20‚Ä¶ 47       Daisuke Nishio   \n 3 3      PreCure‚Ä¶ 2nd        5 Feb 2006   28 Jan 20‚Ä¶ 49       Toshiaki Komura  \n 4 4      Yes! Pr‚Ä¶ 3rd        4 Feb 2007   27 Jan 20‚Ä¶ 49       Toshiaki Komura  \n 5 5      Yes! Pr‚Ä¶ 3rd        3 Feb 2008   25 Jan 20‚Ä¶ 48       Toshiaki Komura  \n 6 6      Fresh P‚Ä¶ 4th        1 Feb 2009   31 Jan 20‚Ä¶ 50       Junji Shimizu, A‚Ä¶\n 7 7      HeartCa‚Ä¶ 5th        7 Feb 2010   30 Jan 20‚Ä¶ 49       Tatsuya Nagamine \n 8 8      Suite P‚Ä¶ 6th        6 Feb 2011   29 Jan 20‚Ä¶ 48       Munehisa Sakai   \n 9 9      Smile P‚Ä¶ 7th        5 Feb 2012   27 Jan 20‚Ä¶ 48       Takashi Otsuka   \n10 10     DokiDok‚Ä¶ 8th        3 Feb 2013   26 Jan 20‚Ä¶ 49       Go Koga          \n11 11     Happine‚Ä¶ 9th        2 Feb 2014   25 Jan 20‚Ä¶ 49       Tatsuya Nagamine \n12 12     Go! Pri‚Ä¶ 10th       1 Feb 2015   31 Jan 20‚Ä¶ 50       Yuta Tanaka      \n13 13     Witchy ‚Ä¶ 11th       7 Feb 2016   29 Jan 20‚Ä¶ 50       Masato Mitsuka   \n14 14     Kirakir‚Ä¶ 12th       5 Feb 2017   28 Jan 20‚Ä¶ 49       Kohei Kureta, Yu‚Ä¶\n15 15     Hug! Pr‚Ä¶ 13th       4 Feb 2018   27 Jan 20‚Ä¶ 49       Junichi Sato, Ak‚Ä¶\n16 16     Star Tw‚Ä¶ 14th       3 Feb 2019   26 Jan 20‚Ä¶ 49       Hiroaki Miyamoto \n17 17     Healin'‚Ä¶ 15th       2 Feb 2020   21 Feb 20‚Ä¶ 45       Yoko Ikeda       \n18 18     Tropica‚Ä¶ 16th       28 Feb 2021  30 Jan 20‚Ä¶ 46       Yutaka Tsuchida  \n19 19     Delicio‚Ä¶ 17th       6 Feb 2022   29 Jan 20‚Ä¶ 45       Toshinori Fukasa‚Ä¶\n20 20     Soaring‚Ä¶ 18th       5 Feb 2023   28 Jan 20‚Ä¶ 50       Koji Ogawa       \n21 21     Wonderf‚Ä¶ 19th       4 Feb 2024   26 Jan 20‚Ä¶ 50       Masanori Sato    \n22 22     You and‚Ä¶ 20th       2 Feb 2025   &lt;NA&gt;       TBA      Chiaki Kon       \n# ‚Ñπ 1 more variable: `Ref(s)` &lt;chr&gt;\n\n## write.csv(pc_df, \"pc_df.csv\", row.names = FALSE)\n\n\n\n\nTo systematically collect and update data for research, I would follow these steps:\n1. Identify reliable data sources such as Wikipedia, government open data portals, or official databases related to the research topic.\n2. Use R web-scraping packages like rvest or APIs through httr to extract tables or JSON data automatically.\n3. Clean and structure the dataset by removing redundant rows/columns, converting date variables, and standardizing variable names.\n4. Store the cleaned data in a local CSV file or database (e.g., SQLite) for analysis.\n5. Automate the process using scheduled R scripts to regularly update the dataset.\n6. Apply the dataset in research ‚Äî for example, analyzing long-term trends in anime production or cultural media evolution.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DC_assignment04.html#a.-modify-the-program-to-scrape-other-tables-in-wikipedia",
    "href": "DC_assignment04.html#a.-modify-the-program-to-scrape-other-tables-in-wikipedia",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "In this assignment, I scraped the Pretty Cure (ÂÖâ‰πãÁæéÂ∞ëÂ•≥) information from Wikipedia. I extracted the series list tables from both the Traditional Chinese and English Wikipedia pages, then integrated the information from both sources into a single table/dataframe.\n\nChinese Wikipedia: https://zh.wikipedia.org/wiki/ÂÖâ‰πãÁæéÂ∞ëÂ•≥Á≥ªÂàó\nEnglish Wikipedia: https://en.wikipedia.org/wiki/Pretty_Cure\n\nThe Chinese page contains detailed broadcast period information in the format ‚ÄúYYYYÂπ¥MMÊúàDDÊó•‚Äù, while the English page provides series titles, generation, episodes, and director information. By merging both sources, I created a comprehensive dataset that includes English titles with properly formatted start and end dates.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DC_assignment04.html#b.-clean-up-the-dataframe",
    "href": "DC_assignment04.html#b.-clean-up-the-dataframe",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "i. Date variable\nii. Removing unneeded rows and columns.\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n# install.packages(\"rvest\")\nlibrary(rvest)\n\n# -------------Import \"Pretty Cure series\" Wikipedia (Chinese)-------------\n\nurl_chi &lt;- 'https://zh.wikipedia.org/wiki/%E5%85%89%E4%B9%8B%E7%BE%8E%E5%B0%91%E5%A5%B3%E7%B3%BB%E5%88%97'\nwiki_pc_chi &lt;- read_html(url_chi)\nclass(wiki_pc_chi)\n\n[1] \"xml_document\" \"xml_node\"    \n\npretty_cure_chi &lt;- wiki_pc_chi %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[2]') %&gt;%\n  html_table()\nclass(pretty_cure_chi)\n\n[1] \"list\"\n\npc_chi = pretty_cure_chi[[1]]\n\n# Remove the last row of the data frame\npc_chi &lt;- pc_chi[-nrow(pc_chi), ]\n\nlibrary(stringr)\npc_chi &lt;- pc_chi %&gt;%\n  mutate(\n    # Extract start date (first date)\n    `Êí≠Âá∫ÈñãÂßã` = str_extract(`Êí≠Âá∫ÊúüÈñì`, \"\\\\d{4}Âπ¥\\\\d{1,2}Êúà\\\\d{1,2}Êó•\"),\n    # Extract end date (second date)\n    `Êí≠Âá∫ÁµêÊùü` = str_extract(`Êí≠Âá∫ÊúüÈñì`, \"\\\\d{4}Âπ¥\\\\d{1,2}Êúà\\\\d{1,2}Êó•(?!.*Âπ¥)\")\n    )\n\n# View results\nhead(pc_chi %&gt;% select(Á≥ªÂàó, `Êí≠Âá∫ÊúüÈñì`, `Êí≠Âá∫ÈñãÂßã`, `Êí≠Âá∫ÁµêÊùü`))\n\n# A tibble: 6 √ó 4\n  Á≥ªÂàó  Êí≠Âá∫ÊúüÈñì                  Êí≠Âá∫ÈñãÂßã     Êí≠Âá∫ÁµêÊùü     \n  &lt;chr&gt; &lt;chr&gt;                     &lt;chr&gt;        &lt;chr&gt;        \n1 1     2004Âπ¥2Êúà1Êó•2005Âπ¥1Êúà30Êó• 2004Âπ¥2Êúà1Êó• 2005Âπ¥1Êúà30Êó•\n2 2     2005Âπ¥2Êúà6Êó•2006Âπ¥1Êúà29Êó• 2005Âπ¥2Êúà6Êó• 2006Âπ¥1Êúà29Êó•\n3 3     2006Âπ¥2Êúà5Êó•2007Âπ¥1Êúà28Êó• 2006Âπ¥2Êúà5Êó• 2007Âπ¥1Êúà28Êó•\n4 4     2007Âπ¥2Êúà4Êó•2008Âπ¥1Êúà27Êó• 2007Âπ¥2Êúà4Êó• 2008Âπ¥1Êúà27Êó•\n5 5     2008Âπ¥2Êúà3Êó•2009Âπ¥1Êúà25Êó• 2008Âπ¥2Êúà3Êó• 2009Âπ¥1Êúà25Êó•\n6 6     2009Âπ¥2Êúà1Êó•2010Âπ¥1Êúà31Êó• 2009Âπ¥2Êúà1Êó• 2010Âπ¥1Êúà31Êó•\n\nlibrary(lubridate)\n# Set English locale\nSys.setlocale(\"LC_TIME\", \"English\")\n\n[1] \"English_United States.1252\"\n\n# Split and convert from Broadcast period\npc_chi &lt;- pc_chi %&gt;%\n  # Step 1: Split dates\n  mutate(\n    temp = str_replace(`Êí≠Âá∫ÊúüÈñì`, \"(Êó•)(\\\\d{4}Âπ¥)\", \"\\\\1|\\\\2\")\n) %&gt;%\n  separate(temp, into = c(\"Êí≠Âá∫ÈñãÂßã\", \"Êí≠Âá∫ÁµêÊùü\"), sep = \"\\\\|\") %&gt;%\n  \n  # Step 2: Convert to English date format\n  mutate(\n    Start_date = `Êí≠Âá∫ÈñãÂßã` %&gt;%\n      str_replace_all(\"Âπ¥|Êúà\", \"-\") %&gt;%\n      str_replace(\"Êó•\", \"\") %&gt;%\n      ymd() %&gt;%\n      format(\"%e %b %Y\") %&gt;%\n      str_trim(),\n    \n    End_date = `Êí≠Âá∫ÁµêÊùü` %&gt;%\n      str_replace_all(\"Âπ¥|Êúà\", \"-\") %&gt;%\n      str_replace(\"Êó•\", \"\") %&gt;%\n      ymd() %&gt;%\n      format(\"%e %b %Y\") %&gt;%\n      str_trim()\n  )\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [22].\n\n# 3. View results\npc_chi %&gt;%\n  select(Á≥ªÂàó, ‰ΩúÂìÅÂêçÁ®±, `Êí≠Âá∫ÈñãÂßã`, Start_date, \n         `Êí≠Âá∫ÁµêÊùü`, End_date) %&gt;%\n  head(10)\n\n# A tibble: 10 √ó 6\n   Á≥ªÂàó  ‰ΩúÂìÅÂêçÁ®±              Êí≠Âá∫ÈñãÂßã     Start_date Êí≠Âá∫ÁµêÊùü      End_date   \n   &lt;chr&gt; &lt;chr&gt;                 &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;      \n 1 1     ÂÖâ‰πãÁæéÂ∞ëÂ•≥            2004Âπ¥2Êúà1Êó• 1 Feb 2004 2005Âπ¥1Êúà30Êó• 30 Jan 2005\n 2 2     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ Max Heart  2005Âπ¥2Êúà6Êó• 6 Feb 2005 2006Âπ¥1Êúà29Êó• 29 Jan 2006\n 3 3     ÂÖâ‰πãÁæéÂ∞ëÂ•≥Splash Star 2006Âπ¥2Êúà5Êó• 5 Feb 2006 2007Âπ¥1Êúà28Êó• 28 Jan 2007\n 4 4     Yes! ÂÖâ‰πãÁæéÂ∞ëÂ•≥5      2007Âπ¥2Êúà4Êó• 4 Feb 2007 2008Âπ¥1Êúà27Êó• 27 Jan 2008\n 5 5     Yes! ÂÖâ‰πãÁæéÂ∞ëÂ•≥5GoGo! 2008Âπ¥2Êúà3Êó• 3 Feb 2008 2009Âπ¥1Êúà25Êó• 25 Jan 2009\n 6 6     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºöÂπ∏Á¶èÁ≤æÈùà  2009Âπ¥2Êúà1Êó• 1 Feb 2009 2010Âπ¥1Êúà31Êó• 31 Jan 2010\n 7 7     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºöÁîúËúúÂ§©‰Ωø  2010Âπ¥2Êúà7Êó• 7 Feb 2010 2011Âπ¥1Êúà30Êó• 30 Jan 2011\n 8 8     ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºöÁæéÊ®ÇÂ§©‰Ωø  2011Âπ¥2Êúà6Êó• 6 Feb 2011 2012Âπ¥1Êúà29Êó• 29 Jan 2012\n 9 9     Smile ÂÖâ‰πãÁæéÂ∞ëÂ•≥ÔºÅ    2012Âπ¥2Êúà5Êó• 5 Feb 2012 2013Âπ¥1Êúà27Êó• 27 Jan 2013\n10 10    ÂøÉÂãïÔºÅÂÖâ‰πãÁæéÂ∞ëÂ•≥      2013Âπ¥2Êúà3Êó• 3 Feb 2013 2014Âπ¥1Êúà26Êó• 26 Jan 2014\n\npc_chi &lt;- pc_chi %&gt;%\n  rename(\n    Series = `Á≥ªÂàó`\n  )\n\ncolnames(pc_chi)\n\n [1] \"Series\"     \"‰ª£\"         \"‰ΩúÂìÅÂêçÁ®±\"   \"Êí≠Âá∫ÊúüÈñì\"   \"Á≥ªÂàóÂ∞éÊºî\"  \n [6] \"ÈõÜÊï∏\"       \"ÂÇôË®ª\"       \"Êí≠Âá∫ÈñãÂßã\"   \"Êí≠Âá∫ÁµêÊùü\"   \"Start_date\"\n[11] \"End_date\"  \n\n# -------------Import \"Pretty Cure\" Wikipedia (English)-------------\nurl_eng &lt;- 'https://en.wikipedia.org/wiki/Pretty_Cure'\nwiki_pc_eng &lt;- read_html(url_eng)\nclass(wiki_pc_eng)\n\n[1] \"xml_document\" \"xml_node\"    \n\npretty_cure_eng &lt;- wiki_pc_eng %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[3]') %&gt;%\n  html_table()\nclass(pretty_cure_eng)\n\n[1] \"list\"\n\npc_eng = pretty_cure_eng[[1]]\n\n# Remove the last row of the data frame\npc_eng &lt;- pc_eng[-nrow(pc_eng), ]\n\nnames(pc_eng) &lt;- c(\n  \"Blank\",\n  \"Series\",\n  \"Title\",\n  \"Generation\", \n  \"Run\",\n  \"Episodes\",\n  \"Series director\",\n  \"Ref(s)\"\n)\n\npc_eng &lt;- pc_eng %&gt;% select(-Blank)\ncolnames(pc_eng)\n\n[1] \"Series\"          \"Title\"           \"Generation\"      \"Run\"            \n[5] \"Episodes\"        \"Series director\" \"Ref(s)\"         \n\nhead(pc_eng)\n\n# A tibble: 6 √ó 7\n  Series Title              Generation Run   Episodes `Series director` `Ref(s)`\n  &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;   \n1 1      Pretty Cure        1st        2004‚Ä¶ 49       Daisuke Nishio    \"\"      \n2 2      Pretty Cure Max H‚Ä¶ 1st        2005‚Ä¶ 47       Daisuke Nishio    \"\"      \n3 3      PreCure Splash St‚Ä¶ 2nd        2006‚Ä¶ 49       Toshiaki Komura   \"\"      \n4 4      Yes! PreCure 5     3rd        2007‚Ä¶ 49       Toshiaki Komura   \"\"      \n5 5      Yes! PreCure 5 Go‚Ä¶ 3rd        2008‚Ä¶ 48       Toshiaki Komura   \"\"      \n6 6      Fresh PreCure!     4th        2009‚Ä¶ 50       Junji Shimizu, A‚Ä¶ \"\"      \n\n# Merge Broadcast period from pc_chi to pc_eng\npc_df &lt;- pc_eng %&gt;%\n  left_join(pc_chi %&gt;% select(Series, Start_date, End_date), \n            by = \"Series\")\n\npc_df &lt;- pc_df %&gt;%\n  rename(\n    `Start date` = Start_date,\n    `End date` = End_date\n  )\n\npc_df &lt;- pc_df %&gt;%\n  select(Series, Title, Generation, `Start date`, `End date`, \n         Episodes, `Series director`, `Ref(s)`)\n\n# View results\ncolnames(pc_df)\n\n[1] \"Series\"          \"Title\"           \"Generation\"      \"Start date\"     \n[5] \"End date\"        \"Episodes\"        \"Series director\" \"Ref(s)\"         \n\n# Display the first 10 rows of the Title variable in the data frame\nhead(pc_df$Title, n=10)\n\n [1] \"Pretty Cure\"           \"Pretty Cure Max Heart\" \"PreCure Splash Star\"  \n [4] \"Yes! PreCure 5\"        \"Yes! PreCure 5 GoGo!\"  \"Fresh PreCure!\"       \n [7] \"HeartCatch PreCure!\"   \"Suite PreCure\"         \"Smile PreCure!\"       \n[10] \"DokiDoki! PreCure\"    \n\n# Print the results\nprint(pc_df, n = 22)\n\n# A tibble: 22 √ó 8\n   Series Title    Generation `Start date` `End date` Episodes `Series director`\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;            \n 1 1      Pretty ‚Ä¶ 1st        1 Feb 2004   30 Jan 20‚Ä¶ 49       Daisuke Nishio   \n 2 2      Pretty ‚Ä¶ 1st        6 Feb 2005   29 Jan 20‚Ä¶ 47       Daisuke Nishio   \n 3 3      PreCure‚Ä¶ 2nd        5 Feb 2006   28 Jan 20‚Ä¶ 49       Toshiaki Komura  \n 4 4      Yes! Pr‚Ä¶ 3rd        4 Feb 2007   27 Jan 20‚Ä¶ 49       Toshiaki Komura  \n 5 5      Yes! Pr‚Ä¶ 3rd        3 Feb 2008   25 Jan 20‚Ä¶ 48       Toshiaki Komura  \n 6 6      Fresh P‚Ä¶ 4th        1 Feb 2009   31 Jan 20‚Ä¶ 50       Junji Shimizu, A‚Ä¶\n 7 7      HeartCa‚Ä¶ 5th        7 Feb 2010   30 Jan 20‚Ä¶ 49       Tatsuya Nagamine \n 8 8      Suite P‚Ä¶ 6th        6 Feb 2011   29 Jan 20‚Ä¶ 48       Munehisa Sakai   \n 9 9      Smile P‚Ä¶ 7th        5 Feb 2012   27 Jan 20‚Ä¶ 48       Takashi Otsuka   \n10 10     DokiDok‚Ä¶ 8th        3 Feb 2013   26 Jan 20‚Ä¶ 49       Go Koga          \n11 11     Happine‚Ä¶ 9th        2 Feb 2014   25 Jan 20‚Ä¶ 49       Tatsuya Nagamine \n12 12     Go! Pri‚Ä¶ 10th       1 Feb 2015   31 Jan 20‚Ä¶ 50       Yuta Tanaka      \n13 13     Witchy ‚Ä¶ 11th       7 Feb 2016   29 Jan 20‚Ä¶ 50       Masato Mitsuka   \n14 14     Kirakir‚Ä¶ 12th       5 Feb 2017   28 Jan 20‚Ä¶ 49       Kohei Kureta, Yu‚Ä¶\n15 15     Hug! Pr‚Ä¶ 13th       4 Feb 2018   27 Jan 20‚Ä¶ 49       Junichi Sato, Ak‚Ä¶\n16 16     Star Tw‚Ä¶ 14th       3 Feb 2019   26 Jan 20‚Ä¶ 49       Hiroaki Miyamoto \n17 17     Healin'‚Ä¶ 15th       2 Feb 2020   21 Feb 20‚Ä¶ 45       Yoko Ikeda       \n18 18     Tropica‚Ä¶ 16th       28 Feb 2021  30 Jan 20‚Ä¶ 46       Yutaka Tsuchida  \n19 19     Delicio‚Ä¶ 17th       6 Feb 2022   29 Jan 20‚Ä¶ 45       Toshinori Fukasa‚Ä¶\n20 20     Soaring‚Ä¶ 18th       5 Feb 2023   28 Jan 20‚Ä¶ 50       Koji Ogawa       \n21 21     Wonderf‚Ä¶ 19th       4 Feb 2024   26 Jan 20‚Ä¶ 50       Masanori Sato    \n22 22     You and‚Ä¶ 20th       2 Feb 2025   &lt;NA&gt;       TBA      Chiaki Kon       \n# ‚Ñπ 1 more variable: `Ref(s)` &lt;chr&gt;\n\n## write.csv(pc_df, \"pc_df.csv\", row.names = FALSE)",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DC_assignment04.html#c.-suggest-a-data-plan-to-acquire-web-data-for-research.",
    "href": "DC_assignment04.html#c.-suggest-a-data-plan-to-acquire-web-data-for-research.",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "To systematically collect and update data for research, I would follow these steps:\n1. Identify reliable data sources such as Wikipedia, government open data portals, or official databases related to the research topic.\n2. Use R web-scraping packages like rvest or APIs through httr to extract tables or JSON data automatically.\n3. Clean and structure the dataset by removing redundant rows/columns, converting date variables, and standardizing variable names.\n4. Store the cleaned data in a local CSV file or database (e.g., SQLite) for analysis.\n5. Automate the process using scheduled R scripts to regularly update the dataset.\n6. Apply the dataset in research ‚Äî for example, analyzing long-term trends in anime production or cultural media evolution.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DC_assignment06.html",
    "href": "DC_assignment06.html",
    "title": "Assignment 6 - Text Analytics using quanteda",
    "section": "",
    "text": "Done.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DC_assignment06.html#a.-biden-xi-summit-data",
    "href": "DC_assignment06.html#a.-biden-xi-summit-data",
    "title": "Assignment 6 - Text Analytics using quanteda",
    "section": "a. Biden-Xi summit data",
    "text": "a. Biden-Xi summit data\n\nTop Hashtags\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\nclass(toks)\n\n[1] \"tokens\"\n\n# Latent Semantic Analysis \n## (https://quanteda.io/reference/textmodel_lsa.html)\n\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm, nd=4,  margin = c(\"both\", \"documents\", \"features\"))\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\nhead(sum_lsa$docs)\n\n              [,1]          [,2]          [,3]          [,4]\ntext1 8.678102e-03  9.529008e-03 -3.178574e-03  1.380732e-02\ntext2 8.676818e-06 -8.806186e-06 -5.989637e-06  1.677631e-05\ntext3 2.922127e-03  6.778967e-03  1.131673e-03 -3.176902e-03\ntext4 1.046624e-02  8.884054e-04 -4.282723e-03  4.960680e-03\ntext5 3.251208e-03  8.005843e-03  2.208204e-04 -4.656367e-03\ntext6 3.251208e-03  8.005843e-03  2.208204e-04 -4.656367e-03\n\nclass(sum_lsa)\n\n[1] \"textmodel_lsa\"\n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\n\nThe most frequently used hashtags in the Biden-Xi summit Twitter data (Top 10):\n\n#china\n#biden\n#xijinping\n#joebiden\n#america\n#americans\n#coronavirus\n#fentanyl\n#xi\n#us\n\n\n\nHashtag Network Analysis (Blue Network)\n\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\nThe hashtag co-occurrence network reveals several key discussion clusters:\n\nCentral cluster: #china, #biden, #xijinping, #joebiden, #america, and #americans form the core of the network, showing that most tweets discussed both leaders together.\nHealth-related topics: #coronavirus and #fentanyl are closely connected to the main cluster, indicating that public health issues were important topics during the summit discussion.\nHuman rights cluster: #uyghurs, #uyghurgenocide, #tibetans, #humanrights, and #xi form a separate cluster on the right side, showing that human rights concerns were a distinct theme in the Twitter conversation.\nGeopolitical topics: #taiwan, #usa, #us, and #ccp appear on the periphery, representing broader geopolitical discussions.\n\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_nfeat ... 10 more features, reached max_nfeat ... 701 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\n\nUser Network Analysis (Red Network)\nThe user mention network shows several distinct groups:\n\nNBA-related cluster: @nba, @washwizards, @pelicansnba, @capitalonearena, and @eneskanter are closely connected. This is likely related to Enes Kanter (NBA player) who was vocal about China‚Äôs human rights issues.\nJournalists cluster: @politico, @phelimkine, @nahaltoosi, @whnsc, and @anderscorr represent media and journalists covering the summit.\nBloomberg reporters: @nwadhams, @jendeben, and @petermartin_pcm form another journalist cluster.\nWall Street Journal reporters: @learyreports and @glubold form a separate cluster representing Wall Street Journal coverage of the summit.\nOther news professionals: @betamoroney, @evasmartai, and @enilev represent other journalists and news professionals discussing the event.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DC_assignment06.html#b.-us-presidential-inaugural-speeches",
    "href": "DC_assignment06.html#b.-us-presidential-inaugural-speeches",
    "title": "Assignment 6 - Text Analytics using quanteda",
    "section": "b. US presidential inaugural speeches",
    "text": "b. US presidential inaugural speeches\ni. Any similarities and differences over time and among presidents?\n\nWord Cloud Analysis (1789-1826)\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\")\n# Website: https://quanteda.io/\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\n\nThis word cloud visualizes the most frequently used words in US presidential inaugural speeches from 1789 to 1826 (early presidents including Washington, Adams, Jefferson, Madison, Monroe, and John Quincy Adams).\nMost prominent words (largest size = highest frequency):\n\ngovernment: The most frequently used word, reflecting the focus on establishing and defining the new government system.\npeople, states, country, united: Core concepts emphasizing national unity and the role of citizens.\npublic, great, nation: Words highlighting the importance of public service and national identity.\n\nKey themes identified:\n\nGovernance and Constitution: ‚Äúgovernment‚Äù, ‚Äúconstitution‚Äù, ‚Äúunion‚Äù, ‚Äúlaws‚Äù, ‚Äúpowers‚Äù, ‚Äúadministration‚Äù - Early presidents emphasized the importance of constitutional governance.\nNational Unity: ‚Äúunited‚Äù, ‚Äústates‚Äù, ‚Äúunion‚Äù, ‚Äúnation‚Äù, ‚Äúcountry‚Äù - Reflecting the priority of keeping the young nation together.\nCivic Values: ‚Äúpeople‚Äù, ‚Äúcitizens‚Äù, ‚Äúpublic‚Äù, ‚Äúrights‚Äù, ‚Äúliberty‚Äù, ‚Äújustice‚Äù - Emphasizing democratic ideals and citizen participation.\nPeace and Foreign Relations: ‚Äúpeace‚Äù, ‚Äúwar‚Äù, ‚Äúforeign‚Äù, ‚Äúnations‚Äù, ‚Äúcommerce‚Äù - Addressing both domestic stability and international relationships.\nPrinciples and Duties: ‚Äúprinciples‚Äù, ‚Äúduties‚Äù, ‚Äúhonor‚Äù, ‚Äútrust‚Äù, ‚Äúconfidence‚Äù - Reflecting the moral language common in early American political discourse.\n\nNote: This analysis only includes speeches up to 1826 (Year &lt;= 1826), representing the founding era of American democracy.\n\n\nComparison Word Cloud: Obama, Bush, and Trump\n\ninaug_speech = data_corpus_inaugural\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\n\nThis word cloud uses the comparison = TRUE parameter, where each word‚Äôs color indicates which president used that word with the highest relative frequency in their inaugural address.\nObama (blue): - High-frequency words: ‚Äúus‚Äù, ‚Äúmust‚Äù, ‚Äúcan‚Äù, ‚Äúnation‚Äù, ‚Äúpeople‚Äù, ‚Äúgeneration‚Äù, ‚Äújourney‚Äù, ‚Äúcommon‚Äù, ‚Äúoath‚Äù - Obama‚Äôs language emphasizes collective action (‚Äúus‚Äù, ‚Äúmust‚Äù), generational legacy (‚Äúgeneration‚Äù), and shared values (‚Äúcommon‚Äù, ‚Äújourney‚Äù)\nBush (light blue/cyan): - High-frequency words: ‚Äúfreedom‚Äù, ‚Äúnation‚Äù, ‚Äúamerica‚Äù, ‚Äúliberty‚Äù, ‚Äúfree‚Äù, ‚Äúhope‚Äù, ‚Äújustice‚Äù, ‚Äústory‚Äù, ‚Äúhistory‚Äù - Bush‚Äôs speeches center on freedom as a core theme (‚Äúfreedom‚Äù, ‚Äúliberty‚Äù, ‚Äúfree‚Äù), along with historical narrative (‚Äústory‚Äù, ‚Äúhistory‚Äù) and hope (‚Äúhope‚Äù)\nTrump (green): - High-frequency words: ‚Äúamerica‚Äù, ‚Äúthank‚Äù, ‚Äúnation‚Äù, ‚Äúcountry‚Äù, ‚Äúback‚Äù, ‚Äúnever‚Äù, ‚Äúgovernment‚Äù, ‚Äúbring‚Äù - Trump‚Äôs distinctive features include frequent use of ‚Äúthank‚Äù, emphasis on restoration (‚Äúback‚Äù), and critical language toward government\nNote: Many words like ‚Äúamerica‚Äù, ‚Äúnation‚Äù, and ‚Äúpeople‚Äù are commonly used by all three presidents, but are assigned to the president with the highest relative proportion.\n\n\nColorful Word Cloud (1789-1826)\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\n\nThis word cloud displays the same data as the first word cloud (inaugural speeches from 1789-1826), but with multiple colors to enhance visual distinction between words. The colors are assigned randomly and do not represent different categories or presidents.\nMost prominent words (largest size = highest frequency):\n\ngovernment (purple): The dominant word, appearing most frequently in early presidential speeches.\nstates, peace, war (red/orange): Key terms reflecting concerns about national unity and international relations.\npeople, country, public (orange/red): Emphasizing democratic ideals and civic responsibility.\ngreat, every, us, may (yellow/orange): Common words used in political rhetoric.\n\nNote: Unlike the comparison word cloud, this visualization uses min_count = 10, meaning only words appearing at least 10 times are displayed. The six colors (red, pink, green, purple, orange, blue) are used purely for visual variety, not to indicate any categorical distinction.\n\n\nLexical Dispersion Plot: ‚ÄúAmerican‚Äù\n\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n\nThis lexical dispersion plot (also called an x-ray plot) shows where the word ‚Äúamerican‚Äù appears within each presidential inaugural speech from 1953 to 2025. Each vertical line represents one occurrence of the word, and the x-axis shows the relative position within the speech (0.00 = beginning, 1.00 = end).\nHow to read this plot:\n- Each row represents one inaugural speech (labeled by Year-President)\n- Vertical black lines indicate where ‚Äúamerican‚Äù appears in the speech\n- More lines = more frequent use of the word\n- Spread of lines shows whether the word is used throughout or concentrated in certain parts\nKey observations:\nHigh frequency users (many vertical lines):\n- 2025-Trump: 15 occurrences, distributed across the speech\n- 2017-Trump: 12 occurrences, with concentration in the middle section\n- 2021-Biden: 10 occurrences, spread throughout the speech\n- 1997-Clinton: 8 occurrences, distributed from middle to end\n- 1985-Reagan: 6 occurrences, spread across the speech\nLow frequency users (few vertical lines):\n- 1953-Eisenhower: Only 1 occurrence\n- 1977-Carter: 3 occurrences\n- 1969-Nixon and 1973-Nixon: 2 occurrences each\nTrend over time: More recent presidents (Clinton, Bush, Obama, Trump, Biden) tend to use ‚Äúamerican‚Äù more frequently than earlier presidents (Eisenhower, Nixon, Carter), suggesting an increasing emphasis on American identity in modern political rhetoric.\n\n\nLexical Dispersion Plot: ‚ÄúAmerican‚Äù vs ‚ÄúPeople‚Äù vs ‚ÄúCommunist‚Äù\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n  \n)\n\n\n\n\n\n\n\n\nLexical Dispersion Plot Interpretation\nThis lexical dispersion plot compares the distribution of two words ‚Äî ‚Äúamerican‚Äù and ‚Äúpeople‚Äù ‚Äî across presidential inaugural speeches from 1953 to 2025. The plot is divided into two panels (left: ‚Äúamerican‚Äù, right: ‚Äúpeople‚Äù). Note: We also attempted to visualize ‚Äúcommunist‚Äù, but this word does not appear in any of the inaugural speeches, so no panel is displayed for it.\nKey observations:\n‚ÄúPeople‚Äù (right panel) ‚Äî Most frequently used (164 total occurrences):\n- Almost every president uses ‚Äúpeople‚Äù multiple times throughout their speeches\n- High frequency users: 1985-Reagan (16), 1969-Nixon (14), 1993-Clinton (12), 1997-Clinton (11), 2013-Obama (11)\n- Low frequency users: 1961-Kennedy (1), 2001-Bush (1)\n- The word appears distributed across entire speeches, indicating it‚Äôs a fundamental term in presidential rhetoric\n‚ÄúAmerican‚Äù (left panel) ‚Äî Moderate usage (87 total occurrences):\n- Usage varies significantly between presidents\n- High frequency: 2025-Trump (15), 2017-Trump (12), 2021-Biden (10), 1997-Clinton (8)\n- Low frequency: 1953-Eisenhower (1), 1961-Kennedy (0), 1973-Nixon (2)\n- Modern presidents tend to use ‚Äúamerican‚Äù more frequently than earlier ones\nComparison insights:\n\n\n\nWord\nTotal Occurrences\nAverage per Speech\n\n\n\n\npeople\n164\n~8.6\n\n\namerican\n87\n~4.6\n\n\n\nInterpretation: ‚ÄúPeople‚Äù is a universal, high-frequency term across all presidents, while ‚Äúamerican‚Äù shows an increasing trend over time, with recent presidents (Trump, Biden) using it more frequently than earlier ones (Eisenhower, Kennedy, Nixon).\n\n\nColored Lexical Dispersion Plot: ‚ÄúAmerican‚Äù (Blue) vs ‚ÄúPeople‚Äù (Red) vs ‚ÄúCommunist‚Äù (Green)\n\n## Why is the \"communist\" plot missing?\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis plot is the same analysis as the previous one, but with colors added for better visual distinction: ‚Äúamerican‚Äù in blue, ‚Äúpeople‚Äù in red, and ‚Äúcommunist‚Äù in green.\nWhy is the ‚Äúcommunist‚Äù plot missing?\nThe word ‚Äúcommunist‚Äù does not appear in any presidential inaugural speech from 1953 to 2025. Even though the code includes pattern = \"communist\" with green color assigned, there are no green lines in the plot because:\n\nInaugural speeches are ceremonial: Presidents focus on unity, hope, and positive vision rather than attacking adversaries\nDiplomatic language: Even during the Cold War, presidents avoided directly naming ‚Äúcommunist‚Äù as it could escalate tensions\nRhetorical strategy: Instead of negative terms, presidents use positive framing like ‚Äúfreedom‚Äù and ‚Äúdemocracy‚Äù\n\nComparison: ‚ÄúAmerican‚Äù (Blue) vs ‚ÄúPeople‚Äù (Red)\n\n\n\n\n\n\n\n\nAspect\n‚ÄúAmerican‚Äù (Blue)\n‚ÄúPeople‚Äù (Red)\n\n\n\n\nFrequency\nModerate, varies by president\nHigh, consistent across all\n\n\nDistribution\nOften clustered in specific sections\nSpread throughout speeches\n\n\nTrend\nIncreasing in modern speeches\nStable over time\n\n\n\nPresidents with notable usage patterns:\n- 2025-Trump: Highest ‚Äúamerican‚Äù usage (13), moderate ‚Äúpeople‚Äù (8)\n- 2017-Trump: High ‚Äúamerican‚Äù (11), balanced ‚Äúpeople‚Äù (10)\n- 1985-Reagan: Highest ‚Äúpeople‚Äù usage (16), moderate ‚Äúamerican‚Äù (7)\n- 1997-Clinton: High usage of both terms (9 and 11)\n- 1969-Nixon: High ‚Äúpeople‚Äù (14), low ‚Äúamerican‚Äù (3)\nKey insight: The absence of ‚Äúcommunist‚Äù (green) demonstrates that presidential inaugural addresses maintain a positive, unifying tone rather than identifying external enemies, even during periods of significant geopolitical tension.\n\n\nWord Frequency Plot (Top 100 Words, 1789-1826)\n\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThis scatter plot displays the frequency of the top 100 most common words in presidential inaugural speeches from 1789 to 1826. Words are arranged from highest to lowest frequency (left to right).\nDistribution pattern:\nThe plot shows a typical Zipf‚Äôs Law distribution ‚Äî a small number of words appear very frequently, while most words appear relatively rarely. The curve drops steeply at first, then flattens out (long tail).\nTop frequency words (frequency &gt; 50):\n\n\n\nRank\nWord\nFrequency\n\n\n\n\n1\ngovernment\n93\n\n\n2\nmay\n69\n\n\n2\ngreat\n69\n\n\n4\nstates\n67\n\n\n5\npeople\n64\n\n\n6\npublic\n60\n\n\n7\nevery\n59\n\n\n8\ncountry\n55\n\n\n9\nwar\n52\n\n\n10\nunited\n48\n\n\n\nMid-frequency words (frequency 25-50):\n- us (47), can (46), union (46), citizens (45), peace (43), nation (41), nations (40)\n- shall (39), foreign (34), state (34), power (32), rights (32), constitution (31)\n- These represent important political and governance concepts\nLower frequency words (frequency &lt; 25):\n- happiness (19), administration (17), liberty (16), executive (15), fellow-citizens (14), success (13)\n- More specific terms that appear less often but still significant\nKey insight: The dominance of ‚Äúgovernment‚Äù (93 occurrences) reflects the primary concern of early presidents: establishing and legitimizing the new federal government system. Words like ‚Äústates‚Äù (67), ‚Äúunion‚Äù (46), and ‚Äúconstitution‚Äù (31) further emphasize the focus on national structure and unity during the founding era.\n\n\nFrequency of ‚ÄúAmerican‚Äù by President (1949-2025)\n\n# Get frequency grouped by president\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\n# Filter the term \"american\"\nfreq_american &lt;- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThis plot shows how many times each president used the word ‚Äúamerican‚Äù in their inaugural speech(es). Note that some presidents served two terms and gave two inaugural addresses, which may affect their total count.\nFrequency ranking:\n\n\n\nPresident\nFrequency\nObservation\n\n\n\n\nTrump\n24\nHighest usage\n\n\nClinton\n13\nSecond highest\n\n\nBiden\n9\nThird highest\n\n\nBush\n8\n(Two terms combined)\n\n\nObama\n8\n(Two terms combined)\n\n\nReagan\n8\n(Two terms combined)\n\n\nJohnson\n6\nModerate usage\n\n\nNixon\n5\n(Two terms combined)\n\n\nCarter\n3\nLow usage\n\n\nEisenhower\n3\nLow usage\n\n\n\nKey observations:\n\nTrump used ‚Äúamerican‚Äù most frequently (24 times), nearly double the second-highest user (Clinton with 13). This reflects his emphasis on American identity and nationalism in his inaugural rhetoric.\nClinton ranks second (13 times), showing strong emphasis on American identity in his speeches.\nBush, Obama, and Reagan tied at 8 occurrences each, despite all serving two terms with two inaugural addresses.\nCold War era presidents (Eisenhower, Nixon, Carter) used ‚Äúamerican‚Äù less frequently (3-5 times).\nGeneral trend: More recent presidents (Trump, Clinton, Biden) tend to use ‚Äúamerican‚Äù more frequently than earlier presidents (Eisenhower, Nixon, Carter).\n\nNote: This analysis counts only the exact word ‚Äúamerican‚Äù (case-insensitive). Related terms like ‚ÄúAmerica‚Äù, ‚ÄúAmericans‚Äù, or ‚ÄúAmerica‚Äôs‚Äù are counted separately.\n\n\nRelative Frequency of ‚ÄúAmerican‚Äù by President (1949-2025)\n\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\nhead(dfm_rel_freq)\n\nDocument-feature matrix of: 6 documents, 4,625 features (86.44% sparse) and 4 docvars.\n                 features\ndocs                      my    friends        ,    before          i\n  1953-Eisenhower 0.14582574 0.14582574 4.593511 0.1822822 0.10936930\n  1957-Eisenhower 0.20975354 0.10487677 6.345045 0.1573152 0.05243838\n  1961-Kennedy    0.19467878 0.06489293 5.451006 0.1297859 0.32446463\n  1965-Johnson    0.17543860 0.05847953 5.555556 0.2339181 0.87719298\n  1969-Nixon      0.28973510 0          5.546358 0.1241722 0.86920530\n  1973-Nixon      0.05012531 0.05012531 4.812030 0.2005013 0.60150376\n                 features\ndocs                   begin      the expression       of     those\n  1953-Eisenhower 0.03645643 6.234050 0.03645643 5.176814 0.1458257\n  1957-Eisenhower 0          5.977976 0          5.034085 0.1573152\n  1961-Kennedy    0.19467878 5.580792 0          4.218040 0.4542505\n  1965-Johnson    0          4.502924 0          3.333333 0.1754386\n  1969-Nixon      0          5.629139 0          3.890728 0.4552980\n  1973-Nixon      0          4.160401 0          3.408521 0.3007519\n[ reached max_nfeat ... 4,615 more features ]\n\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\n\n# Filter the term \"american\"\nrel_freq_american &lt;- subset(rel_freq, feature %in% \"american\")  \n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  xlab(NULL) + \n  ylab(\"Relative frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThis plot shows the relative frequency (percentage of total words) of ‚Äúamerican‚Äù in each president‚Äôs inaugural speech(es). Unlike the previous plot that showed raw counts, this controls for speech length ‚Äî a fairer comparison since some speeches are longer than others.\nWhy use relative frequency? - Raw frequency can be misleading: a longer speech naturally has more words - Relative frequency = (word count / total words) √ó 100 - This shows the proportion of the speech devoted to a particular word\nRelative frequency ranking:\n\n\n\nPresident\nRelative Frequency (%)\nInterpretation\n\n\n\n\nTrump\n1.05%\nHighest emphasis on ‚Äúamerican‚Äù\n\n\nClinton\n0.59%\nSecond highest\n\n\nBush\n0.38%\nStrong emphasis\n\n\nJohnson\n0.35%\nAbove average\n\n\nObama\n0.33%\nAbove average\n\n\nBiden\n0.33%\nAbove average\n\n\nReagan\n0.28%\nModerate\n\n\nNixon\n0.22%\nBelow average\n\n\nCarter\n0.22%\nBelow average\n\n\nEisenhower\n0.14%\nLowest emphasis\n\n\n\nKey observations:\n\nTrump ranks highest with 1.05%, nearly double the second-highest (Clinton at 0.59%). This confirms his strong emphasis on American identity and nationalism in his inaugural rhetoric.\nClinton ranks second even after adjusting for speech length, confirming his emphasis on American identity.\nBush moves up in ranking: His relative frequency (0.38%) is higher than some presidents with similar raw counts, meaning ‚Äúamerican‚Äù made up a larger portion of his speeches.\nEisenhower‚Äôs low usage is confirmed: Both raw and relative frequency show he rarely used ‚Äúamerican‚Äù in his inaugural addresses (0.14%).\nConsistent pattern: The relative frequency ranking is similar to the raw frequency ranking, with Trump and Clinton at the top, and Eisenhower at the bottom.\n\nComparison with raw frequency: This relative measure confirms that Trump‚Äôs high usage of ‚Äúamerican‚Äù is not simply due to longer speeches ‚Äî he genuinely emphasized this word more than any other president in this dataset.\n\n\nTop 10 Words by President (2001-2025)\n\ndfm_weight_pres &lt;- data_corpus_inaugural %&gt;%\n  corpus_subset(Year &gt; 2000) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight &lt;- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n  geom_point() +\n  facet_wrap(~ group, scales = \"free\") +\n  coord_flip() +\n  scale_x_continuous(breaks = nrow(freq_weight):1,\n                     labels = freq_weight$feature) +\n  labs(x = NULL, y = \"Relative frequency\")\n\n\n\n\n\n\n\n\nThis faceted plot shows the top 10 most frequently used words (by relative frequency) for each of the four 21st-century presidents: Biden, Bush, Obama, and Trump. Stopwords (common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù) have been removed.\nBiden‚Äôs Top Words:\n\n\n\nRank\nWord\nRelative Freq\n\n\n\n\n1\nus\n0.024\n\n\n2\namerica\n0.016\n\n\n3\ncan\n0.014\n\n\n4\none\n0.013\n\n\n5\nnation\n0.011\n\n\n6\nmust\n0.009\n\n\n6\ndemocracy\n0.009\n\n\n8\nstory\n0.008\n\n\n8\namerican\n0.008\n\n\n8\npeople\n0.008\n\n\n\nKey themes: Unity (‚Äúus‚Äù, ‚Äúone‚Äù), democracy, collective identity\n\nBush‚Äôs Top Words:\n\n\n\nRank\nWord\nRelative Freq\n\n\n\n\n1\nfreedom\n0.030\n\n\n2\namerica\n0.022\n\n\n3\ncountry\n0.019\n\n\n4\nnation\n0.019\n\n\n5\nevery\n0.017\n\n\n6\ncitizens\n0.017\n\n\n7\nus\n0.017\n\n\n8\nliberty\n0.016\n\n\n9\ncan\n0.014\n\n\n10\namericans\n0.014\n\n\n\nKey themes: Freedom, liberty, citizenship ‚Äî reflecting post-9/11 rhetoric\n\nObama‚Äôs Top Words:\n\n\n\nRank\nWord\nRelative Freq\n\n\n\n\n1\nus\n0.040\n\n\n2\nmust\n0.023\n\n\n3\ncan\n0.018\n\n\n4\npeople\n0.017\n\n\n5\nnation\n0.016\n\n\n6\nnew\n0.015\n\n\n7\ntime\n0.015\n\n\n8\nevery\n0.014\n\n\n9\namerica\n0.013\n\n\n10\nnow\n0.010\n\n\n\nKey themes: Collective action (‚Äúus‚Äù, ‚Äúmust‚Äù, ‚Äúcan‚Äù), change (‚Äúnew‚Äù, ‚Äúnow‚Äù)\n\nTrump‚Äôs Top Words:\n\n\n\nRank\nWord\nRelative Freq\n\n\n\n\n1\namerica\n0.038\n\n\n2\namerican\n0.024\n\n\n3\ncountry\n0.023\n\n\n4\nnation\n0.021\n\n\n5\nthank\n0.020\n\n\n6\npeople\n0.019\n\n\n7\none\n0.016\n\n\n8\nevery\n0.016\n\n\n9\nworld\n0.015\n\n\n9\ngreat\n0.015\n\n\n\nKey themes: National identity (‚Äúamerica‚Äù, ‚Äúamerican‚Äù, ‚Äúcountry‚Äù), greatness\n\nCross-President Comparison:\n\n\n\nTheme\nBiden\nBush\nObama\nTrump\n\n\n\n\nMost used word\nus\nfreedom\nus\namerica\n\n\nNational identity\n‚úì\n‚úì\n‚úì\n‚úì‚úì (strongest)\n\n\nFreedom/Liberty\n-\n‚úì‚úì\n-\n-\n\n\nUnity (‚Äúus‚Äù)\n‚úì‚úì\n‚úì\n‚úì‚úì\n-\n\n\nAction words\ncan, must\ncan\nmust, can\n-\n\n\n\nKey insight: Each president‚Äôs word choices reflect their political messaging ‚Äî Bush emphasized ‚Äúfreedom‚Äù (War on Terror era), Obama and Biden emphasized ‚Äúus‚Äù (unity), and Trump emphasized ‚Äúamerica/american‚Äù (America First).\n\n\nKeyness Analysis: Trump vs Obama\n\n# Only select speeches by Obama and Trump\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\n# Create a dfm grouped by president\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\n# Calculate keyness and determine Trump as target group\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness)\n\n\n\n\n\n\n\n\nThis keyness plot compares the distinctive word usage between Trump (target, blue) and Obama (reference, gray). The chi-squared (œá¬≤) statistic measures how significantly a word is associated with one president over the other.\nHow to read this plot:\n\nPositive chi¬≤ values (right/blue): Words used significantly MORE by Trump than Obama\nNegative chi¬≤ values (left/gray): Words used significantly MORE by Obama than Trump\nLarger bars: Stronger association with that president\n\n\nTrump‚Äôs Distinctive Words (Blue):\n\n\n\nRank\nWord\nChi¬≤ Value\nTrump\nObama\n\n\n\n\n1\nthank\n18.81\n26\n3\n\n\n2\nback\n11.21\n16\n2\n\n\n3\namerica\n10.22\n36\n14\n\n\n4\nbring\n8.55\n11\n1\n\n\n5\namerican\n8.37\n24\n8\n\n\n6\ncountry\n8.37\n24\n8\n\n\n7\nright\n7.56\n10\n1\n\n\n8\ngreat\n7.47\n16\n4\n\n\n9\npresident\n7.36\n14\n3\n\n\n10\ngoing\n7.28\n9\n0\n\n\n\nTrump‚Äôs rhetorical style: Direct, action-oriented, populist messaging with emphasis on restoration (‚Äúback‚Äù, ‚Äúbring‚Äù) and national pride (‚Äúamerica‚Äù, ‚Äúamerican‚Äù, ‚Äúgreat‚Äù).\n\nObama‚Äôs Distinctive Words (Gray):\n\n\n\nRank\nWord\nChi¬≤ Value\nTrump\nObama\n\n\n\n\n1\nus\n-32.93\n4\n44\n\n\n2\nmust\n-14.91\n4\n25\n\n\n3\ncan\n-12.31\n3\n20\n\n\n4\ngeneration\n-6.97\n0\n9\n\n\n5\ntime\n-5.58\n5\n16\n\n\n6\nwork\n-5.20\n2\n10\n\n\n7\nless\n-5.04\n0\n7\n\n\n8\nliberty\n-5.04\n0\n7\n\n\n9\nmay\n-5.04\n0\n7\n\n\n10\njourney\n-4.78\n1\n9\n\n\n\nObama‚Äôs rhetorical style: Inclusive, aspirational language emphasizing collective action (‚Äúus‚Äù, ‚Äúmust‚Äù, ‚Äúcan‚Äù) and historical continuity (‚Äúgeneration‚Äù, ‚Äújourney‚Äù).\n\nKey Contrast:\n\n\n\n\n\n\n\n\nAspect\nTrump\nObama\n\n\n\n\nPrimary pronoun\n-\n‚Äúus‚Äù (œá¬≤ = -32.93, strongest)\n\n\nCore theme\nRestoration (‚Äúback‚Äù, ‚Äúgreat‚Äù)\nProgress (‚Äújourney‚Äù, ‚Äúgeneration‚Äù)\n\n\nTone\nDirect, emphatic\nAspirational, inclusive\n\n\nFocus\nNational identity\nCollective action\n\n\nTop word\n‚Äúthank‚Äù (œá¬≤ = 18.81)\n‚Äúus‚Äù (œá¬≤ = -32.93)\n\n\n\nInsight: The keyness analysis reveals fundamentally different rhetorical strategies ‚Äî Trump‚Äôs ‚ÄúAmerica First‚Äù nationalism versus Obama‚Äôs inclusive progressivism. The word ‚Äúthank‚Äù being Trump‚Äôs most distinctive word is notable because expressions of gratitude are uncommon in traditional inaugural addresses, reflecting his more personal, campaign-style approach.\n\n\nKeyness Analysis: Trump‚Äôs Distinctive Words Only\n\n# Plot without the reference text (in this case Obama)\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\n\n\n\n\n\nThis plot shows only Trump‚Äôs distinctive words (without Obama‚Äôs reference words), making it easier to focus on what makes Trump‚Äôs inaugural speech(es) unique compared to Obama‚Äôs.\nTrump‚Äôs Most Distinctive Words (ranked by chi¬≤ value):\n\n\n\nRank\nWord\nChi¬≤ Value\nTrump\nObama\np-value\n\n\n\n\n1\nthank\n18.81\n26\n3\n&lt;0.001\n\n\n2\nback\n11.21\n16\n2\n&lt;0.001\n\n\n3\namerica\n10.22\n36\n14\n0.001\n\n\n4\nbring\n8.55\n11\n1\n0.003\n\n\n5\namerican\n8.37\n24\n8\n0.004\n\n\n6\ncountry\n8.37\n24\n8\n0.004\n\n\n7\nright\n7.56\n10\n1\n0.006\n\n\n8\ngreat\n7.47\n16\n4\n0.006\n\n\n9\npresident\n7.36\n14\n3\n0.007\n\n\n10\ngoing\n7.28\n9\n0\n0.007\n\n\n\nAdditional distinctive words:\n\nnever (œá¬≤ = 6.61), foreign (œá¬≤ = 6.27): Emphatic promises, international concerns\ndreams, first, millions (œá¬≤ = 5.27): Scale, priority, aspirational\nstates (œá¬≤ = 4.74), many (œá¬≤ = 4.17): Governance references\n\nStatistically significant words (p &lt; 0.05): 17 words total, including: thank, back, america, bring, american, country, right, great, president, going, never, foreign, dreams, first, millions, states, many\n\nThematic clusters in Trump‚Äôs rhetoric:\n\nRestoration: ‚Äúback‚Äù, ‚Äúbring‚Äù, ‚Äúgreat‚Äù ‚Äî returning to a previous state\nNational identity: ‚Äúamerica‚Äù, ‚Äúamerican‚Äù, ‚Äúcountry‚Äù ‚Äî strong patriotic emphasis\nAction/Promise: ‚Äúgoing‚Äù, ‚Äúnever‚Äù, ‚Äúfirst‚Äù ‚Äî direct, emphatic commitments\nGratitude: ‚Äúthank‚Äù ‚Äî acknowledging supporters (unusual in inaugural addresses)\nScale: ‚Äúmillions‚Äù, ‚Äúforeign‚Äù, ‚Äústates‚Äù ‚Äî emphasizing breadth of impact\n\nKey insight: The word ‚Äúthank‚Äù has the highest keyness score (œá¬≤ = 18.81), which is notable because expressions of gratitude are uncommon in traditional inaugural addresses. Trump used ‚Äúthank‚Äù 26 times compared to Obama‚Äôs 3 times. This reflects Trump‚Äôs more personal, campaign-style approach to the ceremonial speech.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DC_assignment06.html#what-is-wordfish",
    "href": "DC_assignment06.html#what-is-wordfish",
    "title": "Assignment 6 - Text Analytics using quanteda",
    "section": "What is Wordfish?",
    "text": "What is Wordfish?\nWordfish is a method that automatically figures out political positions from text ‚Äî you don‚Äôt need to tell the computer who‚Äôs left-wing or right-wing, it discovers this on its own by looking at word patterns.\nHow does it work?\n- It looks at what words each speaker uses and how often\n- Then it automatically places speakers on a political spectrum\n- Use more ‚Äúgovernment-side words‚Äù ‚Üí placed on the right\n- Use more ‚Äúopposition-side words‚Äù ‚Üí placed on the left\nWhat does Wordfish calculate?\n- Œ∏ (theta): Each speaker‚Äôs political position (this is what we care about most)\n- Œ≤ (beta): Which side each word leans toward (positive = government, negative = opposition)\n- œà (psi): How common each word is overall\nComparison of Three Text Scaling Methods\n\n\n\n\n\n\n\n\n\nAspect\nWordscores\nWordfish\nCA\n\n\n\n\nIn one sentence\nTeacher gives you the answer key, you grade based on it\nLet the computer find patterns on its own\nSimplify complex data into fewer dimensions\n\n\nType\nSupervised\nUnsupervised\nUnsupervised\n\n\nNeed reference texts?\nYes (must specify who is left/right first)\nNo\nNo\n\n\nHow it works\nCalculate based on known reference documents\nUse statistical model to estimate automatically\nDimensionality reduction (like compressing 3D ‚Üí 2D)\n\n\nProvides confidence intervals?\nYes\nYes\nNo\n\n\nAdvantage\nSimple and easy to understand\nNo need to know the answer beforehand\nGood for visualization\n\n\nDisadvantage\nMust have reference documents\nMath is more complex\nCan‚Äôt know how accurate the estimate is\n\n\n\nWhen to Use Which Method?\n\nWordscores: When you already know some documents‚Äô positions (e.g., Party A = left, Party B = right) and want to estimate others\n\nWordfish: When you don‚Äôt know who‚Äôs left or right, and want the computer to figure it out\n\nCA: When you want to explore data and visualize the overall distribution",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DC_assignment06.html#practical-analysis---irish-budget-speeches-2010",
    "href": "DC_assignment06.html#practical-analysis---irish-budget-speeches-2010",
    "title": "Assignment 6 - Text Analytics using quanteda",
    "section": "Practical Analysis - Irish Budget Speeches 2010",
    "text": "Practical Analysis - Irish Budget Speeches 2010\n\nWordscores Model\n\nlibrary(quanteda.textmodels)\n\n# Irish budget speeches from 2010 (data from quanteda.textmodels)\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Predict Wordscores model\nws &lt;- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n\nWhat is Wordscores?\nWordscores is a supervised text scaling method that:\n1. Uses reference texts with known positions (in this case: Cowen (FF) = 1, Kenny (FG) = -1)\n2. Assigns scores to each word based on how often it appears in reference texts\n3. Predicts positions of unknown texts based on their word usage\nHow to read this plot:\n\nX-axis (Word score): Political position from -1 (opposition/Fine Gael) to +1 (government/Fianna F√°il)\n\nY-axis (Log term frequency): How often the word appears (higher = more frequent)\n\nRed highlighted words: Selected words of interest\n\nHighlighted words analysis:\n\n\n\n\n\n\n\n\nWord\nScore\nInterpretation\n\n\n\n\nour\n0.69\nStrongly associated with government rhetoric (inclusive, ownership language)\n\n\nhave\n0.15\nSlightly government-leaning, neutral\n\n\nbudget\n-0.15\nSlightly opposition-leaning\n\n\nminister\n-0.49\nAssociated with opposition (addressing/criticizing ministers)\n\n\n\nWords most associated with Government (score close to +1):\n\n\n\nWord\nScore\n\n\n\n\neconomy\n0.89\n\n\n2010\n0.87\n\n\ninvestment\n0.86\n\n\nconfidence\n0.82\n\n\ndevelopment\n0.81\n\n\nemployment\n0.81\n\n\nscheme\n0.80\n\n\nfuture\n0.80\n\n\nmeasures\n0.78\n\n\nsupport\n0.78\n\n\n\nWords most associated with Opposition (score close to -1):\n\n\n\nWord\nScore\n\n\n\n\ntaoiseach\n-0.93\n\n\nhe\n-0.88\n\n\nhis\n-0.85\n\n\nbank\n-0.84\n\n\nyoung\n-0.82\n\n\nopportunity\n-0.82\n\n\nmay\n-0.77\n\n\napril\n-0.72\n\n\nsaid\n-0.72\n\n\nhope\n-0.72\n\n\n\nPattern observations:\n\nCommon words cluster at center-top: Frequent words like ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúto‚Äù, ‚Äúand‚Äù appear at the top center, as they‚Äôre used equally by both sides\nGovernment words focus on economy: Words like ‚Äúeconomy‚Äù, ‚Äúinvestment‚Äù, ‚Äúdevelopment‚Äù, ‚Äúemployment‚Äù suggest positive economic framing\nOpposition words focus on criticism: Words like ‚Äútaoiseach‚Äù, ‚Äúhe‚Äù, ‚Äúhis‚Äù, ‚Äúbank‚Äù suggest direct criticism of government leadership and banking policy\n‚Äúour‚Äù vs ‚Äúminister‚Äù:\n\nGovernment speakers use ‚Äúour‚Äù (score = 0.69) ‚Äî inclusive, ownership language\n\nOpposition speakers address ‚Äúminister‚Äù (score = -0.49) ‚Äî accountability, criticism\n\n\nKey insight: The Wordscores model reveals that seemingly neutral words carry political weight. Government rhetoric emphasizes economic progress (‚Äúeconomy‚Äù, ‚Äúinvestment‚Äù, ‚Äúconfidence‚Äù), while opposition rhetoric focuses on accountability and criticism (‚Äútaoiseach‚Äù, ‚Äúbank‚Äù, ‚Äúminister‚Äù) ‚Äî patterns that reflect the dynamics of parliamentary debate during the 2010 Irish budget crisis.\n\n\nWordscores Document Positions\n\n# Get predictions\npred &lt;- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\nThis plot shows the predicted political positions of each speaker in the 2010 Irish budget debate, grouped by their party affiliation. The positions are estimated using the Wordscores model based on the words each speaker used.\nHow to read this plot:\n\nX-axis (Document position): Political scale from negative (opposition) to positive (government)\nHorizontal lines: Confidence intervals (uncertainty in the estimate)\nDots: Estimated position for each speaker\nPanels: Grouped by party\n\nPredicted positions by speaker:\n\n\n\nSpeaker\nParty\nScore\nSE\nPosition\n\n\n\n\nCowen, Brian\nFF\n0.213\n0.003\nMost government\n\n\nLenihan, Brian\nFF\n0.116\n0.003\nGovernment\n\n\nGormley, John\nGreen\n0.086\n0.009\nGovernment (coalition)\n\n\nCuffe, Ciaran\nGreen\n0.072\n0.009\nGovernment (coalition)\n\n\nRyan, Eamon\nGreen\n0.057\n0.007\nGovernment (coalition)\n\n\nMorgan, Arthur\nSF\n0.019\n0.004\nNear center\n\n\nOCaolain, Caoimhghin\nSF\n0.017\n0.005\nNear center\n\n\nGilmore, Eamon\nLAB\n0.014\n0.005\nNear center\n\n\nBruton, Richard\nFG\n0.003\n0.004\nNear center\n\n\nBurton, Joan\nLAB\n-0.005\n0.004\nSlight opposition\n\n\nODonnell, Kieran\nFG\n-0.008\n0.006\nSlight opposition\n\n\nHiggins, Michael\nLAB\n-0.019\n0.008\nOpposition\n\n\nQuinn, Ruairi\nLAB\n-0.027\n0.008\nOpposition\n\n\nKenny, Enda\nFG\n-0.137\n0.005\nMost opposition\n\n\n\nParty summary:\n\n\n\n\n\n\n\n\n\nParty\nAverage Score\nRole\nInterpretation\n\n\n\n\nFF (Fianna F√°il)\n0.165\nGovernment (ruling party)\nStrongest pro-government language\n\n\nGreen\n0.071\nGovernment (coalition partner)\nModerate pro-government language\n\n\nSF (Sinn F√©in)\n0.018\nOpposition\nNear center, moderate rhetoric\n\n\nLAB (Labour)\n-0.009\nOpposition\nSlightly opposition-leaning\n\n\nFG (Fine Gael)\n-0.047\nOpposition\nMost opposition-leaning overall\n\n\n\nKey observations:\n\nFF (Fianna F√°il) ‚Äî Governing party members (Cowen at 0.213, Lenihan at 0.116) are positioned furthest right, reflecting pro-government language defending the budget.\nGreen Party ‚Äî As coalition partners with FF, all three members (Gormley, Cuffe, Ryan) are positioned on the government side (0.057-0.086) but closer to center than FF, showing more moderate support.\nFine Gael (FG) ‚Äî Shows the most variation within a party:\n\nKenny, Enda has the most negative position (-0.137), indicating strongest opposition rhetoric\n\nBruton, Richard (0.003) and ODonnell, Kieran (-0.008) are much closer to center\n\nLabour (LAB) ‚Äî Members spread from slightly positive (Gilmore at 0.014) to negative (Quinn at -0.027), indicating varied opposition stances.\nSinn F√©in (SF) ‚Äî Both members cluster near zero (0.017-0.019), indicating moderate, centrist rhetoric despite being an opposition party.\n\nModel validation: The Wordscores model successfully separates government speakers (FF, Green with positive scores) from opposition speakers (FG, LAB with negative or near-zero scores) based purely on word usage, demonstrating how language reflects political position.\n\n\nWordscores with LBG Transformation\n\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg &lt;- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\nThis plot shows the same Wordscores analysis as the previous plot, but with LBG (Laver, Benoit, Garry) transformation applied. This rescaling method adjusts the predicted positions to better reflect the true spread of political positions.\nWhat is LBG transformation?\nThe LBG transformation addresses a known issue with Wordscores: raw predictions tend to cluster toward the center. The LBG rescaling: - Expands the scale to show greater differentiation - Makes positions more comparable to the original reference scores - Allows predictions to exceed the original reference range (-1 to +1)\nPredicted positions with LBG rescaling:\n\n\n\nSpeaker\nParty\nLBG Score\nPosition\n\n\n\n\nCowen, Brian\nFF\n3.27\nMost pro-government\n\n\nLenihan, Brian\nFF\n1.57\nPro-government\n\n\nGormley, John\nGreen\n1.03\nGovernment (coalition)\n\n\nCuffe, Ciaran\nGreen\n0.80\nGovernment (coalition)\n\n\nRyan, Eamon\nGreen\n0.52\nGovernment (coalition)\n\n\nMorgan, Arthur\nSF\n-0.14\nMild opposition\n\n\nOCaolain, Caoimhghin\nSF\n-0.18\nMild opposition\n\n\nGilmore, Eamon\nLAB\n-0.23\nMild opposition\n\n\nBruton, Richard\nFG\n-0.42\nOpposition\n\n\nBurton, Joan\nLAB\n-0.56\nOpposition\n\n\nODonnell, Kieran\nFG\n-0.62\nOpposition\n\n\nHiggins, Michael\nLAB\n-0.81\nOpposition\n\n\nQuinn, Ruairi\nLAB\n-0.96\nOpposition\n\n\nKenny, Enda\nFG\n-2.88\nMost opposition\n\n\n\nComparison: Raw vs LBG Transformation\n\n\n\nAspect\nRaw Wordscores (Previous)\nLBG Transformation (This plot)\n\n\n\n\nScale range\n-0.14 to +0.21\n-2.88 to +3.27\n\n\nDifferentiation\nCompressed, subtle\nExpanded, clearer\n\n\nConfidence intervals\nNarrow\nWider\n\n\nInterpretation\nHarder to distinguish\nEasier to distinguish\n\n\n\nParty summary with LBG rescaling:\n\n\n\n\n\n\n\n\n\nParty\nScore Range\nAverage\nInterpretation\n\n\n\n\nFF (Fianna F√°il)\n+1.57 to +3.27\n2.42\nStrongly pro-government\n\n\nGreen\n+0.52 to +1.03\n0.78\nModerate, coalition partner\n\n\nSF (Sinn F√©in)\n-0.18 to -0.14\n-0.16\nMild opposition\n\n\nLAB (Labour)\n-0.96 to -0.23\n-0.64\nModerate opposition\n\n\nFG (Fine Gael)\n-2.88 to -0.42\n-1.31\nOpposition (varies widely)\n\n\n\nKey observations:\n\nGreater separation: The LBG transformation makes the gap between government (FF at +1.57 to +3.27) and opposition (FG at -2.88 to -0.42) much clearer.\nCowen‚Äôs dominance: Brian Cowen (FF) has the highest score (+3.27), far exceeding the original reference score of +1, reflecting extremely strong pro-budget rhetoric.\nKenny‚Äôs strong opposition: Enda Kenny (FG) shows the most extreme opposition position (-2.88), nearly three times the original reference score of -1, consistent with his role as opposition leader.\nGreen Party as middle ground: All Green members (0.52 to 1.03) fall between the governing FF and zero, reflecting their position as coalition partners with more moderate language.\nLabour variation: LAB members range from -0.23 (Gilmore) to -0.96 (Quinn), showing varied levels of opposition rhetoric within the party.\n\nWhen to use LBG transformation: Use LBG when you want to see the full spread of political positions, especially when raw Wordscores predictions are too compressed to interpret clearly. The LBG transformation better reveals speakers who are more extreme than the reference texts.\n\n\nWordfish Model\n\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf &lt;- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n\nThis plot shows the results of a Wordfish model applied to the 2010 Irish budget speeches. Unlike Wordscores (supervised), Wordfish is an unsupervised text scaling method that estimates word and document positions simultaneously without requiring reference texts.\nWhat is Wordfish?\nWordfish is a scaling model that:\n- Estimates political positions from word frequencies alone (no reference texts needed)\n- Assumes word usage follows a Poisson distribution\n- Produces two parameters for each word: beta (position) and psi (frequency)\nHow to read this plot:\n\nX-axis (Estimated beta): Word‚Äôs political position (negative = opposition, positive = government)\n\nY-axis (Estimated psi): Word‚Äôs overall frequency (higher = more common)\n\nRed highlighted words: Selected words of interest\n\nHighlighted words analysis:\n\n\n\nWord\nBeta (Position)\nPsi (Frequency)\nInterpretation\n\n\n\n\nthe\n~0.0\n~4.5 (highest)\nNeutral, most frequent word\n\n\ngovernment\n~0.0\n~2.8\nCentral topic, used by all\n\n\neconomy\n~0.3\n~2.5\nSlightly government-side\n\n\nchildren\n~-0.5\n~0.5\nSlightly opposition-side\n\n\ndeficit\n~0.8\n~0.3\nGovernment-side (defending budget)\n\n\nbank\n~-1.5\n~0.0\nOpposition-side (criticizing banks)\n\n\nglobal\n~1.5\n~-2.5\nGovernment-side (global context)\n\n\ncitizenship\n~-3.5\n~-3.0\nStrong opposition (rare word)\n\n\nproductivity\n~2.5\n~-4.0\nStrong government (rare word)\n\n\n\nPattern observations:\n\nTriangular shape: Common words cluster at top-center; rare words spread to extremes\n\nFrequent neutral words (the, government) ‚Üí top center\n\nRare partisan words ‚Üí bottom left/right corners\n\nOpposition words (left/negative beta):\n\n‚Äúbank‚Äù, ‚Äúcitizenship‚Äù ‚Äî criticism of financial sector, social issues\n\nAssociated with opposition rhetoric\n\nGovernment words (right/positive beta):\n\n‚Äúdeficit‚Äù, ‚Äúglobal‚Äù, ‚Äúproductivity‚Äù ‚Äî economic justification language\n\nUsed to defend budget decisions\n\nNeutral/shared words (center):\n\n‚Äúthe‚Äù, ‚Äúgovernment‚Äù, ‚Äúeconomy‚Äù ‚Äî used equally by both sides\n\n\nKey insight: Wordfish reveals that politically charged, rare words carry the most discriminating power. Common words like ‚Äúthe‚Äù and ‚Äúgovernment‚Äù are neutral, while specific policy terms like ‚Äúproductivity‚Äù (government) and ‚Äúcitizenship‚Äù (opposition) mark distinct political positions.\n\n\nWordfish Document Positions\n\n# Plot estimated document positions\ntextplot_scale1d(wf, \n                 margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\nThis plot shows the estimated political positions (theta) of each speaker in the 2010 Irish budget debate using the Wordfish model. Unlike Wordscores, Wordfish estimates these positions without requiring reference texts.\nHow to read this plot:\n\nX-axis (Estimated theta): Political position scale (negative = opposition, positive = government)\nHorizontal lines: Confidence intervals\nDots: Estimated position for each speaker\nPanels: Grouped by party\n\nPredicted positions by speaker:\n\n\n\nSpeaker\nParty\nTheta\nSE\nPosition\n\n\n\n\nLenihan, Brian\nFF\n1.82\n0.020\nMost pro-government\n\n\nCowen, Brian\nFF\n1.77\n0.023\nPro-government\n\n\nGormley, John\nGreen\n1.19\n0.073\nGovernment (coalition)\n\n\nCuffe, Ciaran\nGreen\n0.76\n0.073\nGovernment (coalition)\n\n\nRyan, Eamon\nGreen\n0.19\n0.063\nNear center\n\n\nMorgan, Arthur\nSF\n-0.13\n0.028\nMild opposition\n\n\nOCaolain, Caoimhghin\nSF\n-0.19\n0.036\nMild opposition\n\n\nODonnell, Kieran\nFG\n-0.49\n0.041\nOpposition\n\n\nGilmore, Eamon\nLAB\n-0.56\n0.029\nOpposition\n\n\nBruton, Richard\nFG\n-0.60\n0.028\nOpposition\n\n\nKenny, Enda\nFG\n-0.72\n0.026\nOpposition\n\n\nQuinn, Ruairi\nLAB\n-0.96\n0.039\nStrong opposition\n\n\nHiggins, Michael\nLAB\n-0.97\n0.038\nStrong opposition\n\n\nBurton, Joan\nLAB\n-1.10\n0.015\nMost opposition\n\n\n\nParty summary:\n\n\n\n\n\n\n\n\n\nParty\nTheta Range\nAverage\nRole\n\n\n\n\nFF (Fianna F√°il)\n+1.77 to +1.82\n1.79\nGovernment (ruling party)\n\n\nGreen\n+0.19 to +1.19\n0.71\nGovernment (coalition partner)\n\n\nSF (Sinn F√©in)\n-0.19 to -0.13\n-0.16\nMild opposition\n\n\nFG (Fine Gael)\n-0.72 to -0.49\n-0.60\nOpposition\n\n\nLAB (Labour)\n-1.10 to -0.56\n-0.90\nOpposition (most negative)\n\n\n\nKey observations:\n\nClear government-opposition divide: Wordfish successfully separates FF (theta ~1.8) from opposition parties (theta &lt; 0) without any reference texts.\nFF positioned furthest right: Lenihan (1.82) and Cowen (1.77) show the strongest pro-government language, consistent with their roles as Finance Minister and Taoiseach.\nGreen Party variation:\n\nGormley (1.19) and Cuffe (0.76) align strongly with government coalition\n\nRyan (0.19) positioned much closer to center, suggesting more moderate language or internal party differences\n\nLabour most oppositional: All Labour members show negative theta, with Burton (-1.10) showing the strongest opposition rhetoric, followed by Higgins (-0.97) and Quinn (-0.96).\nSinn F√©in near center: Both SF members (Morgan at -0.13, OCaolain at -0.19) cluster near zero, indicating more moderate or centrist rhetoric compared to FG and LAB.\nFine Gael moderate opposition: FG members range from -0.49 to -0.72, positioned between SF and LAB.\n\nComparison: Wordfish vs Wordscores (LBG)\n\n\n\nSpeaker\nWordfish Theta\nWordscores LBG\nConsistent?\n\n\n\n\nCowen (FF)\n1.77\n3.27\n‚úì Both highest\n\n\nLenihan (FF)\n1.82\n1.57\n‚úì Both high\n\n\nBurton (LAB)\n-1.10\n-0.56\n‚úì Both opposition\n\n\nKenny (FG)\n-0.72\n-2.88\n‚úì Both opposition\n\n\n\nBoth methods produce similar party orderings (FF &gt; Green &gt; SF &gt; FG &gt; LAB), validating the results. Wordfish achieves this without pre-defined reference scores, demonstrating the robustness of the political dimension in Irish budget debates.\n\n\nCorrespondence Analysis\n\n# Transform corpus to dfm\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\nThis plot shows the estimated political positions of each speaker using Correspondence Analysis (CA), another unsupervised text scaling method. CA is a multivariate technique that reduces high-dimensional word frequency data to reveal underlying patterns.\nWhat is Correspondence Analysis?\nCorrespondence Analysis:\n- Is an unsupervised dimensionality reduction technique (similar to PCA for categorical data)\n- Finds associations between documents and words\n- Does not require reference texts or assumptions about word distributions\n- Projects documents onto a scale based on word usage patterns\nPredicted positions by speaker (Dimension 1):\n\n\n\nSpeaker\nParty\nDim1\nPosition\n\n\n\n\nLenihan, Brian\nFF\n1.57\nMost pro-government\n\n\nCowen, Brian\nFF\n1.47\nPro-government\n\n\nGormley, John\nGreen\n0.68\nGovernment (coalition)\n\n\nCuffe, Ciaran\nGreen\n0.14\nNear center\n\n\nRyan, Eamon\nGreen\n-0.19\nNear center\n\n\nMorgan, Arthur\nSF\n-0.35\nMild opposition\n\n\nOCaolain, Caoimhghin\nSF\n-0.37\nMild opposition\n\n\nODonnell, Kieran\nFG\n-0.62\nOpposition\n\n\nGilmore, Eamon\nLAB\n-0.64\nOpposition\n\n\nBruton, Richard\nFG\n-0.78\nOpposition\n\n\nKenny, Enda\nFG\n-0.80\nOpposition\n\n\nBurton, Joan\nLAB\n-0.96\nStrong opposition\n\n\nQuinn, Ruairi\nLAB\n-1.03\nStrong opposition\n\n\nHiggins, Michael\nLAB\n-1.11\nMost opposition\n\n\n\nParty summary:\n\n\n\n\n\n\n\n\n\nParty\nDim1 Range\nAverage\nRole\n\n\n\n\nFF (Fianna F√°il)\n+1.47 to +1.57\n1.52\nGovernment (ruling party)\n\n\nGreen\n-0.19 to +0.68\n0.21\nGovernment (coalition partner)\n\n\nSF (Sinn F√©in)\n-0.37 to -0.35\n-0.36\nMild opposition\n\n\nFG (Fine Gael)\n-0.80 to -0.62\n-0.73\nOpposition\n\n\nLAB (Labour)\n-1.11 to -0.64\n-0.93\nOpposition (most negative)\n\n\n\nKey observations:\n\nNo confidence intervals: Unlike Wordscores and Wordfish, CA does not provide uncertainty estimates (no horizontal lines in the plot).\nFF clearly on government side: Lenihan (1.57) and Cowen (1.47) are positioned furthest right, consistent with their roles as Finance Minister and Taoiseach.\nGreen Party variation:\n\nGormley (0.68) positioned closest to FF among Greens\n\nRyan (-0.19) positioned on the opposition side, showing internal party variation\n\nLabour most oppositional: All Labour members show strongly negative positions, with Higgins (-1.11) being the most extreme, followed by Quinn (-1.03) and Burton (-0.96).\nSinn F√©in near center: Both SF members cluster around -0.36, more moderate than FG and LAB.\n\n\n\nSummary\nComparison of three text scaling methods:\n\n\n\nSpeaker\nWordscores (LBG)\nWordfish\nCA\n\n\n\n\nLenihan (FF)\n1.57\n1.82\n1.57\n\n\nCowen (FF)\n3.27\n1.77\n1.47\n\n\nGormley (Green)\n1.03\n1.19\n0.68\n\n\nBurton (LAB)\n-0.56\n-1.10\n-0.96\n\n\nHiggins (LAB)\n-0.81\n-0.97\n-1.11\n\n\nKenny (FG)\n-2.88\n-0.72\n-0.80\n\n\n\nParty ordering is consistent across all methods: FF &gt; Green &gt; SF &gt; FG &gt; LAB\nMethod comparison:\n\n\n\n\n\n\n\n\n\n\nMethod\nType\nReference Needed\nUncertainty\nKey Feature\n\n\n\n\nWordscores\nSupervised\nYes\nYes (SE)\nUses known reference texts\n\n\nWordfish\nUnsupervised\nNo\nYes (SE)\nAssumes Poisson distribution\n\n\nCA\nUnsupervised\nNo\nNo\nDimensionality reduction\n\n\n\nConclusion: All three methods consistently identify the government-opposition divide in Irish parliamentary debate, validating the robustness of text-based political scaling. The consistent ordering (FF &gt; Green &gt; SF &gt; FG &gt; LAB) across supervised and unsupervised methods demonstrates that political positions are clearly reflected in word usage patterns.",
    "crumbs": [
      "Home",
      "EPPS6302",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DV_assignment02.html",
    "href": "DV_assignment02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Be sure to run line by line and note the changes\nPay attention to the comments and address the question if there is one\nPlotting functions (note: exercise using the happy planet data set http://happyplanetindex.org)\n\n\n\npar()\nlines()\npoints()\naxis()\nbox()\ntext()\nmtext()\nhist()\nboxplot()\nlegend()\npersp()\nnames()\npie()\n\nd. Post your works on your blog/website\n\n\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n\n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\n\n\n\n\n# Ë®≠ÂÆöÂ∑•‰ΩúÁõÆÈåÑ\n#setwd(\"C:/Users/dells/Desktop/quarto/quarto_website/UTD_SDAR\")\n\n# ÂåØÂÖ• CSV Êñá‰ª∂\nHPIData &lt;- read.csv(\"Happy planet index.csv\")\n\n#Âú®Ë°®Ê†º‰∏≠Ê™¢Ë¶ñHPIdata\nView(HPIData)\n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(1, 1))\n\n\n## Start plotting from basics \nplot(HPIData$GDP, HPIData$HPI, pch=16, xlab = \"GDP\", ylab = \"HPI\", main = \"Happy Planet Index (HPI) versus Gross Domestic Product (GDP)\")\ntext(100000, 55, \"HPI vs GDP\")\n\n\n\n\n\n\n\n# Scatterplot\npar(las=1, mar=c(6, 6, 4, 4), cex= .7) \nplot.new()\nplot.window(range(HPIData$GDP, na.rm = TRUE), range(HPIData$HPI, na.rm = TRUE))\nHPIData &lt;- HPIData[order(HPIData$GDP), ]\nlines(HPIData$GDP, HPIData$HPI, col=\"gray50\")\npoints(HPIData$GDP, HPIData$HPI, pch=21, bg=\"white\", cex=1) \npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\nbox(bty=\"o\")\naxis(1, at = NULL, labels = TRUE) \naxis(2, at = seq(0, 60, 10))\naxis(4, at = seq(0, 60, 10))\nmtext(\"Gross Domestic Product (GDP)\", side=1, line=3, outer=FALSE, col=\"black\", cex=0.8)\nmtext(\"Happy Planet Index (HPI)\", side=2, line=3, las=0, outer=FALSE, col=\"black\", cex=0.8)\ntext(100000, 55, \"HPI vs GDP\")\ntitle(main = \"Happy Planet Index (HPI) versus Gross Domestic Product (GDP)\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n# Histogram\npar(mar = c(4.5, 4.1, 3.1, 0))\npar(col=\"gray50\", fg=\"gray30\", col.axis=\"gray50\")\nhist(HPIData$HPI, breaks = 12, ylim = c(0, 0.06), \n     col = \"gray80\", freq = FALSE, \n     main = \"Histogram of HPI with Normal Distribution\", \n     xlab = \"Happy Planet Index (HPI)\", \n     cex.lab = 1.2) \nmean_hpi &lt;- mean(HPIData$HPI, na.rm = TRUE)\nsd_hpi &lt;- sd(HPIData$HPI, na.rm = TRUE)\nx &lt;- seq(min(HPIData$HPI, na.rm = TRUE), max(HPIData$HPI, na.rm = TRUE), length = 100)\ndn &lt;- dnorm(x, mean = mean_hpi, sd = sd_hpi)\nlines(x, dn, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Boxplot\npar(mar=c(5, 5, 4, 2))\ncolors &lt;- c(\"lightblue\", \"lightgreen\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lavender\", \"peachpuff\")\n\nboxplot(HPI ~ Continent, data = HPIData,\n        boxwex = 0.4,\n        col = colors,\n        main = \"Happy Planet Index by Continent\",\n        ylab = \"Happy Planet Index (HPI)\",\n        xlab = \"Continent\", cex.lab = 1.2,\n        ylim=c(0,60))\ncontinent_names &lt;- c(\"Latin America\", \n                     \"North America & Oceania\", \n                     \"Western Europe\", \n                     \"Middle East & North Africa\", \n                     \"Africa\", \n                     \"South Asia\", \n                     \"Eastern Europe &  Central Asia\", \n                     \"East Asia\")\nlegend(\"bottomleft\", legend = continent_names , fill = colors)\n\n\n\n\n\n\n\n# Persp\nHPIData &lt;- HPIData[!is.na(HPIData$HPI), ]\nHPIData &lt;- HPIData[order(HPIData$HPI), ]\nx &lt;- HPIData$HPI\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\npar(mar=c(0, 0.5, 2.5, 0), lwd=0.9)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5,\n      xlab = \"HPI\", \n      ylab = \"HPI\", \n      main = \"Perspective Plot of HPI\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n\n# Piechart\ncontinent_counts &lt;- table(HPIData$Continent)\nnames(continent_counts) &lt;- c(\"Latin America\", \n                      \"North America & Oceania\", \n                      \"Western Europe\", \n                      \"Middle East & North Africa\", \n                      \"Africa\", \n                      \"South Asia\", \n                      \"Eastern Europe &  Central Asia\", \n                      \"East Asia\")\ncolors &lt;- c(\"lightblue\", \"lightgreen\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lavender\", \"peachpuff\")\npie(continent_counts, \n    labels = paste(names(continent_counts)), \n    main = \"Distribution of Continents in the Dataset\", \n    col = colors)",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DV_assignment02.html#paul-murrells-rgraphics-basic-r-programs",
    "href": "DV_assignment02.html#paul-murrells-rgraphics-basic-r-programs",
    "title": "Assignment 2",
    "section": "",
    "text": "### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n\n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DV_assignment02.html#exercise-using-the-happy-planet-data",
    "href": "DV_assignment02.html#exercise-using-the-happy-planet-data",
    "title": "Assignment 2",
    "section": "",
    "text": "# Ë®≠ÂÆöÂ∑•‰ΩúÁõÆÈåÑ\n#setwd(\"C:/Users/dells/Desktop/quarto/quarto_website/UTD_SDAR\")\n\n# ÂåØÂÖ• CSV Êñá‰ª∂\nHPIData &lt;- read.csv(\"Happy planet index.csv\")\n\n#Âú®Ë°®Ê†º‰∏≠Ê™¢Ë¶ñHPIdata\nView(HPIData)\n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(1, 1))\n\n\n## Start plotting from basics \nplot(HPIData$GDP, HPIData$HPI, pch=16, xlab = \"GDP\", ylab = \"HPI\", main = \"Happy Planet Index (HPI) versus Gross Domestic Product (GDP)\")\ntext(100000, 55, \"HPI vs GDP\")\n\n\n\n\n\n\n\n# Scatterplot\npar(las=1, mar=c(6, 6, 4, 4), cex= .7) \nplot.new()\nplot.window(range(HPIData$GDP, na.rm = TRUE), range(HPIData$HPI, na.rm = TRUE))\nHPIData &lt;- HPIData[order(HPIData$GDP), ]\nlines(HPIData$GDP, HPIData$HPI, col=\"gray50\")\npoints(HPIData$GDP, HPIData$HPI, pch=21, bg=\"white\", cex=1) \npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\nbox(bty=\"o\")\naxis(1, at = NULL, labels = TRUE) \naxis(2, at = seq(0, 60, 10))\naxis(4, at = seq(0, 60, 10))\nmtext(\"Gross Domestic Product (GDP)\", side=1, line=3, outer=FALSE, col=\"black\", cex=0.8)\nmtext(\"Happy Planet Index (HPI)\", side=2, line=3, las=0, outer=FALSE, col=\"black\", cex=0.8)\ntext(100000, 55, \"HPI vs GDP\")\ntitle(main = \"Happy Planet Index (HPI) versus Gross Domestic Product (GDP)\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n\n# Histogram\npar(mar = c(4.5, 4.1, 3.1, 0))\npar(col=\"gray50\", fg=\"gray30\", col.axis=\"gray50\")\nhist(HPIData$HPI, breaks = 12, ylim = c(0, 0.06), \n     col = \"gray80\", freq = FALSE, \n     main = \"Histogram of HPI with Normal Distribution\", \n     xlab = \"Happy Planet Index (HPI)\", \n     cex.lab = 1.2) \nmean_hpi &lt;- mean(HPIData$HPI, na.rm = TRUE)\nsd_hpi &lt;- sd(HPIData$HPI, na.rm = TRUE)\nx &lt;- seq(min(HPIData$HPI, na.rm = TRUE), max(HPIData$HPI, na.rm = TRUE), length = 100)\ndn &lt;- dnorm(x, mean = mean_hpi, sd = sd_hpi)\nlines(x, dn, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Boxplot\npar(mar=c(5, 5, 4, 2))\ncolors &lt;- c(\"lightblue\", \"lightgreen\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lavender\", \"peachpuff\")\n\nboxplot(HPI ~ Continent, data = HPIData,\n        boxwex = 0.4,\n        col = colors,\n        main = \"Happy Planet Index by Continent\",\n        ylab = \"Happy Planet Index (HPI)\",\n        xlab = \"Continent\", cex.lab = 1.2,\n        ylim=c(0,60))\ncontinent_names &lt;- c(\"Latin America\", \n                     \"North America & Oceania\", \n                     \"Western Europe\", \n                     \"Middle East & North Africa\", \n                     \"Africa\", \n                     \"South Asia\", \n                     \"Eastern Europe &  Central Asia\", \n                     \"East Asia\")\nlegend(\"bottomleft\", legend = continent_names , fill = colors)\n\n\n\n\n\n\n\n# Persp\nHPIData &lt;- HPIData[!is.na(HPIData$HPI), ]\nHPIData &lt;- HPIData[order(HPIData$HPI), ]\nx &lt;- HPIData$HPI\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\npar(mar=c(0, 0.5, 2.5, 0), lwd=0.9)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5,\n      xlab = \"HPI\", \n      ylab = \"HPI\", \n      main = \"Perspective Plot of HPI\")\n\n\n\n\n\n\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n\n# Piechart\ncontinent_counts &lt;- table(HPIData$Continent)\nnames(continent_counts) &lt;- c(\"Latin America\", \n                      \"North America & Oceania\", \n                      \"Western Europe\", \n                      \"Middle East & North Africa\", \n                      \"Africa\", \n                      \"South Asia\", \n                      \"Eastern Europe &  Central Asia\", \n                      \"East Asia\")\ncolors &lt;- c(\"lightblue\", \"lightgreen\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lavender\", \"peachpuff\")\npie(continent_counts, \n    labels = paste(names(continent_counts)), \n    main = \"Distribution of Continents in the Dataset\", \n    col = colors)",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 2"
    ]
  },
  {
    "objectID": "DV_assignment04.html",
    "href": "DV_assignment04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "knitr::opts_chunk$set(warning = FALSE)",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DV_assignment04.html#chart-1-by-mamie-cincotta",
    "href": "DV_assignment04.html#chart-1-by-mamie-cincotta",
    "title": "Assignment 4",
    "section": "Chart 1 by Mamie Cincotta",
    "text": "Chart 1 by Mamie Cincotta\nThis graphic is a bar plot depicting the carbon footprint of different regions around the world, with each bar‚Äôs width proportional to the population size of the corresponding region. The y-axis represents the total carbon footprint in metric tons of CO‚ÇÇ equivalent (tCO‚ÇÇe) for each region, while the x-axis lists the regions, including South America, North America & Oceania, Western Europe, and others. The names of the regions are displayed at an angle for readability (due to las = 2), and the bar widths are scaled based on the population size for each region, emphasizing the relative population contribution to the region‚Äôs carbon footprint. The color of the bars is set to ‚Äústeelblue,‚Äù and the overall title of the plot is ‚ÄúCarbon Footprint by Region with Population Widths,‚Äù illustrating the relationship between population and carbon emissions across different global regions.\n\n# Aggregate data\ncontems &lt;- aggregate(happyplanetdf$\"Carbon Footprint (tCO2e)\", list(happyplanetdf$Continent), FUN = sum)\ncontpop &lt;- aggregate(happyplanetdf$\"Population (thousands)\", list(happyplanetdf$Continent), FUN = sum)\n \n# Name the regions\nnames &lt;- c(\"South America\", \"North America and Oceania\",\"Western Europe\",\"Middle East\", \"Africa\", \"South Asia\", \"Eastern Europe & Central Asia\", \"East Asia\")\ndata &lt;- data.frame(contems, names)\n \n# Make the plot\nbarplot(width = contpop$x, height = contems$x, names=rep(data$names), las = 2, cex.names=.5, ylab = \"Carbon Footprint\", xlab = \"Region\", main = \"Carbon Footprint by Region with Population Widths\", col = \"steelblue\")",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DV_assignment04.html#chart-2-by-warren-cox",
    "href": "DV_assignment04.html#chart-2-by-warren-cox",
    "title": "Assignment 4",
    "section": "Chart 2 by Warren Cox",
    "text": "Chart 2 by Warren Cox\nThis code generates a scatterplot matrix for key variables in the Happy Planet Index (HPI) dataset, providing a visual representation of the relationships between six different variables: Population, Life Expectancy, Wellbeing (Ladder of Life), Carbon Footprint, HPI, and GDP per capita. The matrix includes pairwise scatterplots that help identify potential correlations between variables.\nThis matrix helps identify potential trends, correlations, and distributions in the HPI data. It can reveal patterns such as whether regions with higher GDP per capita also have higher well-being or lower carbon footprints, and allows users to explore multivariate relationships between the key HPI indicators.\n\nselected_data &lt;- happyplanetdf[c(\"Population (thousands)\",\"Life Expectancy (years)\",\"Ladder of life (Wellbeing) (0-10)\",\"Carbon Footprint (tCO2e)\",\"HPI\",\"GDP per capita ($)\", \"Continent\")]\n \npar(family = \"serif\", cex = 1.5) \nlibrary(psych)\npairs.panels(selected_data[1:6],\n             main = \"Scatterplot Matrix of HPI Data Variables\",\n             method = \"pearson\", # correlation method\n             hist.col = \"forestgreen\",\n             density = TRUE,  # show density plots\n             ellipses = FALSE)",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DV_assignment04.html#chart-3-by-liberty-smith",
    "href": "DV_assignment04.html#chart-3-by-liberty-smith",
    "title": "Assignment 4",
    "section": "Chart 3 by Liberty Smith",
    "text": "Chart 3 by Liberty Smith\nThis horizontal stacked bar chart visualizes the distribution of countries by Wellbeing Category across various continents. The x-axis represents the number of countries in each continent, while the y-axis lists the continents, including Latin America, North America & Oceania, Western Europe, and others. Each bar is stacked by Wellbeing Category (Good, Average, Poor), with different colors indicating the categories: seagreen for ‚ÄúGood,‚Äù goldenrod for ‚ÄúAverage,‚Äù and firebrick for ‚ÄúPoor.‚Äù The chart allows for easy comparison of how countries within each continent are distributed across these wellbeing categories, with the custom labels improving readability. The use of coord_flip() makes the bars horizontal, providing a clear and concise visual representation of the data.\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\nlibrary(tidyr)\n\n# Categorize Life Expectancy, Wellbeing, and Carbon Footprint\nhappyplanetdf &lt;- happyplanetdf %&gt;%\n  \n  # Categorize Wellbeing\n  mutate(Wellbeing_Category = case_when(\n    `Ladder of life (Wellbeing) (0-10)` &lt; 5.0 ~ \"Poor\",\n    `Ladder of life (Wellbeing) (0-10)` &gt;= 5.1 & `Ladder of life (Wellbeing) (0-10)` &lt; 6.0 ~ \"Average\",\n    `Ladder of life (Wellbeing) (0-10)` &gt;= 6.0 ~ \"Good\",\n    TRUE ~ NA_character_  # For missing values, set NA\n  ))\n# Remove NA values in Wellbeing_Category for this plot\nclean_data &lt;- happyplanetdf %&gt;%\n  filter(!is.na(Wellbeing_Category))\n\n# Count the number of countries per continent and wellbeing category\ncount_data &lt;- clean_data %&gt;%\n  group_by(Continent, Wellbeing_Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Continent'. You can override using the\n`.groups` argument.\n\n# Custom continent labels\ncontinent_labels &lt;- c(\n  \"1\" = \"Latin\\nAmerica\",\n  \"2\" = \"N. America\\n& Oceania\",\n  \"3\" = \"Western\\nEurope\",\n  \"4\" = \"Middle\\nEast\",\n  \"5\" = \"Africa\",\n  \"6\" = \"South\\nAsia\",\n  \"7\" = \"Eastern Europe\\n& Central Asia\",\n  \"8\" = \"East\\nAsia\"\n)\n\n# Create the horizontal stacked bar chart\nggplot(count_data, aes(x = Continent, y = Count, fill = Wellbeing_Category)) +\n  geom_bar(stat = \"identity\") +\n  scale_x_discrete(labels = continent_labels) +  # Apply custom labels\n  labs(title = \"Number of Countries per Continent by Wellbeing Category\",\n       x = \"Continent\", y = \"Number of Countries\", fill = \"Wellbeing Category\") +\n  scale_fill_manual(values = c(\"Good\" = \"seagreen\", \"Average\" = \"goldenrod\", \"Poor\" = \"firebrick\")) +\n  theme_minimal() +\n  coord_flip()  # This flips the chart horizontally",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DV_assignment04.html#chart-4-by-shiu-ting-ling",
    "href": "DV_assignment04.html#chart-4-by-shiu-ting-ling",
    "title": "Assignment 4",
    "section": "Chart 4 By Shiu-Ting Ling",
    "text": "Chart 4 By Shiu-Ting Ling\nThis bar chart depicts the number of countries in each continent that have an HPI (Happy Planet Index) above or below the global average. The x-axis represents continents, with custom labels such as ‚ÄúLatin America,‚Äù ‚ÄúWestern Europe,‚Äù and ‚ÄúEast Asia.‚Äù The y-axis represents the number of countries in each continent. The chart differentiates between countries with higher HPI (colored in firebrick2) and those with lower HPI (colored in dodgerblue2), using side-by-side bars for each continent to indicate the count. The custom labels in the legend clarify the categories: ‚ÄúCountries with higher HPI‚Äù and ‚ÄúCountries with lower HPI.‚Äù The chart‚Äôs title, ‚ÄúNumber of Countries Above/Below Average HPI by Continent,‚Äù succinctly summarizes the data. The labels on the x-axis are kept horizontal for readability, and the minimalist theme ensures the focus remains on the data while small adjustments like legend size and text formatting improve the overall presentation.\n\n# Calculate the average HPI for all countries\naverage_hpi &lt;- mean(happyplanetdf$HPI, na.rm = TRUE)\n\n# Define custom continent names\ncontinent_names &lt;- c(\"Africa\", \n                     \"Latin\\nAmerica\", \n                     \"North\\nAmerica\\n&\\nOceania\", \n                     \"Eastern\\nEurope\\n&\\nCentral\\nAsia\", \n                     \"East\\nAsia\", \n                     \"Middle\\nEast\\n&\\nNorth\\nAfrica\", \n                     \"South\\nAsia\", \n                     \"Western\\nEurope\")\n\n# Update the Continent column as a factor and assign custom labels\nhappyplanetdf$Continent &lt;- factor(happyplanetdf$Continent, \n                                  levels = 1:8, \n                                  labels = continent_names)\n\n# Calculate the number of countries in each continent above and below the average HPI\ncontinent_hpi_comparison &lt;- happyplanetdf %&gt;%\n    group_by(Continent) %&gt;%\n    summarise(\n        High_HPI_Count = sum(HPI &gt; average_hpi, na.rm = TRUE),\n        Low_HPI_Count = sum(HPI &lt;= average_hpi, na.rm = TRUE)\n    ) %&gt;%\n    pivot_longer(cols = c(\"High_HPI_Count\", \"Low_HPI_Count\"), names_to = \"HPI_Category\", values_to = \"Country_Count\")\n\n# Use ggplot2 to create a bar chart and adjust the angle of x-axis labels\nggplot(continent_hpi_comparison, aes(x = Continent, y = Country_Count, fill = HPI_Category)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    labs(title = \"Number of Countries Above/Below Average HPI by Continent\",\n         x = \"Continent\",\n         y = \"Number of Countries\",\n         fill = \"HPI Category\") +\n    scale_fill_manual(\n        values = c(\"High_HPI_Count\" = \"firebrick2\", \"Low_HPI_Count\" = \"dodgerblue2\"),  # Optional: Define custom colors for each category\n        labels = c(\"High_HPI_Count\" = \"Countries with\\nhigher HPI\", \n                   \"Low_HPI_Count\" = \"Countries with\\nlower HPI\")  # Change legend labels\n    ) +\n    theme_minimal() +\n    theme(\n        legend.position = \"right\",                  # Set legend position to the right\n        legend.key.size = unit(0.5, \"cm\"),           # Make the legend boxes smaller\n        legend.text = element_text(size = 6),        # Change the legend text size\n        axis.text.x = element_text(size = 8,         # Adjust x-axis label text size\n                                   angle = 0,        # Keep x-axis labels horizontal\n                                   hjust = 0.5,        # Adjust horizontal alignment\n                                   vjust = 0.5,),      # Adjust vertical alignment\n        plot.title = element_text(size = 12,         # Increase the title font size\n                                  face = \"bold\",     # Set title to bold\n                                  margin = margin(t = 10, b = 10))  # Add margin to title for spacing\n    )",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 4"
    ]
  },
  {
    "objectID": "DV_assignment06.html",
    "href": "DV_assignment06.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Can you change to your personal font?\nHint: Append the following to the ui part of the program tags$style(HTML(‚Äù body { background-color: white; color: black; } h1 { font-family: ‚ÄòPalatino‚Äô, sans-serif; } .shiny-input-container { color: #000000; }‚Äú))",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DV_assignment06.html#example",
    "href": "DV_assignment06.html#example",
    "title": "Assignment 6",
    "section": "Example:",
    "text": "Example:",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DV_assignment06.html#my-shiny-app",
    "href": "DV_assignment06.html#my-shiny-app",
    "title": "Assignment 6",
    "section": "My shiny app:",
    "text": "My shiny app:",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 6"
    ]
  },
  {
    "objectID": "DV_assignment08.html",
    "href": "DV_assignment08.html",
    "title": "Assignment 8",
    "section": "",
    "text": "See #4",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 8"
    ]
  },
  {
    "objectID": "DV_assignment08.html#texas-voter-turnout-rate-data",
    "href": "DV_assignment08.html#texas-voter-turnout-rate-data",
    "title": "Assignment 8",
    "section": "Texas Voter Turnout Rate Data",
    "text": "Texas Voter Turnout Rate Data\n\n\nSource:¬†Texas Voter Turnout Rate Data",
    "crumbs": [
      "Home",
      "EPPS6356",
      "Assignment 8"
    ]
  },
  {
    "objectID": "IM_assignment02.html",
    "href": "IM_assignment02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Answer all questions. Prepare your answers in presentation format and be ready to present in class\n\n1. What are the differences between relation schema, relation and instance? Give an example using the university database to illustrate.\n1. Relation: the table.\n\n2. Relation schema: the logical design of the relation, consisting of a list of attributes and their corresponding domains. The schema of a relation typically does not change.\nInstrctor (ID, Name, depart_name, salary)\n3. Relation instance: a snapshot of data in the relation at a specific point in time. It corresponds to the programming language concept of a variable‚Äôs value. The value of a variable may change over time; similarly, the contents of a relation instance may change over time as the relation is updated.\nAssume that in the first week of the 2025 Spring semester, the instance of the instructor and department table is as follows:\n\nIf data is later added or deleted, the instance will change.\n\n\n2. Draw a schema diagram for the following bank database:\n\n\n\n\n3. Consider the above bank database. Assume that branch names (branch_name)and customer names (customer_name)uniquely identify branches and customers, but loans and accounts can be associated with more than one customer.\n\nWhat are the appropriate primary keys? (Underline each in diagram)\nGiven your choice of primary keys, identify appropriate foreign keys.\n\nSchema diagram was seen in # 2\n\nrepresents primary key.\n\nrepresents foreign key.\nSummary:\n1. Primary keys\n\nbranchÔºöbranch_name\ncustomerÔºöID\nloanÔºöloan_number\nborrowerÔºöID, loan_number\naccountÔºöaccount_number\ndepositorÔºöID, account_number\n\n2. Foreign keys\n\nloan.branch_name ‚Üí branch.branch_name\nborrower.ID ‚Üí customer.ID\nborrower.loan_number ‚Üí loan.loan_number\naccount.branch_name ‚Üí branch.branch_name\ndepositor.ID ‚Üí customer.ID\ndepositor.account_number ‚Üí account.account_number",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 2"
    ]
  },
  {
    "objectID": "IM_assignment04.html",
    "href": "IM_assignment04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Answer 1, 2 and two in 3 (i.e.¬†3a, 3b | 3b, 3c |3a, 3c). Prepare your answers in presentation format and be ready to present in class",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 4"
    ]
  },
  {
    "objectID": "IM_assignment04.html#a-draw-the-e-r-diagram-using-draw.io.-read-this-website-for-instructions.",
    "href": "IM_assignment04.html#a-draw-the-e-r-diagram-using-draw.io.-read-this-website-for-instructions.",
    "title": "Assignment 4",
    "section": "(a) Draw the E-R diagram using draw.io. Read this website for instructions.",
    "text": "(a) Draw the E-R diagram using draw.io. Read this website for instructions.",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 4"
    ]
  },
  {
    "objectID": "IM_assignment04.html#b-expand-to-all-teams-in-the-league-hint-add-team-entity",
    "href": "IM_assignment04.html#b-expand-to-all-teams-in-the-league-hint-add-team-entity",
    "title": "Assignment 4",
    "section": "(b) Expand to all teams in the league (Hint: add team entity)",
    "text": "(b) Expand to all teams in the league (Hint: add team entity)",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 4"
    ]
  },
  {
    "objectID": "IM_assignment04.html#a-consider-the-query",
    "href": "IM_assignment04.html#a-consider-the-query",
    "title": "Assignment 4",
    "section": "(a) Consider the query",
    "text": "(a) Consider the query\n\n\nfile.exists(\"sql.db\")\n\n[1] TRUE\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Establishing a SQLite Connection\nconn &lt;- dbConnect(SQLite(), \"sql.db\")\n\n# Set Quarto to use this SQL connection\nknitr::opts_chunk$set(connection = conn)\n\n\nselect course_id, semester, year, sec_id, avg (tot_cred)  \nfrom takes natural join student  \nwhere year = 2017  \ngroup by course_id, semester, year, sec_id  \nhaving count (ID) &gt;= 2;\n\n\n3 records\n\n\ncourse_id\nsemester\nyear\nsec_id\navg (tot_cred)\n\n\n\n\nCS-101\nFall\n2017\n1\n65\n\n\nCS-190\nSpring\n2017\n2\n43\n\n\nCS-347\nFall\n2017\n1\n67\n\n\n\n\n\n\ni Explain why appending natural join section in the from clause would not change the result. (Consult Ch. 4, 4.1.1)\nAdding NATURAL JOIN section does not change the result because the takes table already contains the necessary attributes (course_id, semester, year, sec_id) that are also present in section. The natural join would not introduce any new information but merely duplicate existing data. As a result, the GROUP BY and HAVING conditions remain unaffected, and the query‚Äôs output stays the same.\n\n\nii Test the results using the Online SQL interpreter (https://www.dbbook.com/university-lab-dir/sqljs.html)\n\nselect course_id, semester, year, sec_id, avg (tot_cred)  \nfrom takes natural join student natural join section  \nwhere year = 2017  \ngroup by course_id, semester, year, sec_id  \nhaving count (ID) &gt;= 2;\n\n\n3 records\n\n\ncourse_id\nsemester\nyear\nsec_id\navg (tot_cred)\n\n\n\n\nCS-101\nFall\n2017\n1\n65\n\n\nCS-190\nSpring\n2017\n2\n43\n\n\nCS-347\nFall\n2017\n1\n67",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 4"
    ]
  },
  {
    "objectID": "IM_assignment04.html#b-write-an-sql-query-using-the-university-schema-to-find-the-id-of-each-student-who-has-never-taken-a-course-at-the-university.-do-this-using-no-subqueries-and-no-set-operations-use-an-outer-join.-consult-ch.-4-4.1.3",
    "href": "IM_assignment04.html#b-write-an-sql-query-using-the-university-schema-to-find-the-id-of-each-student-who-has-never-taken-a-course-at-the-university.-do-this-using-no-subqueries-and-no-set-operations-use-an-outer-join.-consult-ch.-4-4.1.3",
    "title": "Assignment 4",
    "section": "(b) Write an SQL query using the university schema to find the ID of each student who has never taken a course at the university. Do this using no subqueries and no set operations (use an outer join). (Consult Ch. 4, 4.1.3)",
    "text": "(b) Write an SQL query using the university schema to find the ID of each student who has never taken a course at the university. Do this using no subqueries and no set operations (use an outer join). (Consult Ch. 4, 4.1.3)\n\nSELECT student.ID  \nFROM student  \nLEFT OUTER JOIN takes ON student.ID = takes.ID  \nWHERE takes.ID IS NULL;\n\n\n1 records\n\n\nID\n\n\n\n\n70557",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 4"
    ]
  },
  {
    "objectID": "IM_assignment04.html#c-consider-the-following-database-write-a-query-to-find-the-id-of-each-employee-with-no-manager.-note-that-an-employee-may-simply-have-no-manager-listed-or-may-have-a-null-manageruse-natural-left-outer-join.-consult-ch.-4-4.1.3",
    "href": "IM_assignment04.html#c-consider-the-following-database-write-a-query-to-find-the-id-of-each-employee-with-no-manager.-note-that-an-employee-may-simply-have-no-manager-listed-or-may-have-a-null-manageruse-natural-left-outer-join.-consult-ch.-4-4.1.3",
    "title": "Assignment 4",
    "section": "(c) Consider the following database, write a query to find the ID of each employee with no manager. Note that an employee may simply have no manager listed or may have a null manager(use natural left outer join). (Consult Ch. 4, 4.1.3)",
    "text": "(c) Consider the following database, write a query to find the ID of each employee with no manager. Note that an employee may simply have no manager listed or may have a null manager(use natural left outer join). (Consult Ch. 4, 4.1.3)\n\nSELECT employee.ID  \nFROM employee  \nNATURAL LEFT OUTER JOIN manages  \nWHERE manages.manager_id IS NULL;",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 4"
    ]
  },
  {
    "objectID": "IM_assignment06.html",
    "href": "IM_assignment06.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Answer all of the following: Prepare your answers in presentation format and be ready to present in class",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 6"
    ]
  },
  {
    "objectID": "IM_assignment06.html#i-express-the-following-query-in-sql-using-no-subqueries-and-no-set-operations.-hint-left-outer-join",
    "href": "IM_assignment06.html#i-express-the-following-query-in-sql-using-no-subqueries-and-no-set-operations.-hint-left-outer-join",
    "title": "Assignment 6",
    "section": "i Express the following query in SQL using no subqueries and no set operations. (Hint: left outer join)",
    "text": "i Express the following query in SQL using no subqueries and no set operations. (Hint: left outer join)\n\n\nfile.exists(\"sql.db\")\n\n[1] TRUE\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Establishing a SQLite Connection\nconn &lt;- dbConnect(SQLite(), \"sql.db\")\n\n# Set Quarto to use this SQL connection\nknitr::opts_chunk$set(connection = conn)\n\n\nselect ID\nfrom student\nexcept\nselect s_id\nfrom advisor\nwhere i_ID is not null\n\n\n4 records\n\n\nID\n\n\n\n\n19991\n\n\n54321\n\n\n55739\n\n\n70557\n\n\n\n\n\nThe result shows the student IDs that appear in the student table but not in the advisor table (i.e., the students who do not have an advisor).\nThen, rewrite using LEFT OUTER JOIN.\n\nselect s.ID\nfrom student s\nleft join advisor a on s.ID = a.s_id\nwhere a.s_id is null;\n\n\n4 records\n\n\nID\n\n\n\n\n19991\n\n\n54321\n\n\n55739\n\n\n70557",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 6"
    ]
  },
  {
    "objectID": "IM_assignment06.html#ii-using-the-university-schema-write-an-sql-query-to-find-the-names-and-ids-of-those-instructors-who-teach-every-course-taught-in-his-or-her-department-i.e.-every-course-that-appears-in-the-course-relation-with-the-instructors-department-name.-order-result-by-name.",
    "href": "IM_assignment06.html#ii-using-the-university-schema-write-an-sql-query-to-find-the-names-and-ids-of-those-instructors-who-teach-every-course-taught-in-his-or-her-department-i.e.-every-course-that-appears-in-the-course-relation-with-the-instructors-department-name.-order-result-by-name.",
    "title": "Assignment 6",
    "section": "ii Using the university schema, write an SQL query to find the names and IDs of those instructors who teach every course taught in his or her department (i.e., every course that appears in the course relation with the instructor‚Äôs department name). Order result by name.",
    "text": "ii Using the university schema, write an SQL query to find the names and IDs of those instructors who teach every course taught in his or her department (i.e., every course that appears in the course relation with the instructor‚Äôs department name). Order result by name.\n\nselect i.ID, i.name\nfrom instructor i\nwhere not exists (\n  select c.course_id\n  from course c\n  where c.dept_name = i.dept_name\n    and not exists (\n      select t.course_id\n      from teaches t\n      where t.ID = i.ID and t.course_id = c.course_id\n    )\n)\norder by i.name;\n\n\n5 records\n\n\nID\nname\n\n\n\n\n22222\nEinstein\n\n\n32343\nEl Said\n\n\n98345\nKim\n\n\n15151\nMozart\n\n\n12121\nWu",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 6"
    ]
  },
  {
    "objectID": "IM_assignment06.html#a-import-the-full-database-using-sql-code-for-creating-large-relations-from-textbook-website",
    "href": "IM_assignment06.html#a-import-the-full-database-using-sql-code-for-creating-large-relations-from-textbook-website",
    "title": "Assignment 6",
    "section": "(a) Import the full database using SQL code for creating large relations (from textbook website)",
    "text": "(a) Import the full database using SQL code for creating large relations (from textbook website)",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 6"
    ]
  },
  {
    "objectID": "IM_assignment06.html#b-run-rpostgres01.r-in-teams-r-folder",
    "href": "IM_assignment06.html#b-run-rpostgres01.r-in-teams-r-folder",
    "title": "Assignment 6",
    "section": "(b) Run RPostgres01.R (in Teams R folder)",
    "text": "(b) Run RPostgres01.R (in Teams R folder)",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 6"
    ]
  },
  {
    "objectID": "IM_assignment06.html#c-post-results-on-website",
    "href": "IM_assignment06.html#c-post-results-on-website",
    "title": "Assignment 6",
    "section": "(c) Post results on website",
    "text": "(c) Post results on website\n\n# Check required packages\nrequired_packages &lt;- c(\"RPostgres\", \"DBI\", \"odbc\")\nmissing &lt;- setdiff(required_packages, rownames(installed.packages()))\nif (length(missing) &gt; 0) {\n  knitr::opts_chunk$set(eval = FALSE)\n  stop(\"Missing packages: \", paste(missing, collapse = \", \"), \n       \". Please install them manually in the R console.\")\n}\n\n# Optional: cleaner tables (no NA shown)\noptions(knitr.kable.NA = '')\n\n# Load libraries\nlibrary(RPostgres)\nlibrary(DBI)\nlibrary(odbc)\n\n# Connect to PostgreSQL\ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"university\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"postgres\",\n  password = \"Ting87724$\"\n)\n\n# Perform queries\ninstructor_data &lt;- dbGetQuery(con, \"SELECT * FROM instructor\")\ncomp_sci_instructors &lt;- dbGetQuery(\n  con, \n  \"SELECT * FROM instructor \n   WHERE dept_name = 'Comp. Sci.' AND salary &gt; 60000;\"\n)\nstudent_data &lt;- dbGetQuery(con, \"SELECT * FROM student WHERE tot_cred &gt;= 50\")\n\n# Show results in HTML using kable (with English captions)\nknitr::kable(head(instructor_data, 10), caption = \"Top 10 Instructor Records\")\n\n\nTop 10 Instructor Records\n\n\nid\nname\ndept_name\nsalary\n\n\n\n\n63395\nMcKinnon\nCybernetics\n94333.99\n\n\n78699\nPingr\nStatistics\n59303.62\n\n\n96895\nMird\nMarketing\n119921.41\n\n\n4233\nLuo\nEnglish\n88791.45\n\n\n4034\nMurata\nAthletics\n61387.56\n\n\n50885\nKonstantinides\nLanguages\n32570.50\n\n\n79653\nLevine\nElec. Eng.\n89805.83\n\n\n50330\nShuming\nPhysics\n108011.81\n\n\n80759\nQueiroz\nBiology\n45538.32\n\n\n73623\nSullivan\nElec. Eng.\n90038.09\n\n\n\n\nknitr::kable(comp_sci_instructors, caption = \"Computer Science Instructors with Salary &gt; 60000\")\n\n\nComputer Science Instructors with Salary &gt; 60000\n\n\nid\nname\ndept_name\nsalary\n\n\n\n\n34175\nBondi\nComp. Sci.\n115469.11\n\n\n3335\nBourrier\nComp. Sci.\n80797.83\n\n\n\n\nknitr::kable(head(student_data), caption = \"Students with Total Credits &gt;= 50\")\n\n\nStudents with Total Credits &gt;= 50\n\n\nid\nname\ndept_name\ntot_cred\n\n\n\n\n79352\nRumat\nFinance\n100\n\n\n76672\nMiliko\nStatistics\n116\n\n\n14182\nMoszkowski\nCivil Eng.\n73\n\n\n44985\nPrieto\nBiology\n91\n\n\n44271\nSowerby\nEnglish\n108\n\n\n40897\nCoppens\nMath\n58\n\n\n\n\n# Disconnect\ndbDisconnect(con)\n\n```",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Assignment 6"
    ]
  },
  {
    "objectID": "IM_Final.html",
    "href": "IM_Final.html",
    "title": "Traditional Chinese Medicine for Everyday Wellness - A Searchable Health Preservation Database",
    "section": "",
    "text": "Schema in E-R Model",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Final Presentation"
    ]
  },
  {
    "objectID": "IM_Final.html#tcm-shinyapp-english-version",
    "href": "IM_Final.html#tcm-shinyapp-english-version",
    "title": "Traditional Chinese Medicine for Everyday Wellness - A Searchable Health Preservation Database",
    "section": "2.1 TCM Shinyapp (English version)",
    "text": "2.1 TCM Shinyapp (English version)\n\n\nSource:¬†Integrated TCM Query System",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Final Presentation"
    ]
  },
  {
    "objectID": "IM_Final.html#tcm-shinyapp-chinese-version",
    "href": "IM_Final.html#tcm-shinyapp-chinese-version",
    "title": "Traditional Chinese Medicine for Everyday Wellness - A Searchable Health Preservation Database",
    "section": "2.2 TCM Shinyapp (Chinese version)",
    "text": "2.2 TCM Shinyapp (Chinese version)\n\n\nSource:¬†‰∏≠ÈÜ´Êï¥ÂêàÊü•Ë©¢Á≥ªÁµ±",
    "crumbs": [
      "Home",
      "EPPS6354",
      "Final Presentation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is Ling‚Äôs learning journal at UTD. üìöüìù"
  }
]